<!DOCTYPE html>
<html lang="en-US" prefix="og: http://ogp.me/ns#" class="old-ie no-js">
  <head><title>Cloud Automation case study with Ceph storage cluster - MSys 
        Technologies</title>
  </head>
  <body>

<b>Table of Contents</b>

<OL>
  <LI><a href="#CloudAutomationCephStorage">Cloud Automation with Ceph Storage 
         Cluster</a>
  <LI><a href="#TerraformVsVagrant">Terraform Vs Vagrant</a>
  <LI><a href="#SaltStackGrains">SaltStack Grains</a>
  <LI><a href="#SaltstackPillar">Saltstack Pillar</a>
  <LI><a href="#"></a>
</OL>

<a name="CloudAutomationCephStorage"></a>
<h3><span>Cloud Automation: case study with Ceph storage cluster</span> 
<a href="http://msystechnologies.com/cloud-automation-case-study-with-ceph-storage-cluster/" 
target="_b">(Source Origin)</a></h3>

<h4 class="color-blue">Abstract:</h4>


<p>The success of virtual machines(VM) is well known today with its mass adoption 
everywhere. Today we have well-established workflows and tool sets to help manage VM 
life cycles and associated services. The proliferation and growth of this cycle, 
ultimately led to cloud deployments. Amazon Web Services and Google Cloud Engines are 
a few of the dozens of service providers today, who offer terms and services that make 
provisioning VMs anywhere easier. Both with the proliferation of cloud providers and 
the scale of cloud, comes today's newer set of problems. Configuration management 
and provisioning of those VMs has become a nightmare. While one side of physical 
infrastructure dependency has been virtually eliminated it has resulted in another 
domain of configuration management of those VMs (and clusters) that needs to be 
addressed. A slew of tool sets came out to address them. Chef, Puppet, Ansible, 
SaltStack are widely known and used everywhere. SaltStack being the latest entrant 
to this club. Given our Python background, we look at SaltStack as a Configuration 
Management tool. We also used another new entrant Terraform for provisioning VMs 
needed in the cluster, and bootstrapping them to run Saltstack.</p>

<h4 class="vc_custom_heading">Introduction</h4>

<p>With a proliferation of cloud providers providing Infrastructure as a service, there 
has been a constant innovation to deliver more. Microsoft Azure, Amazon Web Services, 
Google Cloud Engine are to name a few here. This has resulted in Platform as a Service 
model, where in not just the infrastructure is managed, but more tools/workflows got 
defined to enable application development and deployment easier. Google App Engine was 
one of the earliest success stories here. But for any user of these cloud platform 
resulted in several headaches.</p>

<ul>
  <li>Vendor lock-in of technologies since services and interfaces for cloud are not a 
      standard.</li>
  <li>Re-creation of platform from development to elsewhere was a pain.</li>
  <li>Migration from one cloud provider to another was nightmare.</li>
</ul>


<p>The need for the following requirements, flow from earlier pain points and dawned 
on everyone using cloud deployments:</p>


<ul>
  <li>A Specification for infrastructure so it can be captured and restored as need be. 

      <P>By infrastructure we do consider a cluster of VMs and associated services. So 
      network configuration, high availability and other services as dictated by the 
      service provider had to be captured.</li>
  <li>A way for the bootstrap logic and configuration on those infrastructure needs to 
      be captured.</li>
  <li>And configuration captured should ideally be agnostic to the cloud provider.</li>
  <li>All of this, in a tool that is understood by everyone, so its simple and easily 
      adaptable are major plus.</li>
</ul>



<p>When I looked at the suite of tools, Ruby, Ruby on Rails were alien to me. Python was 
native. And Saltstack had some nice features that we could really consider. If Saltstack 
can bootstrap and initialize resources, Terraform can help customize external 
environments as well. Put them to work together and we do see a great marriage on the 
cards.  But will they measure up? Let us brush through some of their designs and get 
to a real life scenario and see how they scale up indeed.</p>

<h4>2 Our Cloud Toolbox</h4>

<h5>2.1 <a href="https://www.terraform.io/" target="_b">Terraform</a>
&nbsp;&nbsp;
<a href="https://github.com/hashicorp/terraform" target="_b">hashicorp/terraform 
github</a></h5>

<p><a href="#TerraformVsVagrant" target="_b">Terraform is a successor to Vagrant</a> 
from the stable of Hashicorp.  Vagrant brought spawning of VMs to developers a breeze. 
The key tenets of Vagrant that made it well loved are its ability to perform 
lightweight, reproducible and portable environments. Today, the power of Vagrant is well 
known. As I see it, the need for bootstrapping a distributed cluster applications was 
not easily doable with it. So we have Terraform from the same makers, who understood the 
limitations of Vagrant and enabled it to achieve bootstrapping clustered environments 
easier. Terraform defines extensible providers, that encapsulates connectivity 
information specific to each cloud provider.  Terraform defines resources that 
encapsulate services from each cloud provider.  And each resource could be extended by 
one or more provisioners.  Provisioner has the same concept as in Vagrant but is much 
more extensible. Provisioner in Vagrant can only provision newly created VMs. But here 
enters the power of Terraform.</p>


<p>Terraform has support for local-exec and remote-exec, through which one can automate 
extensible scripts through them either locally or on remote machines. As the name 
implies, local-exec, runs locally on the node where the script is invoked, while 
remote-exec executes in the targeted remote machine. And several property of the new VM 
are readily available. And additional custom attributes can be defined through output 
specification as well. Additionally, there exists a null-resource, which is pseudo 
resource along with explicit dependencies support that transforms Terraform to a 
powerhouse. All of these provide much greater flexibility with setting up complex 
environments outside of just provisioning and bootstrapping VMs.</p>


<p>A better place to understand Terraform in all its glory would be to visit their doc 
page <a href="https://www.terraform.io/docs/index.html" target="_b">Terraform: 
[3]</a>.</p>

<h5>2.2 <a href="https://docs.saltstack.com/en/latest/" target="_b">SaltStack</a>
&nbsp;&nbsp;
<a href="https://github.com/saltstack/salt" target="_b">saltstack/salt github</a></h5>


<p>SaltStack is used to deploy, manage and automate infrastructure and applications at 
cloud scale. SaltStack is written over Python and uses Jinja template engine. SaltStack 
is architected to have a Master node and one or more Minion nodes. Multiple Master 
Nodes can also be setup to create a High Available environment. SaltStack brings some 
newer terminology with it that needs some familiarity. But once it is understood, it is 
fairly easy to use it to suit our purpose. I shall briefly touch upon SaltStack here, 
and would rightly point to their rich source of documentation here <a target="_b" 
href="https://docs.saltstack.com/en/latest/contents.html">SaltStack: [4]</a>.</p>


<p>To put it succinctly, Grains are read-only key-value attributes of Minions. All 
Minions export their immutable attributes to SaltMaster as Grains. As an example, one 
can find cpu speed, cpu make, cpu cores, memory capacity, disk capacity, os flavor, 
version, network cards and many more all available part of that nodes Grains. Pillar 
is part of SaltMaster, holding all customization needed over the cluster. Configuration 
kept part of Pillar can be targeted to minions, and only those minions will have that 
information available. To help with an example, using Pillar one can define two sets of 
users/groups to be configured on nodes in the cluster. Minion nodes that are part of 
Finance domain, will have one set of users applied, while those part of Dev domain will 
have another set. User/Group definition is defined once in the SaltMaster as a Pillar 
file, and can be targeted based on Minion nodes domainname, part of its Grain. Few other 
examples would be package variations across distributions can be handled easily. Any 
Operations person can easily relate to nightmare for automating a simple request to 
install Apache Webserver on any Linux distribution (Hint: the complexity lies in the 
non-standard Linux distributions). Pillar is your friend in this case. All of this 
configuration part of either Pillar or Salt State are confusingly though written in the 
same file format(.sls) and are called Salt States. These Salt State Files (.sls) 
specify the configuration to be applied either explicitly, or through Jinja templating. 
A top level state file at both Pillar [default location: /srv/pillar/top.sls] and State 
[default location: /srv/state/top.sls] exists, wherein targeting of configuration can 
be accomplished.</p>

<a name="Figure1"></a>
<figure class="wpb_wrapper vc_figure">
<div><img src="http://msystechnologies.com/wp-content/uploads/2016/02/ceph-storage_lakshmi-blog-1024x764.jpg" alt=""  height="700" width="900"></div>
</figure>

<h4>3 Case Study</h4>

<p>Let us understand the power of Terraform and SaltStack together in action, for a real 
life deployment. Ceph is an open source distributed storage cluster. Needless to say, 
setting up Ceph cluster is a challenge even with all the documents available <a 
target="_b" href="http://docs.ceph.com/docs/master/start/">Ceph Storage Cluster Setup: 
[6]<a>. Even while using ceph-deploy script, one needs to satisfy pre-flight 
pre-requisites before it can be used. This case study shall first setup a cluster with 
prerequisites met and then use ceph-deploy over it, to bring up the ceph cluster.</p>


<p>Let us try to use the power of tools we have chosen and summarize our findings while 
setting up Ceph Cluster. Is it really that powerful and easy to create and replicate 
the environment anywhere? Let us find out.</p>


<p>We shall replicate a similar setup as provided in the ceph documentation</p>


<p><a href="#Figure1">[Figure: 1].</a> We shall have 4 VM nodes in the cluster, 
ceph-admin, ceph-monitor-0, ceph-osd-0 and ceph-osd-1. Even though in our cluster, we 
have only a single ceph-monitor node, I have suffix'd it with an instance number "0". 
This is to allow later expansion of monitors as needed, since ceph does allow multiple 
monitor nodes too. It is assumed that the whole setup is being created from ones PC 
desktop/laptop environment, which is behind a company proxy and cannot act as SaltMaster.
 We shall use Terraform to create the needed VMs and bootstrap them with appropriate 
configuration to run either as Salt Master or Salt Minion. ceph-admin node shall act as 
a Salt Master Node as well and hold all configuration necessary to install, initialize 
and bring up the whole cluster.</p>


<h5>3.1 Directory structure</h5>

<p>We shall host all files in the below directory structure. This structure is assumed 
in scripts. The files are referenced below in greater details.</p>

<figure class="wpb_wrapper vc_figure">
<div><img src="http://msystechnologies.com/wp-content/uploads/2016/02/Directory-Structure-lakshmi-1024x576.jpg" 
height="500" width="900"></div>
</figure>


<p>We shall use DigitalOcean as our cloud provider for this case study. I am assuming 
the work machine is signed up with DigitalOcean to enable automatic provisioning of 
systems. I will use my local workmachine to this purpose. To work with DigitalOcean 
and provision VMs automatically, there are two steps involved.</p>

<ul>
  <li>Create a Personal Access Token(PAN), which is a form of authentication token to 
      enable auto provisioning resources. \cite{DigitalOceanPAN}:[7]. The key created 
      has to saved securely, as it cannot be recovered again from their console.</li>
  <li>Use the PAN to add the public key of local workmachine to enable auto login into 
      newly provisioned VMs easily \cite{DigitalOceanSSH}:[8]. This is necessary to 
      allow passwordless ssh sessions, that enable further customization auto-magically 
      on those created VMs.The next step is to define these details part of terraform, 
      let us name this file provider.tf.</li>
</ul>


<font size=5>
<pre>
variable "do_token" {}
variable "pub_key" {}
variable "pvt_key" {}
variable "ssh_fingerprint" {}

provider "digitalocean" {
 token = "${var.do_token}"
}
</pre>
</font>


<p>The above defines input variables that needs to be properly setup for provisioning 
services with a particular cloud provider.  do_token is the PAN obtained during 
registration from DigitalOcean directly. The other three properties are used to setup 
the VMs provisioned to enable auto login into them from our local workmachine. The ssh 
fingerprint can be obtained by running ssh-keygen as below.</p>


<font size=5>
<pre>
user@machine&gt; ssh-keygen -lf ~/.ssh/myrsakey.pub
2048 00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd:ee:ff /home/name/.ssh/myrsakey.pub (RSA)
</pre>
</font>

<p>The above input variables, can be assigned values in a file, so they will be 
automatically initialized instead of requesting end users every time scripts are 
invoked. The special file which Terraform looks for initializing the input variables 
are terraform.tfvars. Below would be a sample content of that file.</p>


<font size=5>
<pre>
do_token="07a91b2aa4bc7711df3d9fdec4f30cd199b91fd822389be92b2be751389da90e"
pub_key="/home/name/.ssh/id_rsa.pub"
pvt_key="/home/name/.ssh/id_rsa"
ssh_fingerprint="0:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd:ee:ff"
</pre>
</font>


<p>The above settings should ensure successful connection with DigitalOcean cloud 
provider and enable one to provision services through automation scripts.</p>

<h5>3.3 Ceph-Admin</h5>

<p>Now let us spawn and create a VM to act as our Ceph-Admin node. For each node type, 
let us create a separate terraform file to hold the configuration. It is not a must, 
but it helps keep sanity while perusing code and is self-explanatory.</p>


<p>For Ceph-Admin we have captured bootstrapping part of Terraform configuration. While 
the rest of the nodes configuration is captured part of Salt state files. It is possible 
to run salt minion in Ceph-Admin node as well, and apply configuration. We instead chose 
Terraform for bootstrapping Ceph Admin totally, to help us understand both ways. In 
either case, the configuration is captured part of spec and is readily replicable 
anywhere. The power of Terraform is just not with configuration/provisioning of VMs but 
external environments as well.</p>


<p>The Ceph-Admin node, shall not only satisfy Ceph cluster installation prerequisites, 
but have Salt Master running on it as well. It shall have two users defined, cephadm 
with sudo privileges over the entire cluster, and demo user. The ssh keys are generated 
everytime the cluster is provisioned without caching and replicating the keys. Also the 
user profile is replicated on all nodes in the cluster. The Salt configuration and state 
files have to setup additionally. Setting up this configuration file based on the 
attributes of the provisioned cluster has a dependency here. This dependency is very 
nicely handled through Terraform by their null resources and explicit dependency 
chains.</p>

<h5>3.3.1 admin.tf - Terraform Listing</h5>

<p>Below is listed admin.tf that holds configuration necessary to bring up cephadmin 
node with embedded comments</p>

<font size=5>
<pre>
# resource maps directly to services provided by cloud providers.
# it is always of the form x_y, wherein x is the cloud provider and y is the targeted service.
# the last part that follows is the name of the resource.
# below initializes attributes that are defined by the cloud provider to create VM. 
resource "digitalocean_droplet" "admin" {
 image = "centos-7-0-x64"
 name = "ceph-admin"
 region = "sfo1"
 size = "1gb"
 private_networking = true
 ssh_keys = [
 "${var.ssh_fingerprint}"
 ]
# below defines the connection parameters necessary to do ssh for further customization.
# For this to work passwordless, the ssh keys should be pre-registered with cloud provider.
 connection {
 user = "root"
 type = "ssh"
 key_file = "${var.pvt_key}"
 timeout = "10m"
 }

# All below provisioners, perform the actual customization and run 
# in the order specified in this file.
# "remote-exec" performs action on the remote VM over ssh.
# Below one could see some necessary directories are being created.
 provisioner "remote-exec" {
 inline = [
 "mkdir -p /opt/scripts /srv/salt /srv/pillar",
 "mkdir -p /srv/salt/users/cephadm/keys /srv/salt/users/demo/keys"',
 "mkdir -p /srv/salt/files",
 ]
 }

# "file" provisioner copies files from local workmachine (where the script is being run) to
# remote VM. Note the directories should exist, before this can pass. 
# The below copies the whole directory contents from local machine to remote VM.
# These scripts help setup the whole environment and can be depended to be available at 
# /opt/scripts location. Note, the scripts do not have executable permission bits set.
# Note the usage of "path.module", these are interpolation extensions provided by Terraform.
 provisioner "file" {
 source = "${path.module}/scripts/"
 destination = "/opt/scripts/"
 }

# A cephdeploy.repo file has to be made available at yum repo, for it to pick ceph packages.
# This requirement comes from setting up ceph storage cluster.
 provisioner "file" {
 source = "${path.module}/scripts/cephdeploy.repo"
 destination = "/etc/yum.repos.d/cephdeploy.repo"
 }

# Setup handcrafted custom sudoers file to allow running sudo through ssh without terminal connection.
# Also additionally provide necessary sudo permissions to cephadm user.
 provisioner "file" {
 source = "${path.module}/scripts/salt/salt/files/sudoers"
 destination = "/etc/sudoers"
 }

# Now, setup yum repos and install packages as necessary for Ceph admin node.
# Additionally ensure salt-master is installed.
# Create two users, cephadm privileged user with sudo access for managing the ceph cluster and demo guest user.
# The passwords are also set accordingly.
# Remember to set proper permissions to the scripts.
# The provisioned VM attributes can be easily used to customize several properties as needed. In our case, 
# the IP address (public and private), VM host name are used to customize the environment further.
# For ex: hosts file, salt master configuration file and ssh_config file are updated accordingly.
 provisioner "remote-exec" {
 inline = [
 "export PATH=$PATH:/usr/bin",
 "chmod 0440 /etc/sudoers",
 "yum install -y epel-release yum-utils",
 "yum-config-manager --enable cr",
 "yum install -y yum-plugin-priorities",
 "yum clean all",
 "yum makecache",
 
 "yum install -y wget salt-master",
 "cp -af /opt/scripts/salt/* /srv",

 "yum install -y ceph-deploy --nogpgcheck",
 "yum install -y ntp ntpdate ntp-doc",

 "useradd -m -G wheel cephadm",
 "echo \"cephadm:c3ph@dm1n\" | chpasswd",
 "useradd -m -G docker demo",
 "echo \"demo:demo\" | chpasswd",

 "chmod +x /opt/scripts/*.sh",
 "/opt/scripts/fixadmin.sh ${self.ipv4_address} ${self.ipv4_address_private} ${self.name}",
 ]
 }
}
</pre>
</font>

<h5>3.3.2 Dependency scripts - fixadmin.sh</h5>

<p>Below we list the scripts referenced from above Terraform file. fixadmin.sh script 
will be used to customize the VM further after creation. This script shall perform the 
following functions. It shall update cluster information in /opt/nodes directory, to 
help further customization to know the cluster attributes (read network address etc). 
Additionally, it patches several configuration files to enable automation without 
intervention.</p>


<font size=5>
<pre>
#!/bin/bash
# Expects ./fixadmin.sh   
# Performs the following.
# a. caches cluster information in /opt/nodes
# b. patches /etc/hosts file to connect through private-ip for cluster communication.
# c. patches ssh_config file to enable auto connect without asking confirmation for given node.
# d. creates 2 users, with appropriate ssh keys
# e. customize salt configuration with cluster properties.

mkdir -p /opt/nodes
chmod 0755 /opt/nodes

echo "$1" &gt; /opt/nodes/admin.public
echo "$2" &gt; /opt/nodes/admin.private

rm -f /opt/nodes/masters*
sed -i '/demo-admin/d' /etc/hosts
echo "$2 demo-admin" &gt;&gt; /etc/hosts

sed -i '/demo-admin/,+1d' /etc/ssh/ssh_config
echo "Host demo-admin" &gt;&gt; /etc/ssh/ssh_config
echo " StrictHostKeyChecking no" &gt;&gt; /etc/ssh/ssh_config

for user in cephadm demo; do
 rm -rf /home/${user}/.ssh
 su -c "cat /dev/zero | ssh-keygen -t rsa -N \"\" -q" ${user}
 cp /home/${user}/.ssh/id_rsa.pub /srv/salt/users/${user}/keys/key.pub
 cp /home/${user}/.ssh/id_rsa.pub /home/${user}/.ssh/authorized_keys
done

systemctl enable salt-master
systemctl stop salt-master
sed -i '/interface:/d' /etc/salt/master
echo "#script changes below" &gt;&gt; /etc/salt/master
echo "interface: ${2}" &gt;&gt; /etc/salt/master

systemctl start salt-master
</pre>
</font>

<h5>3.3.3 Dependency - Ceph yum repo spec</h5>

<p>cephdeploy.repo defines a yum repo to fetch the ceph related packages. Below is 
customized to install on CentOS with ceph Hammer package. This comes directly from 
ceph pre-requisite.</p>



<font size=5>
<pre>
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-hammer/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
</pre>
</font>

<h5>3.4 Ceph-Monitor</h5>

<p>Let monitor.tf be the file that holds all configuration necessary to bring up 
ceph-monitor node.</p>

<font size=5>
<pre>
# resource specifies the attributes required to bring up ceph-monitor node. 
# Note have the node name has been customized with an index, and the usage of 'count'
# 'count' is a special attribute that lets one create multiple instances of the same spec.
# That easy!
resource "digitalocean_droplet" "master" {
 image = "centos-7-0-x64"
 name = "ceph-monitor-${count.index}"
 region = "sfo1"
 size = "512mb"
 private_networking = true
 ssh_keys = [
 "${var.ssh_fingerprint}"
 ]
 count=1
 connection {
 user = "root"
 type = "ssh"
 key_file = "${var.pvt_key}"
 timeout = "10m"
 }

 provisioner "remote-exec" {
 inline = [
 "mkdir -p /opt/scripts /opt/nodes",
 ]
 }

 provisioner "file" {
 source = "${path.module}/scripts/"
 destination = "/opt/scripts/"
 }

 # This provisioner has implicit dependency on admin node to be available.
 # below we use admin node's property to fix ceph-monitor's salt minion configuration file,
 # so it can reach salt master.
 provisioner "remote-exec" {
 inline = [
 "export PATH=$PATH:/usr/bin",
 "yum install -y epel-release yum-utils",
 "yum-config-manager --enable cr",
 "yum install -y yum-plugin-priorities",
 "yum install -y salt-minion",
 "chmod +x /opt/scripts/*.sh",
 "/opt/scripts/fixsaltminion.sh ${digitalocean_droplet.admin.ipv4_address_private} ${self.name}",
 ]
 }
}
</pre>
</font>

<h5>3.5 Ceph-Osd</h5>

<p>Let minion.tf file contain configuration necessary to bring up ceph-osd nodes.</p>


<font size=5>
<pre>
resource "digitalocean_droplet" "minion" {
    image = "centos-7-0-x64"
    name = "ceph-osd-${count.index}"
    region = "sfo1"
    size = "1gb"
    private_networking = true
    ssh_keys = [
      "${var.ssh_fingerprint}"
    ]
		# Here we specify two instances of this specification. Look above though the 
		# hostnames are customized already by using interpolation.
    count=2
  connection {
      user = "root"
      type = "ssh"
      key_file = "${var.pvt_key}"
      timeout = "10m"
  }

  provisioner "remote-exec" {
    inline = [
      "mkdir -p /opt/scripts /opt/nodes",
    ]
  }

  provisioner "file" {
     source = "${path.module}/scripts/"
     destination = "/opt/scripts/"
  }

  provisioner "remote-exec" {
    inline = [
      "export PATH=$PATH:/usr/bin",
      "yum install -y epel-release yum-utils yum-plugin-priorities",
      "yum install -y salt-minion",
      "chmod +x /opt/scripts/*.sh",
      "/opt/scripts/fixsaltminion.sh ${digitalocean_droplet.admin.ipv4_address_private} ${self.name}",
    ]
  }
}
</pre>
</font>

<h5>3.5.1 Dependency - fixsaltminion.sh script</h5>

<p>This script enables all saltminion nodes to fix their configuration, so it can reach 
the salt master. Other salt minion attributes are customized as well below.</p>


<font size=5>
<pre>
#!/bin/bash
# The below script ensures salt-minion nodes configuration file
# are patched to reach Salt master.
# args:   
systemctl enable salt-minion
systemctl stop salt-minion

sed -i -e '/master:/d' /etc/salt/minion
echo "#scripted below config changes" &gt;&gt; /etc/salt/minion
echo "master: ${1}" &gt;&gt; /etc/salt/minion
echo "${2}" &gt; /etc/salt/minion_id

systemctl start salt-minion
</pre>
</font>

<h5>3.6 Cluster Pre-flight Setup</h5>

<p>Null resources are great extensions to Terraform for providing the flexibility 
needed to configure complex cluster environments. Let one create cluster-init.tf to 
help fixup the configuration dependencies in cluster.</p>



<font size=5>
<pre>
resource "null_resource" "cluster-init" {
    # so far we have relied on implicit dependency chain without specifying one.
		# Here we will ensure that this resources gets run only after successful creation of its
		# dependencies.
    depends_on = [
        "digitalocean_droplet.admin",
        "digitalocean_droplet.master",
        "digitalocean_droplet.minion",
    ]

  connection {
      host = "${digitalocean_droplet.admin.ipv4_address}"
      user = "root"
      type = "ssh"
      key_file = "${var.pvt_key}"
      timeout = "10m"
  }

  # Below we run few other scripts based on the cluster configuration.
	# And finally ensure all the other nodes in the cluster are ready for
	# ceph installation.
  provisioner "remote-exec" {
    inline = [
        "/opt/scripts/fixmasters.sh ${join(\" \", digitalocean_droplet.master.*.ipv4_address_private)}",
        "/opt/scripts/fixslaves.sh ${join(\" \", digitalocean_droplet.minion.*.ipv4_address_private)}",
        "salt-key -Ay",
        "salt -t 10 '*' test.ping",
        "salt -t 20 '*' state.apply common",
        "salt-cp '*' /opt/nodes/* /opt/nodes",
        "su -c /opt/scripts/ceph-pkgsetup.sh cephadm",
    ]
  }
}
</pre>
</font>

<h5>3.6.1 Dependency - fixmaster.sh script</h5>

<font size=5>
<pre>
#!/bin/bash
# This script fixes host file and collects cluster info under /opt/nodes
# Also updates ssh_config accordingly to ensure passwordless ssh can happen to
# other nodes in the cluster without prompting for confirmation.
# args: 
NODES=""
i=0 

for ip in "$@"
do
    NODE="ceph-monitor-$i"
    sed -i "/$NODE/d" /etc/hosts
    echo "$ip $NODE" &gt;&gt; /etc/hosts
    echo $NODE &gt;&gt; /opt/nodes/masters
    echo "$ip" &gt;&gt; /opt/nodes/masters.ip
    sed -i "/$NODE/,+1d" /etc/ssh/ssh_config
    NODES="$NODES $NODE"
    i=$[i+1]
done

echo "Host $NODES" &gt;&gt; /etc/ssh/ssh_config
echo "  StrictHostKeyChecking no" &gt;&gt; /etc/ssh/ssh_config
</pre>
</font>

<h5>3.6.2 Dependency - fixslaves.sh script</h5>

<h5>3.6.3 Dependency - ceph-pkgsetup.sh script</h5>

<font size=5>
<pre>
#!/bin/bash
# This script fixes host file and collects cluster info under /opt/nodes
# Also updates ssh_config accordingly to ensure passwordless ssh can happen to
# other nodes in the cluster without prompting for confirmation.
# args: 

NODES=""
i=0

mkdir -p /opt/nodes
chmod 0755 /opt/nodes

rm -f /opt/nodes/minions*

for ip in "$@"
do
    NODE="ceph-osd-$i"
    sed -i "/$NODE/d" /etc/hosts
    echo "$ip $NODE" &gt;&gt; /etc/hosts
    echo $NODE &gt;&gt; /opt/nodes/minions
    echo "$ip" &gt;&gt; /opt/nodes/minions.ip
    sed -i "/$NODE/,+1d" /etc/ssh/ssh_config
    NODES="$NODES $NODE"
    i=$[i+1]
done

echo "Host $NODES" &gt;&gt; /etc/ssh/ssh_config
echo "  StrictHostKeyChecking no" &gt;&gt; /etc/ssh/ssh_config
</pre>
</font>


<font size=5>
<pre>
#!/bin/bash
# has to be run as user 'cephadm' with sudo privileges.
# install ceph packages on all nodes in the cluster.
mkdir -p $HOME/my-cluster
cd $HOME/my-cluster

OPTIONS="--username cephadm --overwrite-conf"
echo "Installing ceph components"
RELEASE=hammer
for node in `sudo cat /opt/nodes/masters`
do
    ceph-deploy $OPTIONS install --release ${RELEASE} $node
done

for node in `sudo cat /opt/nodes/minions`
do
    ceph-deploy $OPTIONS install --release ${RELEASE} $node
done
</pre>
</font>

<h5>3.7 Cluster Bootstrapping</h5>

<p>With the previous section, we have completed successfully setting up the cluster to 
meet all pre-requisites for installing ceph. The below final bootstrap script, just 
ensures that the needed ceph functionality gets applied before they are brought up 
online.</p>


<p>File: cluster-bootstrap.tf</p>


<font size=5>
<pre>
resource "null_resource" "cluster-bootstrap" {
    depends_on = [
        "null_resource.cluster-init",
    ]

  connection {
      host = "${digitalocean_droplet.admin.ipv4_address}"
      user = "root"
      type = "ssh"
      key_file = "${var.pvt_key}"
      timeout = "10m"
  }

  provisioner "remote-exec" {
    inline = [
        "su -c /opt/scripts/ceph-install.sh cephadm",
        "salt 'ceph-monitor-*' state.highstate",
        "salt 'ceph-osd-*' state.highstate",
    ]
  }
}

</pre>
</font>

<h5>3.7.1 Dependency - ceph-install.sh script</h5>

<font size=5>
<pre>
#!/bin/bash
# This script has to be run as user 'cephadm', because  this user has
# sudo privileges set all across the cluster.

OPTIONS="--username cephadm --overwrite-conf"
# pre-cleanup.
rm -rf $HOME/my-cluster
for node in `cat /opt/nodes/masters /opt/nodes/minions`
do
    ssh $node "sudo rm -rf /etc/ceph/* /var/local/osd* /var/lib/ceph/mon/*"
    ssh $node "find /var/lib/ceph -type f | xargs sudo rm -rf"
done

mkdir -p $HOME/my-cluster
cd $HOME/my-cluster

echo "1. Preparing for ceph deployment"
ceph-deploy $OPTIONS new ceph-monitor-0
# Adjust the configuration to suit our cluster.
echo "osd pool default size = 2" &gt;&gt; ceph.conf
echo "osd pool default pg num = 16" &gt;&gt; ceph.conf
echo "osd pool default pgp num = 16" &gt;&gt; ceph.conf
echo "public network = `cat /opt/nodes/admin.private`/16" &gt;&gt; ceph.conf

echo "2. Add monitor and gather the keys"
ceph-deploy $OPTIONS mon create-initial

echo "3. Create OSD directory on each minions"
i=0
OSD=""
for node in `cat /opt/nodes/minions`
do
    ssh $node sudo mkdir -p /var/local/osd$i
    ssh $node sudo chown -R cephadm:cephadm /var/local/osd$i
    OSD="$OSD $node:/var/local/osd$i"
    i=$[i+1]
done

echo "4. Prepare OSD on minions - $OSD"
ceph-deploy $OPTIONS osd prepare $OSD

echo "5. Activate OSD on minions"
ceph-deploy $OPTIONS osd activate $OSD

echo "6. Copy keys to all nodes"
for node in `cat /opt/nodes/masters`
do
    ceph-deploy $OPTIONS admin $node
done

for node in `cat /opt/nodes/minions`
do
    ceph-deploy $OPTIONS admin $node
done

echo "7. Set permission on keyring"
sudo chmod +r /etc/ceph/ceph.client.admin.keyring

echo "8. Add in more monitors in cluster if available"
for mon in `cat /opt/nodes/masters`
do
    if [ "$mon" != "ceph-monitor-0" ]; then
        ceph-deploy $OPTIONS mon create $mon
    fi
done
</pre>
</font>

<h5>3.7.2 SaltStack Pillar setup</h5>

<p>As mentioned in the directory structure section, pillar specific files are located 
in a specific directory. The configuration and files are customized for each node with 
specific functionality.</p>


<font size=5>
<pre>
# file: top.sls
base:
  "*":
    - users
</pre>
</font>


<font size=5>
<pre>
# file: users.sls
groups:

users:
  cephadm:
    fullname: cephadm
    uid: 5000
    gid: 5000
    shell: /bin/bash
    home: /home/cephadm
    groups:
      - wheel
    password: $6$zYFWr3Ouemhtbnxi$kMowKkBYSh8tt2WY98whRcq.
    enforce_password: True
    key.pub: True

  demo:
    fullname: demo
    uid: 5031
    gid: 5031
    shell: /bin/bash
    home: /home/demo
    password: $6$XmIJ.Ox4tNKHa4oYccsYOEszswy1
    key.pub: True
</pre>
</font>

<h5>3.7.3 SaltStack State files</h5>

<font size=5>
<pre>
# file: top.sls
base:
    "*":
        - common
    "ceph-admin":
        - admin
    "ceph-monitor-*":
        - master
    "ceph-osd-*":
        - minion
</pre>
</font>


<font size=5>
<pre>
# file: common.sls
{% for group, args in pillar['groups'].iteritems() %}
{{ group }}:
  group.present:
    - name: {{ group }}
{% if 'gid' in args %}
    - gid: {{ args['gid'] }}
{% endif %}
{% endfor %}

{% for user, args in pillar['users'].iteritems() %}
{{ user }}:
  group.present:
    - gid: {{ args['gid'] }}
  user.present:
    - home: {{ args['home'] }}
    - shell: {{ args['shell'] }}
    - uid: {{ args['uid'] }}
    - gid: {{ args['gid'] }}
{% if 'password' in args %}
    - password: {{ args['password'] }}
{% if 'enforce_password' in args %}
    - enforce_password: {{ args['enforce_password'] }}
{% endif %}
{% endif %}
    - fullname: {{ args['fullname'] }}
{% if 'groups' in args %}
    - groups: {{ args['groups'] }}
{% endif %}
    - require:
      - group: {{ user }}

{% if 'key.pub' in args and args['key.pub'] == True %}
{{ user }}_key.pub:
  ssh_auth:
    - present
    - user: {{ user }}
    - source: salt://users/{{ user }}/keys/key.pub
  ssh_known_hosts:
    - present
    - user: {{ user }}
    - key: salt://users/{{ user }}/keys/key.pub
    - name: "demo-master-0"
{% endif %}
{% endfor %}

/etc/sudoers:
  file.managed:
    - source: salt://files/sudoers
    - user: root
    - group: root
    - mode: 440

/opt/nodes:
  file.directory:
    - user: root
    - group: root
    - mode: 755

/opt/scripts:
  file.directory:
    - user: root
    - group: root
    - mode: 755
</pre>
</font>


<font size=5>
<pre>
# file: admin.sls
include:
  - master

bash /opt/scripts/bootstrap.sh:
  cmd.run
</pre>
</font>


<font size=5>
<pre>
# file: master.sls
# one can include any packages, configuration to target ceph monitor nodes here.
masterpkgs:
    pkg.installed:
    - pkgs:
      - ntp
      - ntpdate
      - ntp-doc
</pre>
</font>


<font size=5>
<pre>
# file: minion.sls
# one can include any packages, configuration to target ceph osd nodes here.
minionpkgs:
    pkg.installed:
    - pkgs:
      - ntp
      - ntpdate
      - ntp-doc
</pre>
</font>


<font size=5>
<pre>
# file: files/sudoers
# customized for setting up environment to satisfy 
# ceph pre-flight checks.
# 
## Sudoers allows particular users to run various commands as
## the root user, without needing the root password.
##
## Examples are provided at the bottom of the file for collections
## of related commands, which can then be delegated out to particular
## users or groups.
##
## This file must be edited with the 'visudo' command.

## Host Aliases
## Groups of machines. You may prefer to use hostnames (perhaps using
## wildcards for entire domains) or IP addresses instead.
# Host_Alias     FILESERVERS = fs1, fs2
# Host_Alias     MAILSERVERS = smtp, smtp2

## User Aliases
## These aren't often necessary, as you can use regular groups
## (ie, from files, LDAP, NIS, etc) in this file - just use %groupname
## rather than USERALIAS
# User_Alias ADMINS = jsmith, mikem


## Command Aliases
## These are groups of related commands...

## Networking
# Cmnd_Alias NETWORKING = "/sbin/route, /sbin/ifconfig, /bin/ping, /sbin/dhclient, 
#                         /usr/bin/net, /sbin/iptables, /usr/bin/rfcomm, /usr/bin/wvdial,
#                         /sbin/iwconfig, /sbin/mii-tool"

## Installation and management of software
# Cmnd_Alias SOFTWARE = /bin/rpm, /usr/bin/up2date, /usr/bin/yum

## Services
# Cmnd_Alias SERVICES = /sbin/service, /sbin/chkconfig

## Updating the locate database
# Cmnd_Alias LOCATE = /usr/bin/updatedb

## Storage
# Cmnd_Alias STORAGE = /sbin/fdisk, /sbin/sfdisk, /sbin/parted, /sbin/partprobe, /bin/mount, /bin/umount

## Delegating permissions
# Cmnd_Alias DELEGATING = /usr/sbin/visudo, /bin/chown, /bin/chmod, /bin/chgrp

## Processes
# Cmnd_Alias PROCESSES = /bin/nice, /bin/kill, /usr/bin/kill, /usr/bin/killall

## Drivers
# Cmnd_Alias DRIVERS = /sbin/modprobe

# Defaults specification

#
# Disable "ssh hostname sudo ", because it will show the password in clear.
#         You have to run "ssh -t hostname sudo ".
#
Defaults:cephadm    !requiretty

#
# Refuse to run if unable to disable echo on the tty. This setting should also be
# changed in order to be able to use sudo without a tty. See requiretty above.
#
Defaults   !visiblepw

#
# Preserving HOME has security implications since many programs
# use it when searching for configuration files. Note that HOME
# is already set when the the env_reset option is enabled, so
# this option is only effective for configurations where either
# env_reset is disabled or HOME is present in the env_keep list.
#
Defaults    always_set_home

Defaults    env_reset
Defaults    env_keep =  "COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR LS_COLORS"
Defaults    env_keep += "MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE"
Defaults    env_keep += "LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES"
Defaults    env_keep += "LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE"
Defaults    env_keep += "LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY"

#
# Adding HOME to env_keep may enable a user to run unrestricted
# commands via sudo.
#
# Defaults   env_keep += "HOME"

Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin

## Next comes the main part: which users can run what software on
## which machines (the sudoers file can be shared between multiple
## systems).
## Syntax:
##
##      user    MACHINE=COMMANDS
##
## The COMMANDS section may have other options added to it.
##
## Allow root to run any commands anywhere
root    ALL=(ALL)       ALL

## Allows members of the 'sys' group to run networking, software,
## service management apps and more.
# %sys ALL = NETWORKING, SOFTWARE, SERVICES, STORAGE, DELEGATING, PROCESSES, LOCATE, DRIVERS

## Allows people in group wheel to run all commands
# %wheel  ALL=(ALL)       ALL

## Same thing without a password
%wheel        ALL=(ALL)       NOPASSWD: ALL

## Allows members of the users group to mount and unmount the
## cdrom as root
# %users  ALL=/sbin/mount /mnt/cdrom, /sbin/umount /mnt/cdrom

## Allows members of the users group to shutdown this system
# %users  localhost=/sbin/shutdown -h now

## Read drop-in files from /etc/sudoers.d (the # here does not mean a comment)
#includedir /etc/sudoers.d
</pre>
</font>

<h5>3.8 Putting it all together</h5>

<p>I agree, that was a lengthy setup process. But with the configuration above in place, 
we will now see what it takes to fire a ceph storage cluster up. Hold your breath now, 
since just typing terraform apply does it. Really! Yes that is easy. Not just that, to 
bring down the cluster, just type terraform destroy, and to look at the cluster 
attributes, type terraform show. One can create any number of ceph storage clusters at 
one go; replicate, and recreate it any number of times. So, if one wants to expand the 
number of ceph monitors, then update the count attribute to your liking, similar to the 
rest of the VMs in the cluster. And not to forget, Terraform also lets you setup your 
local environment based on the cluster properties through their local-exec provisioner. 
The combination seems to get just too exciting, and the options endless.</p>

<h4>4 Conclusion</h4>

<p>Terraform and Saltstack both have various functionalities that intersect. But the 
above case study has enabled us to understand the power those tools bring to the table 
together. Specifying infrastructure and its dependencies not just as a specification, 
but allowing it to be reproducible anywhere is truly a marvel. Cloud Technologies and 
myraid tools that are emerging in the horizon are truly redefining the way of software 
development and deployment lifecycles. A marvel indeed!</p>


<p><b>References</b></p>


<p>[1] HashiCorp,<a href="https://hashicorp.com/"> https://hashicorp.com </a></p>


<p>[2] Vagrant from HashiCorp,<a href="https://www.vagrantup.com/"> https://www.vagrantup.com</a></p>


<p>[3] Terraform from HashiCorp Inc.,<a href="https://terraform.io/docs/index.html"> https://terraform.io/docs/index.html</a></p>


<p>[4] SaltStack Documentation, <a href="https://docs.saltstack.com/en/latest/contents.html">https://docs.saltstack.com/en/latest/contents.html</a></p>


<p>[5] Ceph Storage Cluster, <a href="http://ceph.com/">http://ceph.com</a></p>


<p>[6] Ceph Storage Cluster Setup,<a href="http://docs.ceph.com/docs/master/start/"> http://docs.ceph.com/docs/master/start/</a></p>


<p>[7] DigitalOcean Personal Access Token,<a href="https://cloud.digitalocean.com/settings/applications#access-tokens">https://cloud.digitalocean.com/settings/applications#access-tokens</a></p>


<p>[8] DigitalOcean SSH Key Registration,<a href="https://developers.digitalocean.%20com/documentation/v2/#create-a-new-key">https://developers.digitalocean.com/documentation/v2/#create-a-new-key</a></p>


<p><b>This blog was the winning entry of the MSys Blogging Championship 2015.</b></p>



<a name="TerraformVsVagrant"></a>
<h3 class="general">"Terraform" Infrastructure: New Tool From Vagrant's Creators 
<a href="https://www.infoq.com/news/2014/08/terraform" target="_b">(Source 
Origin)</a></h3>
    by <a href="https://www.infoq.com/profile/Jo%C3%A3o-Miranda">Joao Miranda</a>
    on   Aug 04, 2014   <em>|</em>


<a id="noOfComments" title="" href="#theCommentsSection" class="comments_like">
   <span class="comments_counts"><span class="nr">Discuss</span></span></a>

<p><a href="http://www.terraform.io/" target="_b">Terraform</a> is a 
<a href="http://www.hashicorp.com/blog/terraform.html" target="_b">new tool</a> to 
build, change and version infrastructure, such as VMs, network switches or containers. 
It comes from the <a href="http://www.hashicorp.com/" target="_b">creators</a> of 
<a href="http://www.vagrantup.com/" target="_b">Vagrant</a>, the popular tool for 
managing development environments.</p>

<p>Terraform's main selling point is its ability to combine and compose different 
service providers resources in a declarative and agnostic way.  <a target="_b" 
href="http://www.terraform.io/docs/providers/aws/index.html">Amazon Web Services</a> 
(AWS), <a href="http://www.terraform.io/docs/providers/do/index.html" 
target="_b">DigitalOcean</a> or <a target="_b" 
href="http://www.terraform.io/docs/providers/cloudflare/index.html">Cloudflare</a> are 
examples of service providers.</p>

<p>Terraform uses a declarative language to describe resources' configurations. A 
resource can be many things, such as: an <a  target="_b" 
href="http://www.terraform.io/docs/providers/aws/r/instance.html">AWS EC2 instance</a>; 
a <a href="http://www.terraform.io/docs/providers/heroku/r/app.html" target="_b">Heroku 
App</a> or a <a href="http://www.terraform.io/docs/providers/cloudflare/r/record.html" 
target="_b">Cloudflare record</a>. Since Terraform is agnostic to service providers, 
there is the concept of <a href="http://www.terraform.io/docs/providers/index.html" 
target="_b">providers</a>. Providers translate Terraform configurations to low-level 
API interactions with the service providers.</p>

<p>Terraform works at a lower level than configuration management tools like 
<a href="http://puppetlabs.com/" target="_b">Puppet</a> or <a  target="_b" 
href="http://www.getchef.com/">Chef</a>.  These tools work on existing resources, while 
Terraform builds those nodes. Using Terraforms's parlance, configuration management 
tools are <a href="http://www.terraform.io/docs/provisioners/index.html" 
target="_b">provisioners</a>, hence responsible for initializing a resource.</p>

<p>The <a href="http://www.terraform.io/docs/internals/lifecycle.html" 
target="_b">resource lifecycle</a> ensures that any change is safe and explicit by 
generating a change <a href="http://www.terraform.io/docs/commands/plan.html" 
target="_b">plan</a>. Before <a href="http://www.terraform.io/docs/commands/apply.html" 
target="_b">applying</a> the plan to change the infrastructure, the user has the 
opportunity to validate it.</p>

<p>Hashicorp decided to create a <a  target="_b" 
href="http://www.terraform.io/docs/configuration/syntax.html">new configuration 
syntax</a>, with the express aim of making the configurations as readable as possible. 
An alternate JSON-based syntax is also provided. There are several configuration 
<a href="http://www.terraform.io/intro/examples/index.html" target="_b">examples</a> in 
the documentation. The basic structure looks like this:</p>

<font size=5>
<pre>
<code>
resource "digitalocean_droplet" "web" {
   name = "tf-web"
   size = "512mb"
   image = "centos-5-8-x32"
   region = "sfo1"
}
</code>
</pre>
</font>

<p>This block declares a resource of type "digitalocean_droplet" named "web". Everything 
inside the brackets is key-value configurations for the resource.</p>

<p>Since Terraform might be seen as overlapping with several classes of tools, there is 
a <a href="http://www.terraform.io/intro/vs/index.html" target="_b">section</a> on the 
tool's documentation that compares it with other software.</p>

<p>InfoQ talked with Mitchell Hashimoto, Hashicorp's founder and Vagrant creator, to 
learn more about this new tool.</p>

<p><strong>InfoQ: Terraform uses a custom configuration syntax, justified by the need 
to be as readable as possible. Do you think that the new syntax justifies the learning 
cost? Why do you consider that languages like YAML aren't enough?</strong></p>

<blockquote>
<p><strong>Mitchell:</strong> Creating our own configuration language was not a 
decision we made lightly.</p>

<p>Terraform is our 5th tool, and the 5th one that needs to be configured in some way. 
We've transitioned from Ruby (Vagrant) to JSON (Packer, Consul), and we've learned a 
lot along the way. The general thing we learned is that people want a human-readable 
and modifiable language to configure things, and yet with our tools they also want to 
be able to configure with a machine since we try to build tools that can be 
unix-friendly.</p>

<p>JSON is a decent balance, and has worked okay. The main downsides is that there are 
no comments and the format is pretty verbose for complex structures for a human.</p>

<p>I've personally never been a fan of YAML. I've tried reading the spec once and it 
seemed to simply do too much for my liking, and I've found that beginners never quite 
understand the structure they're creating.</p>

<p>As a result, we decided to build our own configuration language. We decided to take 
a different approach: human-readable and editable primary format, but JSON as the 
interoperability layer. Time will tell how this works, but so far we've had no 
complaints.</p>
</blockquote>

<p><strong>InfoQ: You choose to use explicit dependencies declarations for resources. 
Since the debate on explicit vs implicit declarations is a hot one, can you share your 
view on this?</strong></p>

<blockquote>
<p><strong>Mitchell:</strong> Actually, Terraform dependencies are all implicit, with 
optional explicit dependencies.</p>

<p>The dependencies are inferred based on referencing variables in other resources. 
If you aren't referencing an attribute of another resource, there likely isn't a 
dependency, and Terraform assumes as much. However, you can use the 
<code>depends_on</code> parameter for explicit dependencies if you want.</p>
</blockquote>

<p><strong>InfoQ: Hashicorp has already released five different products. What is the 
vision that has been guiding you in building these products? How do your products 
complement each other in that vision?</strong></p>

<blockquote>
<p><strong>Mitchell:</strong> I'm glad you asked this. Many people ask if we're just 
solving random problems, or if there is a grand vision. And the reality is that there 
really is a grand vision behind all of our tools.</p>

<p>The guiding vision is really to make the entire dev and ops lifecycle an easier, more 
scalable process. Vagrant makes development easier, Packer makes base software easier, 
Serf/Consul make service discovery and configuration easier, and now Terraform improves 
provisioning the resources to run software. And all of our tools are designed to work 
with any technology choices underneath: physical machines, virtual machines, containers, 
any language, etc.</p>

<p>There are more products in develop and on the way. We work on multiple new products 
in parallel these days.</p>
</blockquote>

<p><strong>InfoQ: There has been some <a target="_b" 
href="http://www.drdobbs.com/tools/just-let-me-code/240168735">discussion</a> about 
how all the new tools around ALM and DevOps are starting to become <a  target="_b" 
href="http://jeffknupp.com/blog/2014/04/15/how-devops-is-killing-the-developer/">detrimental</a> 
to the typical developer. What is your take on this discussion?</strong></p>

<blockquote>
<p><strong>Mitchell:</strong> We're building tools to solve technical problems. We have 
our own vision of how workflow should exist within an organization but we haven't really 
started prescribing that yet.</p>

<p>I do admit that there is an explosion of DevOps tools that make for a confusing 
ecosystem. I think that the expansion is necessary before a contraction and 
simplification.</p>
</blockquote>

<a name="SaltStackGrains"></a>
<h3>Grains 
<a  href="https://docs.saltstack.com/en/latest/topics/grains/index.html" 
target="_b">(Source Origin)</a></h3>

<p>Salt comes with an interface to derive information about the underlying system. This 
is called the grains interface, because it presents salt with grains of information. 
Grains are collected for the operating system, domain name, IP address, kernel, OS type, 
memory, and many other system properties.</p>

<p>The grains interface is made available to Salt modules and components so that the 
right salt minion commands are automatically available on the right systems.</p>

<p>Grain data is relatively static, though if system information changes (for example, 
if network settings are changed), or if a new value is assigned to a custom grain, 
grain data is refreshed.</p>

<p>Note: Grains resolve to lowercase letters. For example, <code>FOO</code>, and 
<code>foo</code>target the same grain.</p>

<h4>Listing Grains<a href="#listing-grains" title="Permalink to this headline"></a></h4>

<p>Available grains can be listed by using the 'grains.ls' module:</p>

<pre>
------------------------------------------------------------------------------------
salt '*' grains.ls
------------------------------------------------------------------------------------
</pre>

<p>Grains data can be listed by using the 'grains.items' module:</p>

<pre>
------------------------------------------------------------------------------------
salt '*' grains.items
------------------------------------------------------------------------------------
</pre>

<h4>Grains in the Minion Config<a href="#grains-in-the-minion-config" title="Permalink 
to this headline"></a></h4>

<p>Grains can also be statically assigned within the minion configuration file. Just add 
the option <a href="https://docs.saltstack.com/en/latest/ref/configuration/minion.html#std:conf_minion-grains"><code>grains</code></a> 
and pass options to it:</p>

<pre>
------------------------------------------------------------------------------------
grains:
  roles:
    - webserver
    - memcache
  deployment: datacenter4
  cabinet: 13
  cab_u: 14-15
------------------------------------------------------------------------------------
</pre>

<p>Then status data specific to your servers can be retrieved via Salt, or used inside 
of the State system for matching. It also makes targeting, in the case of the example 
above, simply based on specific data about your deployment.</p>

<h4>Grains in /etc/salt/grains<a href="#grains-in-etc-salt-grains" title="Permalink to 
this headline"></a></h4>

<p>If you do not want to place your custom static grains in the minion config file, you 
can also put them in <code>/etc/salt/grains</code> on the minion. They are configured 
in the same way as in the above example, only without a top-level <code>grains:</code> 
key:</p>

<pre>
------------------------------------------------------------------------------------
roles:
  - webserver
  - memcache
deployment: datacenter4
cabinet: 13
cab_u: 14-15
------------------------------------------------------------------------------------
</pre>

<h4>Matching Grains in the Top File<a href="#matching-grains-in-the-top-file" 
title="Permalink to this headline"></a></h4>

<p>With correctly configured grains on the Minion, the <a href="https://docs.saltstack.com/en/latest/glossary.html#term-top-file">top 
file</a> used in Pillar or during Highstate can be made very efficient. For example, 
consider the following configuration:</p>

<pre>
------------------------------------------------------------------------------------
'node_type:webserver':
  - match: grain
  - webserver

'node_type:postgres':
  - match: grain
  - postgres

'node_type:redis':
  - match: grain
  - redis

'node_type:lb':
  - match: grain
  - lb
------------------------------------------------------------------------------------
</pre>

<p>For this example to work, you would need to have defined the grain 'node_type' for 
the minions you wish to match. This simple example is nice, but too much of the code 
is similar. To go one step further, Jinja templating can be used to simplify the 
<a href="https://docs.saltstack.com/en/latest/glossary.html#term-top-file" 
target="_b">top file</a>.</p>

<pre>
------------------------------------------------------------------------------------
{% set the_node_type = salt['grains.get']('node_type', '') %}

{% if the_node_type %}
  'node_type:{{ the_node_type }}':
    - match: grain
    - {{ the_node_type }}
{% endif %}
------------------------------------------------------------------------------------
</pre>

<p>Using Jinja templating, only one match entry needs to be defined.</p>

<p>Note: The example above uses the <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.grains.html#salt.modules.grains.get" 
title="salt.modules.grains.get"><code>grains.get</code></a> function to account for 
minions which do not have the <code>node_type</code> grain set.</p>

<h4>Writing Grains<a href="#writing-grains" title="Permalink to this headline"></a></h4>

<p>The grains are derived by executing all of the "public" functions (i.e. those which 
do not begin with an underscore) found in the modules located in the Salt's core grains 
code, followed by those in any custom grains modules. The functions in a grains module 
must return a Python <a href="https://docs.python.org/2/library/stdtypes.html#typesmapping" 
title="(in Python v2.7)">dict</a>, where the dictionary keys are the names of grains, and
each key's value is that value for that grain.</p>

<p>Custom grains modules should be placed in a subdirectory named <code>_grains</code>
located under the <a href="https://docs.saltstack.com/en/latest/ref/configuration/master.html#std:conf_master-file_roots"><code>file_roots</code></a> 
specified by the master config file. The default path would be <code>/srv/salt/_grains</code>. Custom grains modules
will be distributed to the minions when <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.state.html#salt.modules.state.highstate" title="salt.modules.state.highstate"><code>state.highstate</code></a> is run, or by executing the
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.saltutil.html#salt.modules.saltutil.sync_grains" title="salt.modules.saltutil.sync_grains"><code>saltutil.sync_grains</code></a> or
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.saltutil.html#salt.modules.saltutil.sync_all" title="salt.modules.saltutil.sync_all"><code>saltutil.sync_all</code></a> functions.</p>

<p>Grains modules are easy to write, and (as noted above) only need to return a
dictionary. For example:</p>

<pre>
------------------------------------------------------------------------------------
def yourfunction():
     # initialize a grains dictionary
     grains = {}
     # Some code for logic that sets grains like
     grains['yourcustomgrain'] = True
     grains['anothergrain'] = 'somevalue'
     return grains
------------------------------------------------------------------------------------
</pre>

<p>The name of the function does not matter and will not factor into the grains
data at all; only the keys/values returned become part of the grains.</p>


<h4>When to Use a Custom Grain<a href="#when-to-use-a-custom-grain"></a></h4>

<p>Before adding new grains, consider what the data is and remember that grains should 
(for the most part) be static data.</p>

<p>If the data is something that is likely to change, consider using <a href="https://docs.saltstack.com/en/latest/topics/pillar/index.html#pillar">Pillar</a> 
or an execution module instead. If it's a simple set of key/value pairs, pillar is a 
good match. If compiling the information requires that system commands be run, then 
putting this information in an execution module is likely a better idea.</p>

<p>Good candidates for grains are data that is useful for targeting minions in the
<a href="https://docs.saltstack.com/en/latest/ref/states/top.html#states-top">top 
file</a> or the Salt CLI. The name and data structure of the grain should be designed 
to support many platforms, operating systems or applications. Also, keep in mind that 
Jinja templating in Salt supports referencing pillar data as well as invoking functions 
from execution modules, so there's no need to place information in grains to make it 
available to Jinja templates. For example:</p>

<pre>
------------------------------------------------------------------------------------
...
...
{{ salt['module.function_name']('argument_1', 'argument_2') }}
{{ pillar['my_pillar_key'] }}
...
...
------------------------------------------------------------------------------------
</pre>

<p>Warning</p>

<p>Custom grains will not be available in the top file until after the first
<a href="https://docs.saltstack.com/en/latest/topics/tutorials/states_pt1.html#running-highstate">highstate</a>. 
To make custom grains available on a minion's first highstate, it is recommended to 
use <a href="https://docs.saltstack.com/en/latest/topics/reactor/index.html#minion-start-reactor">this 
example</a> to ensure that the custom grains are synced when the minion starts.</p>

<h4>Loading Custom Grains<a href="#loading-custom-grains"></a></h4>

<p>If you have multiple functions specifying grains that are called from a 
<code>main</code> function, be sure to prepend grain function names with an underscore. 
This prevents Salt from including the loaded grains from the grain functions in the final
grain data structure. For example, consider this custom grain file:</p>

<pre>
------------------------------------------------------------------------------------
#!/usr/bin/env python
def _my_custom_grain():
    my_grain = {'foo': 'bar', 'hello': 'world'}
    return my_grain


def main():
    # initialize a grains dictionary
    grains = {}
    grains['my_grains'] = _my_custom_grain()
    return grains
------------------------------------------------------------------------------------
</pre>

<p>The output of this example renders like so:</p>

<pre>
------------------------------------------------------------------------------------
# salt-call --local grains.items
local:
    ----------
    &lt;Snipped for brevity&gt;
    my_grains:
        ----------
        foo:
            bar
        hello:
            world
------------------------------------------------------------------------------------
</pre>

<p>However, if you don't prepend the <code>my_custom_grain</code> function with an 
underscore, the function will be rendered twice by Salt in the items output: once for 
the <code>my_custom_grain</code> call itself, and again when it is called in the 
<code>main</code> function:</p>

<pre>
------------------------------------------------------------------------------------
# salt-call --local grains.items
local:
----------
    &lt;Snipped for brevity&gt;
    foo:
        bar
    &lt;Snipped for brevity&gt;
    hello:
        world
    &lt;Snipped for brevity&gt;
    my_grains:
        ----------
        foo:
            bar
        hello:
            world
------------------------------------------------------------------------------------
</pre>

<h4>Precedence<a href="#precedence"></a></h4>

<p>Core grains can be overridden by custom grains. As there are several ways of defining 
custom grains, there is an order of precedence which should be kept in mind when 
defining them. The order of evaluation is as follows:</p>

<ol>
<li>Core grains.</li>
<li>Custom grains in <code>/etc/salt/grains</code>.</li>
<li>Custom grains in <code>/etc/salt/minion</code>.</li>
<li>Custom grain modules in <code>_grains</code> directory, synced to minions.</li>
</ol>

<p>Each successive evaluation overrides the previous ones, so any grains defined by 
custom grains modules synced to minions that have the same name as a core grain will 
override that core grain. Similarly, grains from <code>/etc/salt/minion</code> override 
both core grains and custom grain modules, and grains in <code>_grains</code> will 
override <em>any</em> grains of the same name.</p>

<h4>Examples of Grains<a href="#examples-of-grains"></a></h4>

<p>The core module in the grains package is where the main grains are loaded by the Salt 
minion and provides the principal example of how to write grains:</p>

<p><a href="https://github.com/saltstack/salt/blob/develop/salt/grains/core.py">https://github.com/saltstack/salt/blob/develop/salt/grains/core.py</a></p>

<h4>Syncing Grains<a href="#syncing-grains"></a></h4>

<p>Syncing grains can be done a number of ways, they are automatically synced when
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.state.html#salt.modules.state.highstate" title="salt.modules.state.highstate"><code>state.highstate</code></a> is called, or (as noted
above) the grains can be manually synced and reloaded by calling the
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.saltutil.html#salt.modules.saltutil.sync_grains" title="salt.modules.saltutil.sync_grains"><code>saltutil.sync_grains</code></a> or
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.saltutil.html#salt.modules.saltutil.sync_all" title="salt.modules.saltutil.sync_all"><code>saltutil.sync_all</code></a> functions.</p>

<a name="SaltstackPillar"></a>
<h3>Storing Static Data in the Pillar 
<a href="https://docs.saltstack.com/en/latest/topics/pillar/index.html" 
target="_b">(Source Origin)</a></h3>


<p>Pillar is an interface for Salt designed to offer global values that can be 
distributed to minions. Pillar data is managed in a similar way as the Salt State 
Tree.</p>


<p>Pillar was added to Salt in version 0.9.8</p>

<p>Storing sensitive data</p>


<p>Pillar data is compiled on the master. Additionally, pillar data for a given minion 
is only accessible by the minion for which it is targeted in the pillar configuration. 
This makes pillar useful for storing sensitive data specific to a particular minion.</p>

<h4>Declaring the Master Pillar</h4>


<p>The Salt Master server maintains a 
<a href="https://docs.saltstack.com/en/latest/ref/configuration/master.html#std:conf_master-pillar_roots"><code>pillar_roots</code></a> 
setup that matches the structure of the 
<a href="https://docs.saltstack.com/en/latest/ref/configuration/master.html#std:conf_master-file_roots"><code>file_roots</code></a> 
used in the Salt file server. Like 
<a href="https://docs.saltstack.com/en/latest/ref/configuration/master.html#std:conf_master-file_roots"><code>file_roots</code></a>, the 
<a href="https://docs.saltstack.com/en/latest/ref/configuration/master.html#std:conf_master-pillar_roots"><code>pillar_roots</code></a> 
option mapps environments to directories. The pillar data is then mapped to minions 
based on matchers in a top file which is laid out in the same way as the state top file. 
Salt pillars can use the same matcher types as the standard 
<a href="https://docs.saltstack.com/en/latest/ref/states/top.html#states-top">top
file</a>.</p>


<p>conf_master:<cite>pillar_roots</cite> is configured just like 
<a href="https://docs.saltstack.com/en/latest/ref/configuration/master.html#std:conf_master-file_roots"><code>file_roots</code></a>.
For example:</p>


<pre>
---------------------------------------------------------------------------------------
pillar_roots:
  base:
    - /srv/pillar
---------------------------------------------------------------------------------------
</pre>


<p>This example configuration declares that the base environment will be located in the 
<code>/srv/pillar</code> directory. It must not be in a subdirectory of the state 
tree.</p>


<p>The top file used matches the name of the top file used for States, and has the same 
structure:</p>


<p><code>/srv/pillar/top.sls</code></p>


<pre>
---------------------------------------------------------------------------------------
base:
  '*':
    - packages
---------------------------------------------------------------------------------------
</pre>


<p>In the above top file, it is declared that in the <code>base</code> environment, the
glob matching all minions will have the pillar data found in the <code>packages</code>
pillar available to it. Assuming the <code>pillar_roots</code> value of 
<code>/srv/pillar</code> taken from above, the <code>packages</code> pillar would be 
located at <code>/srv/pillar/packages.sls</code>.</p>


<p>Any number of matchers can be added to the base environment. For example, here is an 
expanded version of the Pillar top file stated above:</p>


<p>/srv/pillar/top.sls:</p>


<pre>
---------------------------------------------------------------------------------------
base:
  '*':
    - packages
  'web*':
    - vim
---------------------------------------------------------------------------------------
</pre>


<p>In this expanded top file, minions that match <code>web*</code> will have access to 
the <code>/srv/pillar/packages.sls</code> file, as well as the 
<code>/srv/pillar/vim.sls</code> file.</p>


<p>Another example shows how to use other standard top matching types to deliver 
specific salt pillar data to minions with different properties.</p>


<p>Here is an example using the <code>grains</code> matcher to target pillars to minions
by their <code>os</code> grain:</p>


<pre>
---------------------------------------------------------------------------------------
dev:
  'os:Debian':
    - match: grain
    - servers
---------------------------------------------------------------------------------------
</pre>


<p><code>/srv/pillar/packages.sls</code></p>


<pre>
---------------------------------------------------------------------------------------
{% if grains['os'] == 'RedHat' %}
apache: httpd
git: git
{% elif grains['os'] == 'Debian' %}
apache: apache2
git: git-core
{% endif %}

company: Foo Industries
---------------------------------------------------------------------------------------
</pre>



<p>Important</p>


<p>See <a href="https://docs.saltstack.com/en/latest/faq.html#faq-grain-security">Is 
Targeting using Grain Data Secure?</a> for important security information.</p>


<p>The above pillar sets two key/value pairs. If a minion is running RedHat, then the 
<code>apache</code> key is set to <code>httpd</code> and the <code>git</code> key is 
set to the value of <code>git</code>. If the minion is running Debian, those values 
are changed to <code>apache2</code> and <code>git-core</code> respectively. All minions 
that have this pillar targeting to them via a top file will have the key of 
<code>company</code> with a value of <code>Foo Industries</code>.</p>


<p>Consequently this data can be used from within modules, renderers, State SLS files, 
and more via the shared pillar 
<a href="https://docs.python.org/2/library/stdtypes.html#typesmapping"  
target="_b">dict</a>:</p>


<pre>
---------------------------------------------------------------------------------------
apache:
  pkg.installed:
    - name: {{ pillar['apache'] }}
---------------------------------------------------------------------------------------
</pre>


<pre>
---------------------------------------------------------------------------------------
git:
  pkg.installed:
    - name: {{ pillar['git'] }}
---------------------------------------------------------------------------------------
</pre>


<p>Finally, the above states can utilize the values provided to them via Pillar. All 
pillar values targeted to a minion are available via the 'pillar' dictionary. As seen 
in the above example, Jinja substitution can then be utilized to access the keys and 
values in the Pillar dictionary.</p>


<p>Note that you cannot just list key/value-information in <code>top.sls</code>. Instead,
target a minion to a pillar file and then list the keys and values in the pillar. Here 
is an example top file that illustrates this point:</p>


<pre>
---------------------------------------------------------------------------------------
base:
  '*':
     - common_pillar
---------------------------------------------------------------------------------------
</pre>


<p>And the actual pillar file at '/srv/pillar/common_pillar.sls':</p>


<pre>
---------------------------------------------------------------------------------------
foo: bar
boo: baz
---------------------------------------------------------------------------------------
</pre>


<h4>Pillar Namespace Flattening</h4>


<p>The separate pillar SLS files all merge down into a single dictionary of key-value 
pairs. When the same key is defined in multiple SLS files, this can result in unexpected 
behavior if care is not taken to how the pillar SLS files are laid out.</p>


<p>For example, given a <code>top.sls</code> containing the following:</p>


<pre>
---------------------------------------------------------------------------------------
base:
  '*':
    - packages
    - services
---------------------------------------------------------------------------------------
</pre>


<p>with <code>packages.sls</code> containing:</p>


<pre>
---------------------------------------------------------------------------------------
bind: bind9
---------------------------------------------------------------------------------------
</pre>


<p>and <code>services.sls</code> containing:</p>


<pre>
---------------------------------------------------------------------------------------
bind: named
---------------------------------------------------------------------------------------
</pre>


<p>Then a request for the <code>bind</code> pillar key will only return 
<code>named</code>. The <code>bind9</code> value will be lost, because 
<code>services.sls</code> was evaluated later.</p>



<p>Note</p>


<p>Pillar files are applied in the order they are listed in the top file. Therefore 
conflicting keys will be overwritten in a 'last one wins' manner! For example, in the 
above scenario conflicting key values in <code>services</code> will overwrite those 
in <code>packages</code> because it's at the bottom of the list.</p>


<p>It can be better to structure your pillar files with more hierarchy. For example the 
<code>package.sls</code> file could be configured like so:</p>


<pre>
---------------------------------------------------------------------------------------
packages:
  bind: bind9
---------------------------------------------------------------------------------------
</pre>


<p>This would make the <code>packages</code> pillar key a nested dictionary containing a
<code>bind</code> key.</p>


<h4>Pillar Dictionary Merging</h4>


<p>If the same pillar key is defined in multiple pillar SLS files, and the keys in
both files refer to nested dictionaries, then the content from these dictionaries 
will be recursively merged.</p>


<p>For example, keeping the <code>top.sls</code> the same, assume the following
modifications to the pillar SLS files:</p>


<p><code>packages.sls</code>:</p>


<pre>
---------------------------------------------------------------------------------------
bind:
  package-name: bind9
  version: 9.9.5
---------------------------------------------------------------------------------------
</pre>


<p><code>services.sls</code>:</p>


<pre>
---------------------------------------------------------------------------------------
bind:
  port: 53
  listen-on: any
---------------------------------------------------------------------------------------
</pre>


<p>The resulting pillar dictionary will be:</p>


<pre>
---------------------------------------------------------------------------------------
$ salt-call pillar.get bind
local:
    ----------
    listen-on:
        any
    package-name:
        bind9
    port:
        53
    version:
        9.9.5
---------------------------------------------------------------------------------------
</pre>


<p>Since both pillar SLS files contained a <code>bind</code> key which contained a nested
dictionary, the pillar dictionary's <code>bind</code> key contains the combined contents
of both SLS files' <code>bind</code> keys.</p>


<h4>Including Other Pillars</h4>



<p>New in version 0.16.0.</p>


<p>Pillar SLS files may include other pillar files, similar to State files. Two
syntaxes are available for this purpose. The simple form simply includes the
additional pillar as if it were part of the same file:</p>


<pre>
---------------------------------------------------------------------------------------
include:
  - users
---------------------------------------------------------------------------------------
</pre>


<p>The full include form allows two additional options -- passing default values
to the templating engine for the included pillar file as well as an optional
key under which to nest the results of the included pillar:</p>


<pre>
---------------------------------------------------------------------------------------
include:
  - users:
      defaults:
          sudo: ['bob', 'paul']
      key: users
---------------------------------------------------------------------------------------
</pre>


<p>With this form, the included file (users.sls) will be nested within the 'users'
key of the compiled pillar. Additionally, the 'sudo' value will be available
as a template variable to users.sls.</p>

<h4>In-Memory Pillar Data vs. On-Demand Pillar Data</h4>


<p>Since compiling pillar data is computationally expensive, the minion will
maintain a copy of the pillar data in memory to avoid needing to ask the master
to recompile and send it a copy of the pillar data each time pillar data is
requested. This in-memory pillar data is what is returned by the
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.item" target="_b"><code>pillar.item</code></a>, 
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.get"  target="_b"><code>pillar.get</code></a>, and 
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.raw" target="_b"><code>pillar.raw</code></a>
functions.</p>


<p>Also, for those writing custom execution modules, or contributing to Salt's
existing execution modules, the in-memory pillar data is available as the
<code>__pillar__</code> dunder dictionary.</p>


<p>The in-memory pillar data is generated on minion start, and can be refreshed
using the <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.saltutil.html#salt.modules.saltutil.refresh_pillar" target="_b"><code>saltutil.refresh_pillar</code></a> function:</p>


<pre>
---------------------------------------------------------------------------------------
salt '*' saltutil.refresh_pillar
---------------------------------------------------------------------------------------
</pre>


<p>This function triggers the minion to asynchronously refresh the in-memory
pillar data and will always return <code>None</code>.</p>


<p>In contrast to in-memory pillar data, certain actions trigger pillar data to be
compiled to ensure that the most up-to-date pillar data is available. These
actions include:</p>

<ul>
  <li>Running states</li>
  <li>Running <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.items" target="_b"><code>pillar.items</code></a></li>
</ul>


<p>Performing these actions will <em>not</em> refresh the in-memory pillar data. So, if
pillar data is modified, and then states are run, the states will see the
updated pillar data, but <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.item" target="_b"><code>pillar.item</code></a>,
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.get" target="_b"><code>pillar.get</code></a>, and <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.raw" target="_b"><code>pillar.raw</code></a> will not see this data unless refreshed using
<a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.saltutil.html#salt.modules.saltutil.refresh_pillar" target="_b"><code>saltutil.refresh_pillar</code></a>.</p>


<h4>How Pillar Environments Are Handled</h4>


<p>When multiple pillar environments are used, the default behavior is for the
pillar data from all environments to be merged together. The pillar dictionary
will therefore contain keys from all configured environments.</p>


<p>The <a href="https://docs.saltstack.com/en/latest/ref/configuration/minion.html#std:conf_minion-pillarenv"><code>pillarenv</code></a> minion config option can be used to force the
minion to only consider pillar configuration from a single environment. This
can be useful in cases where one needs to run states with alternate pillar
data, either in a testing/QA environment or to test changes to the pillar data
before pushing them live.</p>


<p>For example, assume that the following is set in the minion config file:</p>


<pre>
---------------------------------------------------------------------------------------
pillarenv: base
---------------------------------------------------------------------------------------
</pre>


<p>This would cause that minion to ignore all other pillar environments besides
<code>base</code> when compiling the in-memory pillar data. Then, when running states,
the <code>pillarenv</code> CLI argument can be used to override the minion's
<a href="https://docs.saltstack.com/en/latest/ref/configuration/minion.html#std:conf_minion-pillarenv"><code>pillarenv</code></a> config value:</p>


<pre>
---------------------------------------------------------------------------------------
salt '*' state.apply mystates pillarenv=testing
---------------------------------------------------------------------------------------
</pre>


<p>The above command will run the states with pillar data sourced exclusively from
the <code>testing</code> environment, without modifying the in-memory pillar data.</p>



<p>Note</p>


<p>When running states, the <code>pillarenv</code> CLI option does not require a
<a href="https://docs.saltstack.com/en/latest/ref/configuration/minion.html#std:conf_minion-pillarenv"><code>pillarenv</code></a> option to be set in the minion config file. When
<a href="https://docs.saltstack.com/en/latest/ref/configuration/minion.html#std:conf_minion-pillarenv"><code>pillarenv</code></a> is left unset, as mentioned above all configured
environments will be combined. Running states with <code>pillarenv=testing</code> in
this case would still restrict the states' pillar data to just that of the
<code>testing</code> pillar environment.</p>


<h4>Viewing Pillar Data</h4>


<p>To view pillar data, use the <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#module-salt.modules.pillar" target="_b"><code>pillar</code></a> execution
module. This module includes several functions, each of them with their own
use. These functions include:</p>

<ul>
  <li><a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.item" target="_b"><code>pillar.item</code></a> - Retrieves the value of
one or more keys from the <a href="#pillar-in-memory">in-memory pillar datj</a>.</li>
  <li><a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.items" target="_b"><code>pillar.items</code></a> - Compiles a fresh pillar
dictionary and returns it, leaving the <a href="#pillar-in-memory">in-memory pillar data</a> untouched. If pillar keys are passed to this function
however, this function acts like <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.item" target="_b"><code>pillar.item</code></a> and returns their values from the <a href="#pillar-in-memory">in-memory
pillar data</a>.</li>
  <li><a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.raw" target="_b"><code>pillar.raw</code></a> - Like <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.items" target="_b"><code>pillar.items</code></a>, it returns the entire pillar dictionary, but
from the <a href="#pillar-in-memory">in-memory pillar data</a> instead of compiling
fresh pillar data.</li>
  <li><a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.get" target="_b"><code>pillar.get</code></a> - Described in detail below.</li>
</ul>

<h4>The <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.get"  target="_b"><code>pillar.get</code></a> Function</h4>



<p>New in version 0.14.0.</p>


<p>The <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.get" target="_b"><code>pillar.get</code></a> function works much in the same
way as the <code>get</code> method in a python dict, but with an enhancement: nested
dictonaries can be traversed using a colon as a delimiter.</p>


<p>If a structure like this is in pillar:</p>


<pre>
---------------------------------------------------------------------------------------
foo:
  bar:
    baz: qux
---------------------------------------------------------------------------------------
</pre>


<p>Extracting it from the raw pillar in an sls formula or file template is done
this way:</p>


<pre>
---------------------------------------------------------------------------------------
{{ pillar['foo']['bar']['baz'] }}
---------------------------------------------------------------------------------------
</pre>


<p>Now, with the new <a href="https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.pillar.html#salt.modules.pillar.get" target="_b"><code>pillar.get</code></a> 
function the data can be safely gathered and a default can be set, allowing the template 
to fall back if the value is not available:</p>


<pre>
---------------------------------------------------------------------------------------
{{ salt['pillar.get']('foo:bar:baz', 'qux') }}
---------------------------------------------------------------------------------------
</pre>


<p>This makes handling nested structures much easier.</p>



<p>Note</p>


<p><code>pillar.get()</code> vs <code>salt['pillar.get']()</code></p>


<p>It should be noted that within templating, the <code>pillar</code> variable is just
a dictionary.  This means that calling <code>pillar.get()</code> inside of a template 
will just use the default dictionary <code>.get()</code> function which does not include 
the extra <code>:</code> delimiter functionality.  It must be called using the above 
syntax (<code>salt['pillar.get']('foo:bar:baz', 'qux')</code>) to get the salt function, 
instead of the default dictionary behavior.</p>


<h4>Setting Pillar Data at the Command Line</h4>


<p>Pillar data can be set at the command line like the following example:</p>


<pre>
---------------------------------------------------------------------------------------
salt '*' state.apply pillar='{"cheese": "spam"}'
---------------------------------------------------------------------------------------
</pre>


<p>This will add a pillar key of <code>cheese</code> with its value set to 
<code>spam</code>.</p>



<p>Note</p>


<p>Be aware that when sending sensitive data via pillar on the command-line that the 
publication containing that data will be received by all minions and will not be 
restricted to the targeted minions. This may represent a security concern in some 
cases.</p>


<h4>Master Config in Pillar</h4>


<p>For convenience the data stored in the master configuration file can be made 
available in all minion's pillars. This makes global configuration of services and 
systems very easy but may not be desired if sensitive data is stored in the master 
configuration. This option is disabled by default.</p>


<p>To enable the master config from being added to the pillar set
<code>pillar_opts</code> to <code>True</code> in the minion config file:</p>


<pre>
---------------------------------------------------------------------------------------
pillar_opts: True
---------------------------------------------------------------------------------------
</pre>


<h4>Minion Config in Pillar</h4>


<p>Minion configuration options can be set on pillars. Any option that you want to 
modify, should be in the first level of the pillars, in the same way you set the 
options in the config file. For example, to configure the MySQL root password to 
be used by MySQL Salt execution module, set the following pillar variable:</p>


<pre>
---------------------------------------------------------------------------------------
mysql.pass: hardtoguesspassword
---------------------------------------------------------------------------------------
</pre>

<h4>Master Provided Pillar Error</h4>


<p>By default if there is an error rendering a pillar, the detailed error is hidden 
and replaced with:</p>


<pre>
---------------------------------------------------------------------------------------
Rendering SLS 'my.sls' failed. Please see master log for details.
---------------------------------------------------------------------------------------
</pre>


<p>The error is protected because it's possible to contain templating data which would 
give that minion information it shouldn't know, like a password!</p>


<p>To have the master provide the detailed error that could potentially carry
protected data set <code>pillar_safe_render_error</code> to <code>False</code>:</p>


<pre>
---------------------------------------------------------------------------------------
pillar_safe_render_error: False
---------------------------------------------------------------------------------------
</pre>



<ul>
<li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html">Pillar Walkthrough</a>
  <ul>
    <li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html#setting-up-pillar">Setting Up Pillar</a>
      <ul>
        <li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html#more-complex-data">More Complex Data</a></li>
      </ul></li>
  <li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html#parameterizing-states-with-pillar">Parameterizing States With Pillar</a></li>
  <li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html#pillar-makes-simple-states-grow-easily">Pillar Makes Simple States Grow Easily</a></li>
  <li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html#setting-pillar-data-on-the-command-line">Setting Pillar Data on the Command Line</a></li>
  <li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html#more-on-pillar">More On Pillar</a></li>
  <li><a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html#minion-config-in-pillar">Minion Config in Pillar</a></li>
  </ul></li>
</ul>
</body></html>
