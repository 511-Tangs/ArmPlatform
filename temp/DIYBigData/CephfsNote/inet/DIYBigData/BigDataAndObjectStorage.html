<!DOCTYPE html>
<html lang="en">
  <head><title>Big Data, OpenStack and object storage: Size matters, people -- The 
               Register</title>
  </head>

<body data-pagetype="Story" data-pagenum="0">

<P><b>Table of Contents</b>

<OL>
  <LI><a href="#IntroBigData">Introduction to Big Data</a>
  <UL>
     <LI><a href="#IntroBigData">Introduction to Big Data</a>
     <LI><a href="#BigDataWithCephStorage">3 Ways Big Data Thrives with Ceph Storage</a>
     <LI><a href="#MonitoringCephCluster">What to Monitor in a Ceph Cluster</a>
     <LI><a href="#"></a>
  </UL>
  <LI><a href="#BigDataAndObjectStoragePart1">Big Data And Object Storage</a>
  <UL>
    <LI><a href="#HadoopAdoptionHype">Hadoop adoption hurt by hype</a>
    <LI><a href="#LittleSpendingOnBigData">little spending on Big Data ...</a>
  </UL>
  <LI><a href="#LargeDataVsBigData">Storing Large Data</a>
  <LI><a href="#NoSQLDMComparison">Comparison Of NoSQL Database Management Systems</a>
  <LI><a href="#"></a>
</OL>

<a name="IntroBigData"></a>
    <h3>An Introduction to Big Data Concepts and Terminology 
<a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-big-data-concepts-and-terminology">(Source Origin)</a>
    </h3>

    Posted September 28, 2016

    9.1k views

    
<a href="https://www.digitalocean.com/community/tags/scaling?type=tutorials">Scaling</a> 
<a href="https://www.digitalocean.com/community/tags/clustering?type=tutorials">Clustering</a> 
<a href="https://www.digitalocean.com/community/tags/big-data?type=tutorials">Big 
Data</a>
    

<h4 id="introduction">Introduction</h4>

<p><strong>Big data</strong> is a blanket term for the non-traditional strategies and 
technologies needed to gather, organize, process, and gather insights from large 
datasets.  While the problem of working with data that exceeds the computing power or 
storage of a single computer is not new, the pervasiveness, scale, and value of this 
type of computing has greatly expanded in recent years.</p>

<p>In this article, we will talk about big data on a fundamental level and define common 
concepts you might come across while researching the subject.  We will also take a 
high-level look at some of the processes and technologies currently being used in this 
space.</p>

<h3 id="what-is-big-data">What Is Big Data?</h3>

<p>An exact definition of "big data" is difficult to nail down because projects, 
vendors, practitioners, and business professionals use it quite differently.  With that 
in mind, generally speaking, <strong>big data</strong> is:</p>

<ul>
  <li>large datasets</li>
  <li>the category of computing strategies and technologies that are used to handle 
      large datasets</li>
</ul>

<p>In this context, "large dataset" means a dataset too large to reasonably process or 
store with traditional tooling or on a single computer.  This means that the common 
scale of big datasets is constantly shifting and may vary significantly from 
organization to organization.</p>

<h3 id="why-are-big-data-systems-different">Why Are Big Data Systems Different?</h3>

<p>The basic requirements for working with big data are the same as the requirements for 
working with datasets of any size.  However, the massive scale, the speed of ingesting 
and processing, and the characteristics of the data that must be dealt with at each 
stage of the process present significant new challenges when designing solutions.  The 
goal of most big data systems is to surface insights and connections from large volumes 
of heterogeneous data that would not be possible using conventional methods.</p>

<p>In 2001, Gartner's Doug Laney first presented what became known as the "three Vs of 
big data" to describe some of the characteristics that make big data different from 
other data processing:</p>

<h4 id="volume">Volume</h4>

<p>The sheer scale of the information processed helps define big data systems.  These 
datasets can be orders of magnitude larger than traditional datasets, which demands more 
thought at each stage of the processing and storage life cycle.</p>

<p>Often, because the work requirements exceed the capabilities of a single computer, 
this becomes a challenge of pooling, allocating, and coordinating resources from groups 
of computers.  Cluster management and algorithms capable of breaking tasks into smaller 
pieces become increasingly important.</p>

<h4 id="velocity">Velocity</h4>

<p>Another way in which big data differs significantly from other data systems is the 
speed that information moves through the system.  Data is frequently flowing into the 
system from multiple sources and is often expected to be processed in real time to gain 
insights and update the current understanding of the system.</p>

<p>This focus on near instant feedback has driven many big data practitioners away from 
a batch-oriented approach and closer to a real-time streaming system.  Data is constantly
 being added, massaged, processed, and analyzed in order to keep up with the influx of 
new information and to surface valuable information early when it is most relevant.  
These ideas require robust systems with highly available components to guard against 
failures along the data pipeline.</p>

<h4 id="variety">Variety</h4>

<p>Big data problems are often unique because of the wide range of both the sources 
being processed and their relative quality.</p>

<p>Data can be ingested from internal systems like application and server logs, from 
social media feeds and other external APIs, from physical device sensors, and from other 
providers.  Big data seeks to handle potentially useful data regardless of where it's 
coming from by consolidating all information into a single system.</p>

<p>The formats and types of media can vary significantly as well.  Rich media like 
images, video files, and audio recordings are ingested alongside text files, structured 
logs, etc.  While more traditional data processing systems might expect data to enter 
the pipeline already labeled, formatted, and organized, big data systems usually accept 
and store data closer to its raw state.  Ideally, any transformations or changes to the 
raw data will happen in memory at the time of processing.</p>

<h4 id="other-characteristics">Other Characteristics</h4>

<p>Various individuals and organizations have suggested expanding the original three Vs, 
though these proposals have tended to describe challenges rather than qualities of big 
data.  Some common additions are:</p>

<ul>
  <li><strong>Veracity</strong>: The variety of sources and the complexity of the 
      processing can lead to challenges in evaluating the quality of the data (and 
      consequently, the quality of the resulting analysis)</li>
  <li><strong>Variability</strong>: Variation in the data leads to wide variation in 
      quality.  Additional resources may be needed to identify, process, or filter low 
      quality data to make it more useful.</li>
  <li><strong>Value</strong>: The ultimate challenge of big data is delivering value.  
      Sometimes, the systems and processes in place are complex enough that using the 
      data and extracting actual value can become difficult.</li>
</ul>

<h3 id="what-does-a-big-data-life-cycle-look-like">What Does a Big Data Life Cycle 
Look Like?</h3>

<p>So how is data actually processed when dealing with a big data system?  While 
approaches to implementation differ, there are some commonalities in the strategies and 
software that we can talk about generally.  While the steps presented below might not 
be true in all cases, they are widely used.</p>

<p>The general categories of activities involved with big data processing are:</p>

<ul>
  <li>Ingesting data into the system</li>
  <li>Persisting the data in storage</li>
  <li>Computing and Analyzing data</li>
  <li>Visualizing the results</li>
</ul>

<p>Before we look at these four workflow categories in detail, we will take a moment to 
talk about <strong>clustered computing</strong>, an important strategy employed by most 
big data solutions.  Setting up a computing cluster is often the foundation for 
technology used in each of the life cycle stages.</p>

<h4 id="clustered-computing">Clustered Computing</h4>

<p>Because of the qualities of big data, individual computers are often inadequate for 
handling the data at most stages.  To better address the high storage and computational 
needs of big data, computer clusters are a better fit.</p>

<p>Big data clustering software combines the resources of many smaller machines, seeking 
to provide a number of benefits:</p>

<ul>
  <li><strong>Resource Pooling</strong>: Combining the available storage space to hold 
      data is a clear benefit, but CPU and memory pooling is also extremely important.  
      Processing large datasets requires large amounts of all three of these 
      resources.</li>
  <li><strong>High Availability</strong>: Clusters can provide varying levels of fault 
      tolerance and availability guarantees to prevent hardware or software failures 
      from affecting access to data and processing.  This becomes increasingly important 
      as we continue to emphasize the importance of real-time analytics.</li>
  <li><strong>Easy Scalability</strong>: Clusters make it easy to scale horizontally by 
      adding additional machines to the group.  This means the system can react to 
      changes in resource requirements without expanding the physical resources on a 
      machine.</li>
</ul>

<p>Using clusters requires a solution for managing cluster membership, coordinating 
resource sharing, and scheduling actual work on individual nodes.  Cluster membership 
and resource allocation can be handled by software like <strong>Hadoop's YARN</strong> 
(which stands for Yet Another Resource Negotiator) or <strong>Apache Mesos</strong>.</p>

<p>The assembled computing cluster often acts as a foundation which other software 
interfaces with to process the data.  The machines involved in the computing cluster are 
also typically involved with the management of a distributed storage system, which we 
will talk about when we discuss data persistence.</p>

<h4 id="ingesting-data-into-the-system">Ingesting Data into the System</h4>

<p>Data ingestion is the process of taking raw data and adding it to the system.  The 
complexity of this operation depends heavily on the format and quality of the data 
sources and how far the data is from the desired state prior to processing.</p>

<p>One way that data can be added to a big data system are dedicated ingestion tools.  
Technologies like <strong>Apache Sqoop</strong> can take existing data from relational 
databases and add it to a big data system.  Similarly, <strong>Apache Flume</strong> 
and <strong>Apache Chukwa</strong> are projects designed to aggregate and import 
application and server logs.  Queuing systems like <strong>Apache Kafka</strong> can 
also be used as an interface between various data generators and a big data system.  
Ingestion frameworks like <strong>Gobblin</strong> can help to aggregate and normalize 
the output of these tools at the end of the ingestion pipeline.</p>

<p>During the ingestion process, some level of analysis, sorting, and labelling usually 
takes place.  This process is sometimes called ETL, which stands for extract, transform, 
and load.  While this term conventionally refers to legacy data warehousing processes, 
some of the same concepts apply to data entering the big data system. Typical operations 
might include modifying the incoming data to format it, categorizing and labelling data, 
filtering out unneeded or bad data, or potentially validating that it adheres to certain 
requirements.</p>

<p>With those capabilities in mind, ideally, the captured data should be kept as raw as 
possible for greater flexibility further on down the pipeline.</p>

<h4 id="persisting-the-data-in-storage">Persisting the Data in Storage</h4>

<p>The ingestion processes typically hand the data off to the components that manage 
storage, so that it can be reliably persisted to disk.  While this seems like it would 
be a simple operation, the volume of incoming data, the requirements for availability, 
and the distributed computing layer make more complex storage systems necessary.</p>

<p>This usually means leveraging a distributed file system for raw data storage.  
Solutions like <strong>Apache Hadoop's HDFS</strong> filesystem allow large quantities 
of data to be written across multiple nodes in the cluster.  This ensures that the data 
can be accessed by compute resources, can be loaded into the cluster's RAM for in-memory 
operations, and can gracefully handle component failures.  Other distributed filesystems 
can be used in place of HDFS including <strong>Ceph</strong> and 
<strong>GlusterFS</strong>.</p>

<p>Data can also be imported into other distributed systems for more structured access.  
Distributed databases, especially NoSQL databases, are well-suited for this role because 
they are often designed with the same fault tolerant considerations and can handle 
heterogeneous data.  There are many different types of distributed databases to choose 
from depending on how you want to organize and present the data.  To learn more about 
some of the options and what purpose they best serve, read our <a target="_b" 
href="#NoSQLDMComparison">NoSQL comparison guide</a>.</p>

<h4 id="computing-and-analyzing-data">Computing and Analyzing Data</h4>

<p>Once the data is available, the system can begin processing the data to surface 
actual information.  The computation layer is perhaps the most diverse part of the 
system as the requirements and best approach can vary significantly depending on what 
type of insights desired.  Data is often processed repeatedly, either iteratively by a 
single tool or by using a number of tools to surface different types of insights.</p>

<p><strong>Batch processing</strong> is one method of computing over a large dataset.  
The process involves breaking work up into smaller pieces, scheduling each piece on an 
individual machine, reshuffling the data based on the intermediate results, and then 
calculating and assembling the final result. These steps are often referred to 
individually as splitting, mapping, shuffling, reducing, and assembling, or collectively 
as a distributed map reduce algorithm.  This is the strategy used by <strong>Apache 
Hadoop's MapReduce</strong>.  Batch processing is most useful when dealing with very 
large datasets that require quite a bit of computation.</p>

<p>While batch processing is a good fit for certain types of data and computation, other 
workloads require more <strong>real-time processing</strong>.  Real-time processing 
demands that information be processed and made ready immediately and requires the system 
to react as new information becomes available.  One way of achieving this is 
<strong>stream processing</strong>, which operates on a continuous stream of data 
composed of individual items.  Another common characteristic of real-time processors is 
in-memory computing, which works with representations of the data in the cluster's 
memory to avoid having to write back to disk.</p>

<p><strong>Apache Storm</strong>, <strong>Apache Flink</strong>, and <strong>Apache 
Spark</strong> provide different ways of achieving real-time or near real-time 
processing.  There are trade-offs with each of these technologies, which can affect 
which approach is best for any individual problem.  In general, real-time processing is 
best suited for analyzing smaller chunks of data that are changing or being added to the 
system rapidly.</p>

<p>The above examples represent computational frameworks.  However, there are many other 
ways of computing over or analyzing data within a big data system.  These tools 
frequently plug into the above frameworks and provide additional interfaces for 
interacting with the underlying layers.  For instance, <strong>Apache Hive</strong> 
provides a data warehouse interface for Hadoop, <strong>Apache Pig</strong> provides a 
high level querying interface, while SQL-like interactions with data can be achieved 
with projects like <strong>Apache Drill</strong>, <strong>Apache Impala</strong>, 
<strong>Apache Spark SQL</strong>, and <strong>Presto</strong>.  For machine learning, 
projects like <strong>Apache SystemML</strong>, <strong>Apache Mahout</strong>, and 
<strong>Apache Spark's MLlib</strong> can be useful.  For straight analytics programming 
that has wide support in the big data ecosystem, both <strong>R</strong> and 
<strong>Python</strong> are popular choices.</p>

<h4 id="visualizing-the-results">Visualizing the Results</h4>

<p>Due to the type of information being processed in big data systems, recognizing 
trends or changes in data over time is often more important than the values themselves.  
Visualizing data is one of the most useful ways to spot trends and make sense of a 
large number of data points.</p>

<p>Real-time processing is frequently used to visualize application and server metrics.  
The data changes frequently and large deltas in the metrics typically indicate 
significant impacts on the health of the systems or organization. In these cases, 
projects like <strong>Prometheus</strong> can be useful for processing the data streams 
as a time-series database and visualizing that information.</p>

<p>One popular way of visualizing data is with the <b>Elastic Stack</b>, formerly known 
as the ELK stack.  Composed of Logstash for data collection, Elasticsearch for indexing 
data, and Kibana for visualization, the Elastic stack can be used with big data systems 
to visually interface with the results of calculations or raw metrics.  A similar stack 
can be achieved using <strong>Apache Solr</strong> for indexing and a Kibana fork 
called <strong>Banana</strong> for visualization.  The stack created by these is called 
<strong>Silk</strong>.</p>

<p>Another visualization technology typically used for interactive data science work is 
a data "notebook".  These projects allow for interactive exploration and visualization 
of the data in a format conducive to sharing, presenting, or collaborating.  Popular 
examples of this type of visualization interface are <strong>Jupyter Notebook</strong> 
and <strong>Apache Zeppelin</strong>.</p>

<h3 id="big-data-glossary">Big Data Glossary</h3>

<p>While we've attempted to define concepts as we've used them throughout the guide, 
sometimes it's helpful to have specialized terminology available in a single place:</p>

<ul>
  <li><strong>Big data</strong>: Big data is an umbrella term for datasets that cannot 
      reasonably be handled by traditional computers or tools due to their volume, 
      velocity, and variety.  This term is also typically applied to technologies and 
      strategies to work with this type of data.</li>
  <li><strong>Batch processing</strong>: Batch processing is a computing strategy that 
      involves processing data in large sets.  This is typically ideal for non-time 
      sensitive work that operates on very large sets of data.  The process is started 
      and at a later time, the results are returned by the system.</li>
  <li><strong>Cluster computing</strong>: Clustered computing is the practice of pooling 
      the resources of multiple machines and managing their collective capabilities to 
      complete tasks.  Computer clusters require a cluster management layer which 
      handles communication between the individual nodes and coordinates work 
      assignment.</li>
  <li><strong>Data lake</strong>: Data lake is a term for a large repository of 
      collected data in a relatively raw state.  This is frequently used to refer to 
      the data collected in a big data system which might be unstructured and frequently 
      changing.  This differs in spirit to data warehouses (defined below).</li>
  <li><strong>Data mining</strong>: Data mining is a broad term for the practice of 
      trying to find patterns in large sets of data.  It is the process of trying to 
      refine a mass of data into a more understandable and cohesive set of 
      information.</li>
  <li><strong>Data warehouse</strong>: Data warehouses are large, ordered repositories 
      of data that can be used for analysis and reporting.  In contrast to a <em>data 
      lake</em>, a data warehouse is composed of data that has been cleaned, integrated 
      with other sources, and is generally well-ordered.  Data warehouses are often 
      spoken about in relation to big data, but typically are components of more 
      conventional systems.</li>
  <li><strong>ETL</strong>: ETL stands for extract, transform, and load. It refers to 
      the process of taking raw data and preparing it for the system's use.  This is 
      traditionally a process associated with data warehouses, but characteristics of 
      this process are also found in the ingestion pipelines of big data systems.</li>
  <li><strong>Hadoop</strong>: Hadoop is an Apache project that was the early 
      open-source success in big data.  It consists of a distributed filesystem called 
      HDFS, with a cluster management and resource scheduler on top called YARN (Yet 
      Another Resource Negotiator).  Batch processing capabilities are provided by the 
      MapReduce computation engine.  Other computational and analysis systems can be 
      run alongside MapReduce in modern Hadoop deployments.</li>
  <li><strong>In-memory computing</strong>: In-memory computing is a strategy that 
      involves moving the working datasets entirely within a cluster's collective 
      memory. Intermediate calculations are not written to disk and are instead held 
      in memory.  This gives in-memory computing systems like Apache Spark a huge 
      advantage in speed over I/O bound systems like Hadoop's MapReduce.</li>
  <li><strong>Machine learning</strong>: Machine learning is the study and practice of 
      designing systems that can learn, adjust, and improve based on the data fed to 
      them.  This typically involves implementation of predictive and statistical 
      algorithms that can continually zero in on "correct" behavior and insights as 
      more data flows through the system.</li>
  <li><strong>Map reduce (big data algorithm)</strong>: Map reduce (the big data 
      algorithm, not Hadoop's MapReduce computation engine) is an algorithm for 
      scheduling work on a computing cluster.  The process involves splitting the 
      problem set up (mapping it to different nodes) and computing over them to produce 
      intermediate results, shuffling the results to align like sets, and then reducing 
      the results by outputting a single value for each set.</li>
  <li><strong>NoSQL</strong>: <a href="https://en.wikipedia.org/wiki/NoSQL" 
      target="_b">NoSQL</a> is a broad term referring to databases designed 
      outside of the traditional relational model.  NoSQL databases have different 
      trade-offs compared to relational databases, but are often well-suited for big 
      data systems due to their flexibility and frequent distributed-first 
      architecture.</li>
  <li><strong>Stream processing</strong>: Stream processing is the practice of computing 
      over individual data items as they move through a system.  This allows for 
      real-time analysis of the data being fed to the system and is useful for 
t      ime-sensitive operations using high velocity metrics.</li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>Big data is a broad, rapidly evolving topic.  While it is not well-suited for all 
types of computing, many organizations are turning to big data for certain types of 
work loads and using it to supplement their existing analysis and business tools.  Big 
data systems are uniquely suited for surfacing difficult-to-detect patterns and 
providing insight into behaviors that are impossible to find through conventional means. 
By correctly implement systems that deal with big data, organizations can gain 
incredible value from data that is already available.</p>

     <h3>Related Tutorials</h3>

      <ul>
        <li><a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-cluster-of-docker-containers-with-docker-swarm-and-digitalocean-on-centos-7">How to Create 
            a Cluster of Docker Containers with Docker Swarm and DigitalOcean on CentOS 
            7</a></li>
  <li><a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-cluster-of-docker-containers-with-docker-swarm-and-digitalocean-on-ubuntu-16-04">How to Create a 
            Cluster of Docker Containers with Docker Swarm and DigitalOcean on Ubuntu 
            16.04</a></li>
  <li><a href="https://www.digitalocean.com/community/tutorials/how-to-deploy-a-node-js-and-mongodb-application-with-rancher-on-ubuntu-16-04">How To Deploy a Node.js and 
            MongoDB Application with Rancher on Ubuntu 16.04</a></li>
  <li><a href="https://www.digitalocean.com/community/tutorials/how-to-manage-multi-node-deployments-with-rancher-and-docker-machine-on-ubuntu-16-04">How To Manage Multi-Node 
            Deployments with Rancher and Docker Machine on Ubuntu 16.04</a></li>
  <li><a href="https://www.digitalocean.com/community/tutorials/how-to-deploy-cockroachdb-on-a-three-node-cluster-on-ubuntu-16-04">How To Deploy CockroachDB on a Three-Node 
            Cluster on Ubuntu 16.04</a></li>
      </ul>

<a name="BigDataWithCephStorage"></a>
<h3 class="single-post-title entry-title">3 Ways Big Data Thrives with Ceph Storage 
<a href="https://www.concurrent.com/blog/3-ways-big-data-thrives-with-ceph-storage/" 
target="_b">(Source Origin)</a></h3>


<p class="byline vcard">Posted on <time>December 7, 2015</time> by 
<a href="https://www.concurrent.com/blog/author/concurrent/" title="Posts by 
Concurrent">Concurrent</a> &amp; filed under 
<a href="https://www.concurrent.com/blog/category/uncategorized/">Storage</a></p>


</header><img src="https://www.concurrent.com/wp-content/uploads/2015/09/shutterstock_223785412-320x320.jpg" width="320" height="320">


<p>The rain in Atlanta has finally stopped, and I decided to catch up on some industry 
reading. Sitting on the patio, I was surprised by an Accenture/GE study showing that 
88% of executive respondents were working on big data projects. What surprised me wasn't 
the high percentage, but rather the breadth of businesses looking to leverage big data. 
This study included a wide variety of industrial enterprises &#8211; including 
healthcare.</p>


<p>I shook my head. Can you imagine the headaches these monolithic initiatives are 
giving IT? According to Gartner and others, more than half of big data projects fail. 
After all, how can legacy hardware, including legacy scale-up storage, adapt to massive 
amounts of new data?</p>


<p>It brought home for me many of the advantages of using scale-out storage. Ceph-based, 
next generation storage can help ease the headache in IT. Here are three ways scale-out 
storage enables big data projects when deployed properly:</p>


<ol>
  <li><strong>Beginning with the future in mind:</strong> One of the starkest features 
      of big data is that, well, it's big. Traditional storage systems can be designed 
      to store large quantities of data, but proper planning is not always simple. 
      Legacy storage systems require IT to guess how much data they need to store over 
      the next 1-3 years.  The more data expected, the higher the cost of the storage 
      system. Making a bad guess is like putting your money on the wrong horse-either 
      you never get a return or it could cost you your job.

      <p>Ceph-based, scale-out storage has some major advantages here. You can add 
      capacity whenever it is needed. You only pay for what you need, and as your needs 
      grow, you can grow accordingly. With Ceph you simply add a new server-based 
      node... no more complex migrations or scheduled down-time to upgrade storage 
      capacity. Scaling this way makes much more sense when trying to predict and stay 
      ahead of big data projects.</li>

  <li><strong>Keeping it simple:</strong> Big data comes in many shapes and sizes. 
      Unstructured big data can be drawn from virtually any source in your environment. 
      It is not uncommon to find file, block and object-based storage all uniting in a 
      big data project, nor is it unusual to find data residing in several different and 
      fractured storage systems. I hope you have your Alka-Seltzer and aspirin cocktail 
      ready when trying to bring it all together.

      <p>Ceph is a unified storage system for file, block and object-based storage. 
         Storing all your data in one cost-effective architecture will eliminate 
         complexity. Additionally, as new data sources are added to the big data 
         project, they can all be extracted using the various interfaces on one 
         Ceph-based system.</li>

  <li><strong>Staying Responsive:</strong> We all know big data has turned storage on 
      its head. Data collected by machine sensors piles up at alarming rates. Data 
      queries crunching those numbers are made constantly. Putting it mildly, 
      responsiveness requirements and expectations are sharply on the rise.

      <p>This is possibly the most important advantage of Ceph. While scaling to 
      exabytes is fantastic, not losing performance along the way is even better. 
      Traditional storage relies on controllers to access the data. When these 
      controllers max out on RAM, CPU or network speeds, all the storage space in the 
      world won't help with responsiveness. Maxed is maxed.</p>


      <p>Using a Ceph-based system, CPU, RAM and network speed are added with each new 
      Ceph node (along with storage space). By adding responsiveness as the system 
      grows, performance never suffers as storage expands.</li>
</ol>


<p>Back on the patio, I wonder how many more big data projects will have to fail before 
enterprises start to realize how important the underlying technology is. Thankfully, 
finding a storage solution that meets the requirements of the project can be 
cost-effective and painless to implement.</p>


<p>Find out more at <a href="http://www.concurrent.com">www.concurrent.com</a> "Powering 
Brighter Ideas"</p>


<a name="MonitoringCephCluster"></a>
<h3>Stratosphere</h3> 

<P>A blog featuring thoughts, technical insights and creative ideas from our very own 
Stratonauts

<h3 class="entry-title">What to Monitor in a Ceph Cluster 
<a href="http://www.stratoscale.com/blog/storage/ceph-monitor-cluster/">(Source 
Origin)</a></h3>

<p class="post-meta"> by <a href="http://www.stratoscale.com/author/avishay/">Avishay 
Traeger</a> | Aug 17, 2016 | 
<a href="http://www.stratoscale.com/blog/storage/">Storage</a></p>

<p>
<img src="http://www.stratoscale.com/wp-content/uploads/ceph-icon.png" alt="ceph icon" 
height="300" width="300">

<P>Once you have a running 
<a href="http://www.stratoscale.com/blog/storage/introduction-to-ceph/" 
target="_blank">Ceph</a> cluster, it must be kept running by <b>monitoring and 
troubleshooting issues, and profiling its CPU and memory usage</b>.
 In this article I'll describe some useful standard operating system 
tools and Ceph's built-in functions that can be used to diagnose issues 
and handle common errors. </p>

<h4>Ceph Health</h4>

<p>Getting started with Ceph monitoring and troubleshooting starts with the 'ceph health'
 command. You can execute this command on any node of a Ceph cluster, or even on a 
standalone machine that is connected to a Ceph cluster. In the latter case, you need a 
valid Ceph configuration file (usually in /etc/ceph.ceph.conf) and of course, a 
<a href="http://docs.ceph.com/docs/master/start/quick-rbd/#install-ceph" 
target="_blank">Ceph client</a> installed on this machine. A common use case:</p>

<pre> 
$ ceph health
HEALTH_OK
</pre>

<p>The 'HEALTH_OK' status obviously means that your Ceph cluster is up and running, and 
that there are no observable issues that were detected by Ceph's built-in self-diagnosis.
 Other possible statuses are 'HEALTH_WARN' and 'HEALTH_ERR'.  The 'ceph health detail'  
command shows more detailed information about your cluster's health. </p>

<p></p>
Learn how to integrate Ceph storage with OpenStack <a href="http://www.stratoscale.com/blog/storage/integrating-ceph-storage-openstack-step-step-guide/">here</a>.
 <p></p>

<h4>Monitoring OSDs</h4>

<p>The 'ceph' tool provides additional monitoring commands that include: </p>

<ul> 
  <li>'ceph osd stat' (shows the Ceph object-based storage devices - OSDs statuses)</li> 
  <li>'ceph mon stat' (shows the Ceph monitors' statuses)</li> 
  <li>'ceph quorum_status' (shows the quorum status)</li>
</ul>

<p><b>An OSD typically represents a single disk</b>.  Although Ceph maintains redundant 
copies of objects across OSDs to provide data resiliency, it is still important to 
detect situations when one or more OSDs are not functional and discover why this 
occurred. The 'ceph osd stat' command shows the status of OSDs in the cluster. Each 
OSD can have either of these statuses:</p>

<ul> 
  <li>"up" (the OSD daemon is running and responsive)  </li> 
  <li>"down" (the OSD daemon is either stopped or not responsive). </li>
</ul>

<p><b>The OSD sends heartbeats to other OSDs and to the Ceph Monitors</b>. If the OSD is 
marked as "down", then it simply means that other OSDs or the Ceph Monitors have not 
received answers to their heartbeats from that specific OSD. </p>

<p>Therefore,  the first step you should do for such an OSD is to understand why it is
 marked as "down" (node is down, OSD daemon is not running and so on). At the same time, 
each OSD can have one of these statuses:</p>

<ul> 
  <li>"in" (OSD participates in data placement)  </li> 
  <li>"out" (OSD does not participate in data placement)</li>
</ul>

<p><b>The OSD can be "down" but still "in". </b></p>

<p>There is a configurable delay between the time that an OSD is marked as "down" before 
it is marked as "out". This delay is required to avoid unnecessary rebalancing while the 
OSD is experiencing a short time failure. The delay is 5 minutes by default, but you can 
change it in the configuration file ('mon osd down out interval'). </p>

<p>You can also disable/enable an "auto out" function cluster-wide, using the commands 
'ceph osd set noout' and 'ceph osd unset noout'. If you know that an OSD which is marked 
as "down" will never be functional again, for example, due to unrecoverable disk error, 
you can mark it as "out" by executing the command 'ceph osd out OSD_ID'. Ceph will 
immediately start a recovery operation.</p>

<h4>Monitoring Capacity</h4>

<p>It is very important to monitor your cluster for available storage capacity. When 
your cluster gets close to its maximum capacity (the 'mon osd full ratio' parameter, the 
default value is 95%), Ceph stops accepting write requests. The 'mon osd nearfull ratio' 
parameter (the default value is 85%) allows setting a threshold to report a 
corresponding warning.</p>

<p><b>To check data usage and data distribution among pools, you can use the 'ceph df' 
command. </b>The 'GLOBAL' section of the output contains the overall storage capacity 
of the cluster, the amount of free space available in the cluster, the amount of raw 
storage used (including replicas, snapshots, and clones), and the percentage of raw 
storage used. The 'POOLS' section contains a list of pools and the storage usage of 
each pool. Note that the storage usage for pools does not contain storage used by 
replicas, snapshots, or clones:</p>

<pre> 
$ ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED 
    82799M     78594M        4204M          5.08 

POOLS:
    NAME             ID     USED       %USED     MAX AVAIL     OBJECTS 
    data             0         0           0        39294M           0
    ... 
</pre>

<p>The 'ceph osd df' command allows you to see disk utilizations per OSD:</p>

<pre> 
$ ceph osd df
ID WEIGHT  REWEIGHT SIZE  USE  AVAIL %USE  VAR
0  0.90999  1.00000 931G  545G  385G 58.64 0.98
10 0.90999  1.00000 931G  725G  205G 77.95 1.31
11 0.90999  1.00000 931G  432G  498G 46.47 0.78
...
</pre>

<p>If some of OSDs are near full and others are not, then you may have a problem with 
the CRUSH weight (<a href="http://docs.ceph.com/docs/master/rados/operations/crush-map/" 
target="_blank">Controlled, Scalable, Decentralized Placement of Replicated Data</a>) 
for some OSDs. Uneven data distribution across OSDs is one of the most common Ceph 
problems.</p>


<h4>Standard system tools</h4>

<p>Standard GNU/Linux system tools can be used to monitor Ceph nodes or troubleshoot a 
specific node. First of all, check the node overall health. For example, check the 
'dmesg' output for any suspicious messages. For long term operations it is a good idea 
to use a monitoring tool, such as Zabbix, to monitor the overall health of the Ceph 
nodes. Because Ceph is sensitive to clock drift, make sure that ntp daemon is running 
on each Ceph node and there is no issues with time synchronization. </p>

<p>Use standard tools, such as <a href="https://en.wikipedia.org/wiki/Hdparm" 
target="_blank">hdparm</a> or <a href="https://www.smartmontools.org/" 
target="_blank">smartmontools</a> to check disk health. The smartmontools package 
contains a daemon, smartd, which can be configured to send warnings of disk degradation 
and failures.</p>

<p>Other metrics that you may want to monitor for each node in the cluster include: </p>

<ul> 
  <li>CPU and memory utilization (using Zabbix or <a href="http://www.atoptool.nl/" 
      target="_blank">atop</a>)</li> 
  <li>Number of dropped RX/TX packets on node's NICs (using ifconfig; a big number of 
      dropped packets means that you need to increase the corresponding queue size)</li>
</ul>

<h4>Ceph admin socket</h4>

<p>Each Ceph daemon, such as Ceph OSD, Ceph Monitor, or Ceph Metadata Server reads its 
configuration from a corresponding section in the Ceph configuration file 
(/etc/ceph.ceph.conf). Sometimes you may need to see the actual configuration for the 
specific daemon or even to change its configuration. The <a href="http://docs.ceph.com/docs/master/rados/operations/monitoring/#using-the-admin-socket" 
target="_blank">Ceph admin socket</a> allows you to show and set your configuration at 
runtime. To get a daemon configuration at runtime via the admin socket, login to the 
node running the daemon and execute the following command:</p>

<pre> 
$ ceph daemon osd.0 config show
</pre>

<p>Note that you also can use the admin socket and the 'ceph daemon' command to change 
configuration at runtime directly, therefore you do not need running Ceph Monitors for 
that. For example, to set log level to '0/5' (read more about setting 
<a href="http://docs.ceph.com/docs/master/rados/troubleshooting/log-and-debug/#subsystem-log-and-debug-settings" 
target="_blank">Ceph log levels</a>) for the specified Ceph OSD, execute the following 
command on the node where the specified Ceph OSD is running:</p>

<pre> 
$ sudo ceph daemon osd.0 config set debug_osd 0/5
</pre>

<p>Note that you can use the same admin socket for Ceph OSD to view:</p>

<ul> 
  <li>performance counters: 'perf dump' command</li> 
  <li>view recent slow operations: 'dump_historic_ops'</li> 
  <li>view current operations: 'dump_ops_in_flight'</li>
</ul>

<h4>Ceph logging </h4>

<p>The default location of the Ceph log files is /var/log/ceph, and as expected the 
logs contain a historical record of events. The 'ceph -w' displays the current status of 
the cluster and major events in real time. Running this command can be very useful to 
troubleshoot a specific operation, or if you have issues when starting your cluster.</p>

<p>You can <a href="http://docs.ceph.com/docs/master/rados/troubleshooting/log-and-debug/#boot-time" 
target="_blank">enable Ceph debug logging</a> in a Ceph configuration file, or 
temporarily enable it for a specific component in runtime. Note that logging is resource 
intensive, so enabling debug logging permanently for a production cluster is not a good 
idea - it will slow down your system and can generate a significant amount of data 
(gigabytes per hour). Instead, if you are encountering a problem with a specific 
component of your cluster, enable logging for that component temporarily. You can use 
the admin socket (see the section above 'Ceph configuration'), but you need to log in 
to the corresponding node to do this. To set a log level for a specific component from 
any Ceph node, you can use the 'ceph injectargs' command (you will need a running Ceph 
Monitor): </p>

<pre> 
$ ceph tell osd.0 injectargs --debug-osd 0/5
</pre>

<h4>Ceph scrubbing</h4>

<p>Each Ceph OSD is responsible for checking its data integrity via a periodic operation 
called <a href="http://docs.ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing" 
target="_blank">scrubbing</a>. <b>Light scrubbing</b> usually runs daily and checks the 
object size and attributes. <b>Deep scrubbing</b> usually runs weekly and reads the data 
and recalculates and verifies checksums to ensure data integrity. Scrubbing is needed 
for data integrity, but it can also reduce performance of the Ceph cluster, so it is 
important to know when light and deep scrubbing run and adjust the corresponding 
parameters. In some cases, you may want to run scrubbing manually to ensure data 
integrity. The following commands schedule light and deep scrubbing correspondingly:</p>

<pre> 
$ ceph pg scrub PG_ID
$ ceph pg deep-scrub PG_ID
</pre>

<p>In some cases, for example, you add new OSDs to the cluster or mark some OSDs as out 
for maintenance, you may want to temporarily disable scrubbing to make recovery or 
backfill (moving data to a new OSD) operations faster. To disable scrubbing cluster-wide 
and then enable it again, use the following commands:</p>

<pre> 
$ ceph osd set noscrub
$ ceph osd unset noscrub
</pre>

<h4>Limit the impact of backfill and recovery operations</h4>

<p>If  you have noticed that the overall performance of your Ceph cluster is low, or you 
are planning maintenance of one or more OSDs, then you may want to minimize the impact 
of backfill and recovery operations and preserve the performance of the cluster. There 
are two parameters you may want to temporarily change:</p>

<ul> 
  <li>'osd max backfills': (the number of concurrent backfills per OSD, 10 by 
      default)</li> 
  <li>'osd recovery max active' (the number of concurrent recovery operations per OSD, 
      15 by default).</li>
</ul>

<p>For further information about how to deploy Ceph with OpenStack, see <a href="http://www.stratoscale.com/blog/storage/deploying-ceph-challenges-solutions/" 
target="_blank">Deploying Ceph with OpenStack - Challenges and Solutions</a>&nbsp;&nbsp;
<a href="http://www.stratoscale.com/blog/storage/integrating-ceph-storage-openstack-step-step-guide/" 
target="_blank">Integrating Ceph Storage with OpenStack -- A Step by Step Guide.</a></p>



<h4 style="text-align: right;"></h4>


<h4>You may also like:</h4>

<ul>
  <li><a href="http://www.stratoscale.com/blog/openstack/openstack-security-storage-cloud/"><img src="http://www.stratoscale.com/wp-content/uploads/bigstock-140923931-150x150.jpg"
 alt="secure storage wooden box" height="150" width="150"><br>
OpenStack Security - How to Secure the Storage Setup in a Cloud, Part 1</a></li>
  <li><a href="http://www.stratoscale.com/blog/storage/ceph-radosgw-compared-openstack-swift-aws-s3/"><img src="http://www.stratoscale.com/wp-content/uploads/ceph-icon-150x150.png" 
alt="ceph icon" height="150" width="150"><br>
Ceph Object Gateway (RadosGW) compared to OpenStack Swift and AWS S3</a></li>
  <li><a href="http://www.stratoscale.com/blog/openstack/managing-openstack-cinder-backends/"><img src="http://www.stratoscale.com/wp-content/uploads/managing-openstack-cinder-backends-150x150.jpg" alt="Two Approaches to Managing OpenStack Cinder 
Backend" height="150" width="150"><br>
Two Approaches to Managing OpenStack Cinder Backends</a></li>
</ul>



<a name="BigDataAndObjectStoragePart1"></a>
<h3>Big Data, OpenStack and object storage: Size matters, people 
<a href="http://www.theregister.co.uk/2015/05/18/size_matters_storage_open_stack/" 
target="_b">(Source Origin)</a></h3>

<h4>Consider your needs before rushing out and investing in new storage tech</h4>


<img src="https://regmedia.co.uk/2015/05/18/hubble_horsehead_nebula.jpg" height="348" 
width="648">
<div class="img_caption">Are your data needs billowing up?</div>

    
<P> 18 May 2015 at 11:58,

    
    
<a href="http://www.theregister.co.uk/Author/2942" title="Read more by this author" 
class="alt_colour dcl">Enrico Signoretti</a>
    
<p><strong class="trailer">COMMENT</strong> I'm talking about Big Data, OpenStack and 
object storage. In the last two days I've come across a couple of articles (<a 
target="_blank" href="#LittleSpendingOnBigData">here</a> and <a target="_blank" 
href="#HadoopAdoptionHype">here</a>, both cached locally), which discuss the adoption 
of these technologies.</p>


<p>Both articles start from surveys that talk about scarce adoption in the enterprise 
space for these technologies. When talking about them, two questions arise time after 
time. Is it too soon? And is real interest lacking in traditional enterprises? The short 
answer is 'yes' to both questions, but I think some elaboration is necessary.</p>

<h4>Do you have the problem?</h4>


<p>These technologies are all designed to solve problems on a big scale:</p>

<ul>
  <li>Object storage = storing huge amounts of unstructured data</li>

  <li>Big Data (analytics) = analysing huge amounts of data</li>

  <li>OpenStack (and cloud management platforms in general) = managing huge pools of 
      compute, networking and storage resources</li>
</ul>


<p>The common word here is HUGE. Otherwise, it's like shooting sparrows with 
   bazookas.</p>


<p>Yes, you could be interested. And yes, you could have them in a lab to better 
understand what they do and how you could leverage them. But at the end of the day, 
you'll stick with your traditional infrastructure: your NAS for unstructured data, 
SQL DBs for analytics, and VMware or Microsoft virtualisation stacks with some fancy
 automation and provisioning tools.</p>


<p>That's all you need (today), and this is why most of OpenStack and Big Data 
analytics infrastructures are still PoCs (proof of concepts) in the labs.</p>

<h4>It's just too soon</h4>


<p>In some cases data (and infrastructure) growth is quickly heading towards that HUGE 
mode mentioned above. It's just a matter of time and if you don't want to outgrow your 
IT team, you'll be looking at these technologies in the not-very-distant future.</p>


<p>In fact, if you want to manage petabytes instead of terabytes, or thousands of VMs 
instead of hundreds per person, you will need something other than what you are used 
to.</p>


<p>At the same time, if your organisation is not experiencing an exponential growth in 
terms of data and compute needs, but the trend is more linear, any new hardware 
generation will probably suffice to avoid structural changes.</p>


<p>Furthermore, incremental updates to legacy technologies (like adding in-memory 
capabilities to a traditional RDBMS) can give some extra juice, and will still be 
cheaper to implement than starting from scratch with next-generation technology and 
the investment needed to train people within your organisation.</p>

<h4>But you're probably already using them</h4>


<p>On the other hand, most of us (both consumers and enterprises) are already using the 
technologies mentioned in this article. Actually, many modern solutions we are adopting 
in our organisations are based on these technologies.</p>


<p>Take object storage as an example. Somewhere in your organisation there is a sync 
&amp; share solution, a cloud storage gateway of some sort, back-ups are being sent 
to the cloud or something else. In all these situations, even when the front-end is 
installed locally, you are already leveraging object storage at the back-end.</p>


<p>It's likely that if you sum together all of the data managed by these applications, 
it would still be cheaper to buy a service instead of building a new on-premises 
infrastructure. But have you ever checked it out?</p>


<p>In the adoption charts of the surveys mentioned at the beginning of the article, all 
those (numerous) companies, accessing the same object storage platform provided by a 
single service provider are counted as one - even if the SP has a multi-petabyte 
installation serving thousands of tenants.</p>

<p class="wptl btm">Sponsored: <a href="http://go.theregister.com/tl/1672/-5288/customer-identity-and-access-management?td=wptl1672bt">Customer 
Identity and Access Management</a></p>

<P>    Next page:
    <a href="DataLakesOrDataPonds">Data lakes or data ponds?</a>

<div id="page-nav">
    Page:
    <ul>
        <li class="current">1</li>
        <li><a href="#BigDataAndObjectStoragePart2">2</a></li>
    </ul>
</div>



<a class="reg_btn count" title="View comments on this article" 
href="https://forums.theregister.co.uk/forum/1/2015/05/18/size_matters_storage_open_stack/">
1 Comment</a></div>


<a name="BigDataAndObjectStoragePart2"></a>
<h3>Big Data, OpenStack and object storage: Size matters, people, Part2</h3>

<h4>Consider your needs before rushing out and investing in new storage tech</h4>


18 May 2015 at 11:58,
    
    
 <a href="http://www.theregister.co.uk/Author/2942" title="Read more by this author" 
class="alt_colour dcl">Enrico Signoretti</a>
    

<a name="DataLakesOrDataPonds"></a>
<h4>Data lakes or data ponds?</h4>


<p>In some cases the quantity of data is not huge. In others, organisational issues make 
a data lake very hard to build, while collecting many different smaller sets of data is 
quite easy. This means that you probably have different projects within the company, 
with different types of data under management and stored on different platforms. This 
leads to smaller clusters (or cloud services), less overall efficiency and makes it 
impossible to build a single huge infrastructure without consolidation.</p>

<h4>Lack of ease of use and appliances</h4>


<p>Up to now, another big obstacle to the adoption of these technologies, especially in 
medium-sized enterprises, is the lack of pre-packaged appliances and ease of use. 
Fortunately, this is now quickly changing and vendors are finally presenting
pre-packaged - and in some cases hyper-converged - solutions, putting together different 
components in a pre-assembled fashion.</p>


<p>Part of the benefit comes from simplicity, but fast provisioning and automation also 
play an important role. With this approach, the IT department can now give freedom and 
flexibility to all business/organisational units, and can allow them to choose the right 
product for each one of their projects.</p>


<p>For example, if you look at solutions such as the recently launched by <a 
target="_blank" 
href="http://www.theregister.co.uk/2015/04/29/hds_hyper_scaleout_platform/">HDS HSP</a>
 - a hyper-converged appliance based on KVM, Openstack and a proprietary distributed 
file system highly optimised for Big Data workloads - you'll find that you can easily 
build a data lake and leverage it through different data analytics tools in a cloud-ish 
way.</p>


<p>It's like having a specialised Big Data-as-a-Service-in-a-box! It doesn't come cheap 
(you pay for the integration, support and industrialisation of the product), but minimum 
configuration is five nodes. Which is not much higher than the 3/4 node configuration of 
most Hadoop clusters out there, while adding a lot of flexibility by supporting many 
different Hadoop distributions, NoSQL databases and whatever else you need, while 
enabling the creation of a data lake.</p>

<h4>Closing the circle</h4>


<p>On-prem Big Data, OpenStack (private clouds) and object storage are not for everyone. 
If you don't have the problem, you don't need them. It's just common sense, isn't it?</p>


<p>In fact, only surveyors and <a target="_blank" href="http://www.theregister.co.uk/2015/05/13/hadoop_two_years_of_anaemia/">some analysts</a> 
aren't aware of it. In this case, leveraging external services is the best choice.</p>


<p>If you are experiencing an exponential growth of data and infrastructure then, sooner 
or later, you are going to need them. In that case, it's time to start building the 
two-tier strategy I have mentioned many times <a target="_blank" 
href="http://juku.it/en/data-needs-need-rethink-storage/">in my blog</a>. A strategy 
where the secondary tier is the data lake, maybe based on object storage.</p>


<p>Consolidation of different storage/data islands will become an important part of this 
process, but at the same time, flexibility remains the pillar to maintaining simplicity 
and usability of data and resources.</p>


<p>This is why I'm sure we will see more data-centric, hyper-converged systems in the 
market soon, especially in mid-sized organisations where there aren't enough resources 
to build these kind of infrastructures from scratch. </p>

<p class="wptl btm">Sponsored: <a href="http://go.theregister.com/tl/1700/shttp://pubads.g.doubleclick.net/gampad/clk?id=116303739&amp;iu=/6978">Continuous lifecycle London 2017 event. DevOps, continuous delivery and containerisation. Register now</a></p>


<P> Page:
<ul>
        <li><a href="#BigDataAndObjectStoragePart1">1</a></li>
        <li class="current">2</li>
</ul>

<div class="comments ">
<a class="reg_btn count" title="View comments on this article" 
href="https://forums.theregister.co.uk/forum/1/2015/05/18/size_matters_storage_open_stack/">
1 Comment</a>
</div>
            
<a name="HadoopAdoptionHype"></a>
<h3>Hadoop adoption hurt by hype. Hey, this stuff is hard! 
<a href="http://fortune.com/2015/05/13/hadoop-adoption-hype/" 
target="_b">(Source Origin)</a></h3>

<P>
<img src="https://fortunedotcom.files.wordpress.com/2014/09/488979113.jpg" alt="Big Data" title="Big Data" data-reactid="166" width="800" height="600">

<P> Even people who are not data scientists know that Hadoop is a big deal, even
 if they don't know exactly what it is. Hadoop in many people's eyes 
equals big data. And everyone wants big data, right?<!-- /react-text --></p><p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="217"><!-- react-text: 218 -->The
 sales pitch is that Hadoop can take reams of information in different 
formats, crunch it, and provide the fodder for better business 
decisions. Companies, for example, can use Hadoop and associated 
analytics to dig into social networks to find out what people are saying
 about their products on Twitter, Facebook etc., and use information to 
change course if necessary.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="219"><!-- react-text: 220 -->That sort of thing can be valuable, which is why vendors like <!-- /react-text --><a href="http://fortune.com/2015/02/23/that-ipo-cloudera-bides-its-time/" data-reactid="221"><!-- react-text: 222 -->Cloudera<!-- /react-text --></a><!-- react-text: 223 --> and <!-- /react-text --><a href="http://fortune.com/2014/11/10/hortonworks-files-for-ipo-6-things-to-know/" data-reactid="224"><!-- react-text: 225 -->Hortonworks<!-- /react-text --></a><!-- react-text: 226 --> <!-- /react-text --><a href="http://fortune.com/company/hdp" class="_11XURygs _23OPiUxU" data-reactid="227"><!-- react-text: 228 -->(<!-- /react-text --><!-- react-text: 229 -->hdp, +1.69%<!-- /react-text --><!-- react-text: 230 -->)<!-- /react-text --></a><!-- react-text: 231 --> are built on that Hadoop foundation.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="232"><!-- react-text: 233 -->And yet ... adoption isn't setting the world on fire, according to a new <!-- /react-text --><a target="_blank" href="http://www.gartner.com/newsroom/id/3051717" data-reactid="234"><!-- react-text: 235 -->Gartner survey <!-- /react-text --></a><!-- react-text: 236 -->which shows less than 50% of 284 respondents have invested in Hadoop technology or even plan to do so.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="237"><!-- react-text: 238 -->Just
 over a quarter (26%) of respondents said they are deploying or 
experimenting with Hadoop and only 11% said they plan to invest in 
Hadoop within 12 months. In a press release, Gartner <!-- /react-text --><a href="http://fortune.com/company/it" class="_11XURygs _23OPiUxU" data-reactid="239"><!-- react-text: 240 -->(<!-- /react-text --><!-- react-text: 241 -->it, +0.58%<!-- /react-text --><!-- react-text: 242 -->)<!-- /react-text --></a><!-- react-text: 243 -->
 analysts found two possible reasons. One was that respondents did not 
feel Hadoop was a priority and others felt it was "overkill." Ouch.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="244"><!-- react-text: 245 -->The survey also confirmed the notion that a persistent <!-- /react-text --><a target="_blank" href="http://www.forbes.com/sites/teradata/2015/02/24/its-complicated-why-the-hadoop-skills-gap-will-widen-in-2015/" data-reactid="246"><!-- react-text: 247 -->shortage of Hadoop skills<!-- /react-text --></a><!-- react-text: 248 --> is hindering adoption.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="249"><!-- react-text: 250 -->The
 respondents work in big companies--the average annual revenue for the 
member companies is $3.4 billion and mean head count is just under 7,900
 employees. They hear that Hadoop is this sexy new thing but they don't 
know what to make of it, said Gartner Research Director Nick Heudecker.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="251"><!-- react-text: 252 -->"We
 get lots of questions from clients about what this thing is good for," 
he said. "It doesn't help that Hadoop is not a thing but several dozen 
software components. That is challenging for an enterprise that has lots
 of priorities."<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="253"><!-- react-text: 254 -->And, for those who aren't sure what Hadoop is, it basically consists of two important subsystems called <!-- /react-text --><a target="_blank" href="http://hadoop.apache.org/mapreduce/" data-reactid="255"><!-- react-text: 256 -->Hadoop MapReduce<!-- /react-text --></a><!-- react-text: 257 --> and <!-- /react-text --><a target="_blank" href="http://hadoop.apache.org/hdfs/" data-reactid="258"><!-- react-text: 259 -->Hadoop Distributed File System<!-- /react-text --></a><!-- react-text: 260 -->.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2  _10M0Ygc4" data-reactid="261"><!-- react-text: 262 -->Together
 they take enormous quantities of data and process it relatively 
quickly. It does this by splitting the jobs into tiny pieces and 
spreading it out over multiple servers. By keeping the data on servers 
instead of dedicated storage systems, the jobs are processed faster and 
more cheaply. But there are still compromises. Running a Hadoop job 
still takes time and is done in batches, so it's not good for real-time 
data or jobs that see a continuous inflow of data.<!-- /react-text --></p>

<p class="column small-12 medium-10 medium-offset-1 large-offset-2 end _10M0Ygc4" data-reactid="263"><!-- react-text: 264 -->But
 back to the survey. The upshot seems to be that while Hadoop can handle
 huge data sets and make them useable, the capabilities needed to set up
 and run Hadoop remain scarce and expensive. And, for at least a subset 
of the corporate population, the perceived advantages do not yet 
outweigh the cost and complications.<!-- /react-text --></p>


<a name="LittleSpendingOnBigData"></a>
<h3>There's little spending on Big Data, object storage and OpenStack 
<a href="http://www.theregister.co.uk/2015/05/13/little_spending_on_big_data_object_storage_and_openstack/">(Source Origin)</a></h3>

<h4>DataCore survey shows just a dribble of dollars for glam products</h4>

    
  13 May 2015 at 15:33,
    
 <a href="http://www.theregister.co.uk/Author/1861" title="Read more by this author" 
class="alt_colour dcl">Chris Mellor</a>
    

<p>A DataCore survey has shown little spending on sexy 
storage products in the Big Data, public cloud, object storage, and 
OpenStack areas, arguing software-defined storage (SDS) gives a faster 
payback.</p>


<p>DataCore asked 477 IT professionals currently using 
or evaluating software-defined storage technology about their 2015 
spending plans (relating to storage software enabling freedom from 
hardware lock-in) and other areas.</p>


<p>OpenStack is being ignored by 70 per cent of the 
respondents. More than half have no public cloud storage spending plans 
this year. Half are not putting money into Big Data or object storage.</p>


<p>VDI is of interest to only 49 per cent, while flash 
storage is a minority interest with less than 30 per cent of the 
respondents interested in spending in that area.</p>


<p>Can we generalise these results to all IT professionals, or is there something special about SW-defined storage buyers?</p>


<p><i>Here's the table of results in the other areas category</i>:</p>
<div class="CaptionedImage Left Float">
<img src="https://regmedia.co.uk/2015/05/13/datacore_2015_survey.jpg" 
alt="DataCore_2015_survey" height="344" width="500">
</div>


<p>The survey showed people are looking at SDS to extend
 the life of existing storage assets, avoid hardware lock-in and 
automate storage operations (52, 49 and 45 per cent respectively).</p>


<p><i>El Reg</i> thinks these concerns may discourage 
them from buying new storage hardware and silos. They are not 
representative of the overall IT buying population and DataCore's 
results can't really be generalised.</p>


<p>DataCore's 2015 State of Software-Defined Storage Survey was conducted in April 2015, and you can view the report <a target="_blank" href="http://www.datacore.com/sds2015">here (PDF)</a>. </p>

<div class="comments ">
<a class="reg_btn count" title="View comments on this article" 
href="https://forums.theregister.co.uk/forum/1/2015/05/13/little_spending_on_big_data_object_storage_and_openstack/">
4 Comments</a>
</div>
            
<a name="LargeDataVsBigData"></a>
<h3 class="entry-title">Living Large: The Challenge of Storing Video, Graphics, and 
other "LARGE Data" 
<a href="https://www.suse.com/communities/blog/living-large-challenge-storing-video-graphics-large-data/">(Source Origin)</a></h3>
 
    By: <a href="https://www.suse.com/communities/blog/author/jbgeorge/" 
title="View all posts by Joseph George">Joseph George</a>		</p>
	                        <p class="icon_expert-views"></p>	

<p class="date">August 24, 2016 11:17 am Reads:<strong>3,571</strong>
Comments:<strong>0</strong></p>

<P><a href="https://www.suse.com/docrep/documents/9vh81j50vc/orchard_park_police_department_cs.pdf"><img  src="https://www.suse.com/communities/blog/files/2016/08/OrchardPark3-368x450.jpg"  height="232" width="190"></a>

<p><em><strong>UPDATE SEP 2, 2016</strong>: SUSE has released a brand new customer case 
study on "large data" featuring New York's Orchard Park Police Department, focused on 
video storage. </em><em>Read the full story <a href="https://www.suse.com/docrep/documents/9vh81j50vc/orchard_park_police_department_cs.pdf" 
target="_blank"><strong>here</strong></a>!</em></p>

<p>Everyone's talking about "big data" - I'm even hearing about "small data" - but those 
of you who deal in video, audio, and graphics are in the throes of a new challenge:  large data.</p>


<p><strong>Big Data vs Large Data</strong></p>


<p>It's 2016 - we've all heard about big data in some capacity - generally speaking, it 
is truckloads of data points from various sources, magnanimous in its volume (LOTS of 
individual pieces of data), its velocity (the speed at which that data is generated), 
and its variety (both structured and unstructured data types).  Open source projects 
like Hadoop have been enabling a generation of analytics work on big data.</p>


<p>So what in the world am I referring to when I say "large data?"</p>


<p>For comparison, while "big data" is a significant number of individual data that is 
of "normal" size, I'm defining "large data" as an individual piece of data that is 
massive in its size. Large data, generally, is not required to have real-time or fast 
access, and is often unstructured in its form (ie. doesn't conform to the parameters of
 relational databases).</p>


<p>Some examples of large data:</p>


<p><img src="https://www.suse.com/communities/blog/files/2016/08/LargeDataChart1.jpg" 
height="269" width="874"></p>


<p><strong>Why Traditional Storage Has Trouble with Large Data</strong></p>


<p>So why not just throw this into our legacy storage appliances?</p>


<p>Traditional storage solutions (much of what is in most datacenters today) is great 
at handling standard data.  Meaning, data that is:</p>


<ul>
<li class="first-child">average / normal in individual data size</li>
<li>structured and fits nicely in a relational database</li>
<li class="last-child">when totaled up, doesn't exceed ~400TB or so in total space</li>
</ul>
 
 
<p>Unfortunately, none of this works for large data.  Large data, due to its size, can 
consume traditional storage appliances VERY rapidly.  And, since traditional storage 
was developed when data was thought of in smaller terms (megabytes and gigabytes), large 
data on traditional storage can bring about performance / SLA impacts.</p>
 
 
<p>So when it comes to large data, traditional storage ends up being consumed too 
rapidly, forcing us to consider adding expensive traditional storage appliances to 
accommodate.</p>
 
 
<blockquote><p><em>"Overall cost, performance concerns, complexity and inability to 
support innovation are the top four frustrations with current storage systems." - SUSE, 
Software Defined Storage Research Findings (Aug 2016)</em></p></blockquote>
 
 
<p><strong>Object Storage Tames Large Data</strong></p>
 
 
<p>Object storage, as a technology, is designed to handle storage of large, unstructured 
data from the ground up.  And since it was built on scalable cloud principles, it can 
scale to terabytes, petabytes, exabytes, and theoretically beyond.</p>
 
 
<p>When you introduce open source to the equation of object storage software, the 
economics of the whole solution become even better. And since scale-out, open source 
object storage is essentially software running on commodity servers with local drives, 
the object storage should scale without issue - as more capacity is needed, you just add 
more servers.</p>
 
 
<p>When it comes to large data - data that is unstructured and individually large, such 
as video, audio, and graphics - SUSE Enterprise Storage provides the open, scalable, 
cost-effective, and performant storage experience you need.</p>
 
 
<p><strong>SUSE Enterprise Storage - Using Object Storage to Tame Large Data</strong></p>
 
 
<p>Here's how: 
<img src="https://www.suse.com/communities/blog/files/2016/08/SES-437x450.jpg" 
height="241" width="234"></p>
 
 
<ul>
  <li class="first-child"><strong>It is designed from the ground-up to tackle large 
      data</strong>.  The Ceph project, which is core of SUSE Enterprise Storage, is 
      built on a foundation of RADOS (Reliable Autonomic Distributed Object Store), and 
      leverages the CRUSH algorithm to scale data across whatever size cluster you have 
      available, without performance hits.</li>
  <li><strong>It provides a frequent and rapid innovation pace</strong>. It is 100% 
      open source, which means you have the power of the Ceph community to drive 
      innovation, like erasure coding.  SUSE gives these advantages to their customers 
      by providing a full updated release every six months, while other Ceph vendors 
      give customers large data features only as part of a once-a-year release.</li>
  <li class="last-child"><strong>It offers pricing that works for large data</strong>.
      Many object storage vendors, both commercial and open source, choose to charge you 
      based on how much data you store.  The price to store 50TB of large data is 
      different than the price to store 100TB of large data, and there is a different 
      price if you want to store 400TB of large data - even if all that data stays on 
      one server!  SUSE chooses to provide their customers "per node" pricing - you pay 
      subscription only as servers are added.  And when you use storage dense servers, 
      like the <a href="http://www8.hp.com/h20195/v2/getpdf.aspx/c04616500.pdf?ver=13" 
      target="_blank">HPE Apollo 4000 </a>storage servers, you get tremendous value.</li>
</ul>
 
 
<p>Why wait?  It's time to kick off a conversation with your SUSE rep on how SUSE 
Enterprise Storage can help you with your large data storage needs.  You can also click 
<a href="https://www.suse.com/products/suse-enterprise-storage" target="_blank">here</a> 
to learn more.</p>
 
 
<p>Until next time,</p>
 
 
<p><strong>JOSEPH </strong></p>
 
 
<p><a data-di-id="di-id-a17cb93c-274f08e3" href="http://www.twitter.com/jbgeorge" target="_blank">@jbgeorge</a></p>
 
 
<p> </p>

<a name="NoSQLDMComparison"></a>
  <h3>A Comparison Of NoSQL Database Management Systems And Models 
<a href="https://www.digitalocean.com/community/tutorials/a-comparison-of-nosql-database-management-systems-and-models" 
target="_b">(Source Origin)</a>
    </h3>

    Posted: February 21, 2014 164.8k views

    <a class="tag" href="https://www.digitalocean.com/community/tags/nosql?
       type=tutorials">NoSQL</a> 
    <a class="tag" href="https://www.digitalocean.com/community/tags/mongodb?
       type=tutorials">MongoDB</a>
      
 
  <h4 id="introduction">Introduction</h4>

<hr>

<p><a href="./NosqlDatabaseList.html" target="_b">NoSQL databases</a> try to offer 
certain functionality that more traditional relational 
database management systems do not. Whether it is for holding simple key-value pairs 
for shorter lengths of time for caching purposes, or keeping unstructured collections 
(e.g. collections) of data that could not be easily dealt with using relational 
databases and the <em>structured query language</em> (SQL) - they are here to help.</p>

<p>In this DigitalOcean article, we are going to try to introduce you to various popular 
NoSQL database management systems and explain their purposes and functionality, so that 
you can decide which one to use, or if they even suit your application's needs -- <em>at 
all</em>.</p>

<h4 id="glossary">Glossary</h4>

<hr>

<h4>1. <a href="#database-management-systems">Database Management Systems<a></h4>


<h4>2.  <a href="#nosql-database-management-systems">NoSQL Database Management 
        Systems<a></h4>


<ol>
  <li> <a href="#key-value-based">Key / Value Based </a></li>

    <ol>
      <li> <a href="#popular-key-value-based-databases">Popular Key / Value Based 
            Databases </a></li>
      <li> <a href="#key-value-when-to-use">When To Use</a></li>
    </ol>

  <li> <a href="#column-based">Column Based<a></li>

    <ol>
      <li> <a href="#popular-column-based-databases">Popular Column Based 
            Databases<a></li>
      <li> <a href="#column-when-to-use">When To Use</li>
    </ol>

  <li> <a href="#document-based">Document Based<a></li>
    <ol>
      <li> <a href="#popular-document-based-databases">Popular Document Based 
            Databases<a></li>
      <li> <a href="#document-when-to-use">When To Use<a></li>
    </ol>
  <li> <a href="#graph-based">Graph Based<a></li>

    <ol>
      <li> <a href="#popular-graph-based-databases">Popular Graph Based Databases<a></li>
      <li> <a href="#graph-when-to-use">When To Use<a></li>
    </ol>

</ol>


<h4 id="7-nosql-dbmss-in-comparison-to-relational-dbmss">7.  
<a href="#nosql-dbmss-in-comparison-to-relational-dbmss">NoSQL DBMSs In Comparison 
To Relational DBMSs</a></h4>

<ol>
  <li><a href="#when-to-use-nosql-databases">When To Use NoSQL Databases</a></li>
</ol>

<hr>

<div name="database-management-systems"></div>
<h4 id="database-management-systems">Database Management Systems</h4>


<p>Databases are logically modeled storage spaces for all kinds of different information 
(data). Each database, other than schema-less ones, have a model which provides 
structure for the data being dealt with. Database management systems are applications 
(or libraries) which manage databases of various shapes, sizes, and sorts.</p>

<p><strong>Note:</strong> To learn more about Database Management Systems, check out 
our article: <a href="http://link_to_10_1_understanding_databases/">Understanding 
Databases</a>.</p>

<div name="nosql-database-management-systems"></div>
<h4 id="nosql-database-management-systems">NoSQL Database Management Systems</h4>


<p>In the past decade or so, relational database management systems have been the choice 
of many developers and system administrators for a variety of applications, for a 
variety of reasons. Despite not being exactly flexible, the powerful nature of many 
RDBMS allowed complex database set-ups to be created, queried and used. This was more 
than enough for many requirements, since it was not until long ago that different needs 
started to rise.</p>

<p> The term "NoSQL" was coined over a decade ago, funnily enough as a name to 
yet-another relational database. However, this database had a different idea behind it: 
eliminating the use of the standardised SQL.  In the next years to come, others picked 
up and continued to grow this thought, by referring to various other non-relational 
databases as <strong>NoSQL databases</strong>.</p>

<p>By design, NoSQL databases and management systems are relation-less (or schema-less). 
They are not based on a single model (e.g. <em>relational model</em> of RDBMSs) and each 
database, depending on their target-functionality, adopt a different one.</p>

<hr>

<p>There are almost a handful of different operational models and functioning systems 
for NoSQL databases.:</p>

<table>
  <tr><th>Operational Model <th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
      <th>Examples
  
  <tr><td><strong>Key / Value:</strong><td>&nbsp;  <td> Redis, MemcacheDB, etc.

  <tr><td><strong>Column:</strong><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  
      <td>Cassandra, HBase, etc.

  <tr><td><strong>Document:</strong><td>&nbsp;&nbsp;  <td>MongoDB, Couchbase, etc

  <tr><td><strong>Graph:</strong><td>&nbsp;&nbsp; <td> OrientDB, Neo4J, etc. 
</table>

<p>In order to better understand the roles and underlying technology of each database 
management system, let's quickly go over these four operational models.</p>

<h4 id="key-value-based"><a href="https://en.wikipedia.org/wiki/Key-value_database" 
target="_b">Key / Value Based</a></h4>

<hr>

<p>We will begin our NoSQL modeling journey with key / value based database management 
simply because they can be considered the most basic and backbone implementation of 
NoSQL.</p>

<p>These type of databases work by matching keys with values, similar to a dictionary. 
There is no structure nor relation. After connecting to the database server (e.g. 
Redis), an application can state a key (e.g. <code>the_answer_to_life</code>) and 
provide a matching value (e.g. <code>42</code>) which can later be retrieved the same 
way by supplying the key.</p>

<p> Key / value DBMSs are usually used for quickly storing basic information, and 
sometimes not-so-basic ones after performing, for example, a CPU and memory intensive 
computation. They are extremely performant, efficient and usually easily scalable.</p>

<p><strong>Note:</strong> When it comes to computers, a <em>dictionary</em> usually 
refers to a special sort of data object. They constitutes of arrays of collections with 
individual keys matching values.</p>

<h4 id="column-based"><a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS" 
target="_b">Column (oriented) Based</a></h4>


<p>Column based NoSQL database management systems work by advancing the simple nature 
of key / value based ones.</p>

<p>Despite their complicated-to-understand image on the internet, these databases work 
very simply by creating collections of one or more key / value pairs that match a 
record.</p>

<p>Unlike the traditional defines schemas of relational databases, column-based NoSQL 
solutions do not require a pre-structured table to work with the data. Each record 
comes with one or more columns containing the information and each column of each 
record can be different.</p>

<p>Basically, column-based NoSQL databases are two dimensional arrays whereby each key 
(i.e. row / record) has one or more key / value pairs attached to it and these 
management systems allow very large and un-structured data to be kept and used (e.g. a 
record with tons of information).</p>

<p>These databases are commonly used when simple key / value pairs are not enough, and 
storing very large numbers of records with very large numbers of information is a must. 
DBMS implementing column-based, schema-less models can scale extremely well.</p>

<h4 id="document-based"><a target="_b" 
href="https://en.wikipedia.org/wiki/Document-oriented_database">Document-oriented 
(Based)</a></h4>


<p>Document based NoSQL database management systems can be considered the latest craze that managed to take <em>a lot</em>
 of people by storm. These DBMS work in a similar fashion to 
column-based ones; however, they allow much deeper nesting and complex 
structures to be achieved (e.g. a document, within a document, within a 
document).</p>

<p>Documents overcome the constraints of one or two level of key / value nesting of 
columnar databases. Basically, any complex and arbitrary structure can form a document, 
which can be stored using these management systems.</p>

<p>Despite their powerful nature, and the ability to query records by individual keys, 
document based management systems have their own issues and downfalls compared to 
others. For example, retrieving a value of a record means getting the whole lot of it 
and same goes for updates, all of which affect the performance.  </p>

<h4 id="graph-based"><a href="https://en.wikipedia.org/wiki/Graph_database" 
target="_b">Graph Based</a></h4>

<p>Finally, the very interesting flavour of NoSQL database management systems is the 
graph based ones.</p>

<p>The graph based DBMS models represent the data in a completely different way than 
the previous three models. They use tree-like structures (i.e. graphs) with nodes and 
edges connecting each other through relations.</p>

<hr>

<p>Similarly to mathematics, certain operations are much simpler to perform using these 
type of models thanks to their nature of linking and grouping related pieces of 
information (e.g. connected people).</p>

<p>These databases are commonly used by applications whereby clear boundaries for 
connections are necessary to establish. For example, when you register to a social 
network of any sort, your friends' connection to you and their friends' friends' 
relation to you are much easier to work with using graph-based database management 
systems.  </p>

<hr>
<h4 id="key-value-based-nosql-database-management-systems">Key / Value Based NoSQL 
Database Management Systems</h4>

<p>Key / Value data stores are highly performant, easy to work with and they usually 
scale well.</p>

<h4 id="popular-key-value-based-databases">Popular Key / Value Based Databases</h4>

<p>Some popular key / value based data stores are: <strong>Redis:</strong></p>


<p>In-memory K/V store with optional persistence. <strong>Riak:</strong></p>


<p>Highly distributed, replicated K/V store. <strong>Memcached / MemcacheDB:</strong></p>

<p>Distributed memory based K/V store.</p>

<h4 id="key-value-when-to-use">When To Use</h4>


<p>Some popular use cases for key / value based data stores are: 
   <strong>Caching:</strong></p>


<p>Quickly storing data for - sometimes frequent - future use. 
   <strong>Queue-ing:</strong></p>


<p>Some K/V stores (e.g. Redis) supports lists, sets, queues and more. 
   <strong>Distributing information / tasks:</strong></p>


<p>They can be used to implement <em>Pub/Sub</em>. 
   <strong>Keeping live information:</strong></p>

<p>Applications which need to keep a <em>state</em> cane use K/V stores easily.  </p>

<hr>
<h4 id="column-based-nosql-database-management-systems">Column Based NoSQL Database 
Management Systems</h4>


<p>Column based data stores are extremely powerful and they can be reliably used to 
keep important data of very large sizes. Despite not being "flexible" in terms of what 
constitutes as data, they are highly functional and performant.</p>

<h4 id="popular-column-based-databases">Popular Column Based Databases</h4>

<p>Some popular column based data stores are: <strong>Cassandra:</strong></p>

<p>Column based data store based on BigTable and DynamoDB. 
   <strong>HBase:</strong></p>


<p>Data store for Apache Hadoop based on ideas from BigTable.</p>

<h4 id="column-when-to-use">When To Use</h4>


<p>Some popular use cases for column based data stores are:
   <strong>Keeping unstructured, non-volatile information:</strong></p>

<p>If a large collection of attributes and values needs to be kept for long periods of 
time, column-based data stores come in extremely handy. <strong>Scaling:</strong></p>

<p>Column based data stores are highly scalable by nature. They can handle an awful 
amount of information.</p>

<hr>
<h4 id="document-based-nosql-database-management-systems">Document Based NoSQL Database 
Management Systems</h4>

<p>Document based data stores are excellent for keeping a lot of unrelated complex 
information that is highly variable in terms of structure from one another.</p>

<h4 id="popular-document-based-databases">Popular Document Based Databases</h4>

<p>Some popular document based data stores are: <strong>Couchbase:</strong></p>

<p>JSON-based, Memcached-compatible document-based data store. 
   <strong>CouchDB:</strong></p>

<p>A ground-breaking document-based data store. <strong>MongoDB:</strong></p>

<p>An extremely popular and highly-functional database.</p>

<h4 id="document-when-to-use">When To Use</h4>

<p>Some popular use cases for document based data stores are: 
   <strong>Nested information:</strong></p>


<p>Document-based data stores allow you to work with deeply nested, complex data 
structures. <strong>JavaScript friendly:</strong></p>

<p>One of the most critical functionalities of document-based data stores are the way 
they interface with applications: Using JS friendly JSON.</p>

<HR>
<h4 id="graph-based-nosql-database-management-systems">Graph Based NoSQL Database 
Management Systems</h4>


<p>Graph based data stores offer a very unique functionality that is unmatched with any 
other DBMSs.</p>

<h4 id="popular-graph-based-databases">Popular Graph Based Databases</h4>


<p>Some popular graph based data stores are: <strong>OrientDB:</strong></p>


<p>A very fast graph and document based hybrid NoSQL data store written in Java that 
comes with different operational modes. <strong>Neo4J:</strong></p>

<p>A schema-free, extremely popular and powerful Java graph based data store.</p>

<h4 id="graph-when-to-use">When To Use</h4>


<p>Some popular use cases for graph based data stores are: 
   <strong>Handling complex relational information:</strong></p>


<p>As explained in the introduction, graph databases make it extremely efficient and 
easy to use to deal with complex but relational information, such as the connections 
between two entities and various degrees of other entities indirectly related to 
them. <strong>Modelling and handling classifications:</strong></p>

<p>Graph databases excel in any situation where relationships are involved. Modelling 
data and classifying various information in a relational way can be handled very well 
using these type of data stores.</p>

<hr>
<h4 id="nosql-dbmss-in-comparison-to-relational-dbmss">NoSQL DBMSs In Comparison To 
Relational DBMSs</h4>


<p>In order to draw a clear picture of how NoSQL solutions differ from relational 
database management systems, let's create a quick comparison list:</p>

<h4 id="when-to-use-nosql-databases">When To Use NoSQL Databases</h4>

<UL>
  <li><strong>Size matters:</strong><br>

      <p>If will be working with very large sets of data, consistently scaling is easier 
         to achieve with many of the DBMS from NoSQL family.</li>

  <li><strong>Speed:</strong><br>


      <p>NoSQL databases are usually faster - and sometimes extremely speedier - when 
         it comes to <em>write</em>s. <em>Read</em>s can also be very fast depending  
         on the type of NoSQL database and data being queried.</li>


  <li><strong>Schema-free design:</strong><br>


      <p>Relational DBMSs require structure from the beginning. NoSQL solutions offer a 
         large amount of flexibility.</p>


  <li><strong>Automated (or easy) replications / scaling:</strong><br>


      <p>NoSQL databases are growing rapidly and they are being actively built 
         <em>today</em> - vendors are trying to tackle common issues and one of them 
         clearly is replication and scaling. Unlike RDBMSs, NoSQL solutions can easily 
         scale and work with(in) clusters.</p>

  <li><strong>Multiple choices:</strong><br>

      <p>When it comes to choosing a NoSQL data store, there are a variety of models, as 
         we have discussed, that you can choose from to get the most out of the database 
         management system - depending on your data type.</li>
</UL>

<div class="author">Submitted by: <a href="https://twitter.com/ostezer">O.S. Tezer</a></div>

</body></html>