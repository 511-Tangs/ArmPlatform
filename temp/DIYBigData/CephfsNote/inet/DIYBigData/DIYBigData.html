<!DOCTYPE html>
<html lang="en-US">
  <head><title>Configuring the ODROID XU4 Operating System - DIY Big Data</title>
  </head>

  <body>
<h4>Table of Contents</h4>

<OL>
  <LI><a href="#BOM">Bits and Parts</a>
  <LI><a href="#ConfiguringXU4OperatingSystem">Configuring the ODROID XU4 Operating 
         System</a>
  <LI><a href="#AddingNewNoteToCluster">Adding a New Node to the ODROID XU4 Cluster</a>
  <LI><a href="#AirlineFlightDataPart1">Airline Flight Data Analysis - Part 1 - Data 
         Preparation</a>
  <LI><a href="#AirlineFlightDataPart2">Airline Flight Data Analysis - Part 2 - 
         Analyzing On-Time Performance</a>
  <LI><a href=""></a>
  <LI><a href=""></a>
</OL>

<a name="BOM"></a>
<h4>Bits and Parts</h4>

<P> Also check: <a href="./CephClusterOnPi2.html#Parts" target="_b">Parts for Pi2</a></P>

<UL>
  <LI>  6 x ODROID-XU4 computers -- The computers to be clustered.
  <LI>  6 x 16 GB eMMC Module XU4 -- These are the drives the OS and software will be 
        installed on. Get the one with Ubuntu Linux preinstalled (not Android). You 
        can save some money by getting the 8 GB versions, but I wanted to have some 
        extra space to ensure I won't run out when installing software.
  <LI>  USB-UART Module -- This device is so that you can directly log into the console 
        on each machine (if needed)
  <LI>  <a href="https://unclutterer.com/2009/01/17/power-strips-that-work-well-with-wall-warts/">Wall 
        wart friend power strip</a> -- I will have 5 device to plug in.

<P>
<TABLE>
  <TR align="center"><TD>
<img src="http://www.wirelessmicrocolorcam.com/images/14521825414481619154753.jpeg">
                     <TD>
<img src="https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcQ6Iyv5dGzvneecfQw5W39ECm3P3qj2sT5qxTiZG00JN-4yshCf">
  <TR align="center"><TD>Wall Wart
                     <TD>USB-UART
</TABLE>

  <LI>  40mm standoffs -- This is for stacking the computers together
  <LI>  12mm standoffs -- More stacking hardware. These are for the "feet"
  <LI>  8-port Gigabit Switch -- This switch will drive the node-to-node communication
  <LI>  7 pack 6 inch Cat6 cables -- Notice the short, 6 inch cables. You may choose 
        to make your own cables custom lengths.
  <LI>  1 10 foot Ethernet cable -- This cable is to connect the cluster to my home 
        network
  <LI>  USB3 to Ethernet Adaptor (only one) -- The onboard ethernet will be used for 
        non-to-node communications. This ethernet adapter will be attached to the head 
        node to allow external communications.
  <LI>  6 x 64GB MicroSD UHS-1 card -- These cards are for data storage on each computer.

        <P> Do we need these?  After all, we will have 1TB (or 2) USB3.0 external disk 
            for each xu4.
  <LI>  6 x XU4 Shifter Shield
</UL>

<a name="ConfiguringXU4OperatingSystem"></a>
  <h3>Configuring the ODROID XU4 Operating System 
<a href="http://diybigdata.net/2016/06/configuring-the-odroid-xu4-operating-system/" 
target="_b">(Source Origin)</a></h3>

<P> June 20, 2016 by michael


<p>UPDATE - This post was originally written for HardKernel's distribution of Ubuntu 
15.10, but now has been changed to use the 
<a href="http://forum.odroid.com/viewtopic.php?f=95&amp;t=15261">ODROID server image 
for Ubuntu 14.04 LTS</a>, which is available from HardKernel <a href="http://odroid.in/ubuntu_14.04lts/ubuntu-14.04lts-server-odroid-xu3-20150725.img.xz">here</a>.
 The motivation for this change was to use an official HardKernel support distribution 
that was specifically built for headless server application. Ubuntu 14.04 LTS may be an 
older distribution, but it works for our purposes.</p>

<h4>Temporary Networking Setup</h4>

<p>When setting up the nodes initially, you will need to SSH into them to configure 
their settings. However, if we go straight to our network design, we will not be able 
to connect to any node between the master node is not yet set up as a router. So we 
will need to connect each node directly to the external network (e.g., you home 
network).</p>

<p>Since the <a href="http://diybigdata.net/2016/06/hardware-selection-for-a-low-cost-cluster/#BOM">bill of materials</a>
 called for a 5-port ethernet switch and there are only 4 nodes, we will
 start by having the external network connect directly to the switch.</p>


<P> At the time of this posting, the total cost came to $583.67. If I conveniently 
ignore shipping and tax costs, this is within my $150/node cost target -- Success! 
The computers are being shipped from Korea, so I will have to wait a week or two to 
start putting things together.

<h4>Flashing the eMMC Drives</h4>

<p>The eMMC drives I ordered from Hard Kernel came with Ubuntu 15.10 preinstalled. Due 
to the issues I described above, I found it best just to re-flash the drives with a 
fresh OS install. However, you may skip this part if you are confident your eMMC drive 
image is good.</p>

<p>If the eMMC drives are attached to your nodes, remove them. Each drive came with a 
MicroSD interface that allows you to connect the drive to anything that accepts a 
MicroSD card. Attach the drive to this interface, then attach the interface to something 
that will allow you to connect the drive to your computer. My computer is a MacBook Pro, 
which comes with a builtin SD card reader. So I attached the eMMC drive MicroSD 
interface to a MicroSD to SD adapter.</p>

<figure id="attachment_59" style="width: 601px">
<img src="http://diybigdata.net/wp-content/uploads/2016/06/IMG_4295.jpg" alt="eMMC Drive 
to SD Card Adapter" height="450" width="601">
<figcaption>eMMC drive attached to MicroSD adapter</figcaption>
</figure>

<p>After doing this, I downloaded the Ubuntu 14.04 LTS server image for the XU4 from 
HardKernel, 
<a href="http://odroid.com/dokuwiki/doku.php?id=en:xu3_release_linux_ubuntu">which are 
here</a>.  From there, I followed the instructions in the 
<a href="http://magazine.odroid.com/odroid-xu4/">XU4 user manual</a> for flashing the 
drive. I describe the steps to use on OS X, since that is what I use. Please refer to 
the XU4 manual for other platforms.</p>

<p>Once downloading the Ubuntu image, uncompress it:</p>

<PRE>
xz -d /path/to/compressed-ubuntu-image-file
</PRE>

<p>Then, for each eMMC drive to be flashed, do the following steps. Attach the eMMC 
drive to your computer using the SD card adapter. Then, you will need to find the the 
device name for the eMMC drive. It will have the form of 
<code>/dev/diskX</code>, and can be found by using the OS X command line utility 
<code>diskutil</code>. Once the device name is found, unmount it, and then us 
<code>dd</code> to flash the Ubuntu image eMMC drive. Please be careful here. If you 
mistype the device name, you could seriously mess up the drive on your computer.</p>

<pre>
diskutil list
diskutil unmountdisk /dev/diskX
sudo dd of=/dev/diskX bs=1m if=/path/to/ubuntu-14.04lts-server-odroid-xu3-20150725.img
</pre>

<h4>Configuring the Operating System</h4>

<p>After all the eMMC drives have been flashed, attached them to the XU4s in the 
cluster. Then power up the ethernet switch, but do not yet add power to any node. Then, 
repeat the following steps for each of the nodes in the cluster. For the sake of 
ordering, start with the master node. Before starting, ensure that the MicroSD card is 
not attached to a node, and unsure that the XU4s' boot selector switch is set to the 
eMMC drive.</p>

<ol>
  <li>Attach the power to the node. You will hear the node's fan whirl and see the 
      various LEDs on the device flash. The default configuration of the OS we flashed 
      has the device grab an IP address from a DHCP server.  Since the cluster switch 
      is connected to your home network right now, the device should grab an IP address 
      from your house router. Log into your router or use a network scanning utility to 
      find the IP address assigned to the node that just booted. I use 
<a href="https://geo.itunes.apple.com/us/app/inet-for-ipad-network-scanner/id938231740?mt=8&amp;at=11l4FU&amp;ct=diybigdata">iNet Network Scanner</a> 
      from my iPad. there is also an 
<a href="https://geo.itunes.apple.com/us/app/inet-pro-network-scanner/id305242949?mt=8&amp;at=11l4FU&amp;ct=diybigdata">iPhone version</a>.</li>

  <li>Once you have the device's IP address, remotely SSH into it using the account 
      odroid and password odroid. From the terminal on your personal computer:

<PRE>
ssh odroid@192.168.1.50
</pre>

<p>If you get the login prompt but cannot login with the ordroid account, that means 
the odroid account wasn't created as part of the Ubuntu image. Login instead as user 
root with password odroid. Then, add the odroid user with:</p>

<PRE>
adduser odroid
adduser odroid sudo
apt-get update
apt-get upgrade
exit
</pre>


<p>You will be prompted for a password and other information. Once completed, exit out 
and log into the machine with user odroid.</p></li>

  <li>We need to get and record the hardware MAC address for each node. This will be 
needed in the next post when we set up the router service. Issue the command 
<code>ifconfig</code> into the  shell that you just opened on the newly booted XU4. You 
will get a bunch of text that looks something like this:

<PRE>
eth0      Link encap:Ethernet  HWaddr 00:1e:06:32:2a:7f  
          inet addr:192.168.1.116  Bcast:192.168.1.255  Mask:255.255.255.0
          inet6 addr: fe80::21e:6ff:fe32:2a7f/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:589 errors:0 dropped:0 overruns:0 frame:0
          TX packets:146 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:110461 (110.4 KB)  TX bytes:20988 (20.9 KB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:16 errors:0 dropped:0 overruns:0 frame:0
          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:1216 (1.2 KB)  TX bytes:1216 (1.2 KB)
</PRE>

<p> Look for the block that has the <code>inet addr</code> with the same value as the 
IP address that you logged into, and in that section look for the <code>HWaddr</code> 
item. What follows is the MAC address for the onboard ethernet device. Record it in 
association with what node it is for. On the master node with the USB ethernet dongle 
attached, you can run <code>ifconfig -a</code> to get all available network interfaced. 
The <code>eth1</code> block will be the USB ethernet dongle's network interface. You 
will want to record the dongle's MAC address so that you can set up a static DHCP IP 
address assignment in your home network's router (see your router's manual for 
instructions).</p></li>

  <li>Install and run a very useful tool called 
<a href="https://github.com/mdrjr/odroid-utility">Odroid Utility made by Hard 
Kernel</a>. To do this, issue the following commands:

<PRE>
sudo -s
wget -O /usr/local/bin/odroid-utility.sh https://raw.githubusercontent.com/mdrjr/odroid-utility/master/odroid-utility.sh
chmod +x /usr/local/bin/odroid-utility.sh
odroid-utility.sh
</PRE>

</li>

  <li>You are going to do three things in this utility:

<ol type="A">
  <li><strong>Name the Node</strong> - The "Change Hostname" menu option will give you 
      the opportunity to name the node. Name the node with the ethernet dongle 
      <code>master</code>, and the other three <code>slave1</code>, <code>slave2</code>, 
      and <code>slave3</code>.</li>
  <li><strong>Turn off Xorg</strong> - We are running these nodes as headless servers. 
      The Ubuntu image you just flashed runs a GUI by default. We don't need our nodes 
      loaded with that overhead, so we need to turn off the X server that gets launched 
      by default. Use the menu item "Xorg On/Off" to do this.</li>
  <li><strong>Resize the Boot Drive</strong> - When you flashed the the eMMC drive, it 
      created the root file system with a 4 GB partition. The drives we ordered were 16 
      GB, and the smallest you can buy are 8 GB. Either way, there is unclaimed space 
      on your drive. Use the "Resize your root partition" menu option to resize the root 
      file system to make available all remaining space on the eMMC drive.</li>
</ol></li>

  <li>Update the hosts file on the machine to define short host names for each node in 
      the cluster. Do that by editing the hosts file:

<pre>
vi /etc/hosts
</pre>

<p>Update the file's contents to the following:</p>

<PRE>
127.0.0.1       localhost
10.10.10.1      master
10.10.10.2      slave1
10.10.10.3      slave2
10.10.10.4      slave3
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters
</PRE>

<p>Next, we need to turn off IPv6 on the internal cluster network. Open the following 
file for editing:</p>

<PRE>
sudo vi /etc/sysctl.conf
</PRE>

<p>Add the following lines at the end of the file:</p>

<PRE>
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
</PRE>

</li>

  <li>Shutdown the node.

<PRE>
shutdown -h now
</PRE>


</li>
</ol>

<p>That's it. All four nodes now have their OS set up. In the next post, we will set up 
the router service on the master node.</p>


	<footer>
<a href="http://diybigdata.net/category/low-cost-cluster/">Low Cost Cluster</a>,&nbsp; 
<a href="http://diybigdata.net/category/low-cost-cluster/set-up/">Set Up</a>,&nbsp;
<a href="http://diybigdata.net/tag/networking/">networking</a>,&nbsp; 
<a href="http://diybigdata.net/tag/odroid-xu4/">ODROID XU4</a>,&nbsp; 
<a href="http://diybigdata.net/tag/setup/">setup</a>
	</footer>

  

<h3>Post navigation</h3>
		<div><div><a href="http://diybigdata.net/2016/06/network-design-for-the-low-cost-cluster/" rel="prev">Network Design for the Low Cost Cluster</a></div><div><a href="http://diybigdata.net/2016/06/configuring-dhcp-and-nat-for-odroid-xu4-cluster/">Configuring DHCP and NAT in ODROID XU4 Cluster</a></div></div>

			
<div id="comments"

	
<h3>One thought on "<span>Configuring the ODROID XU4 Operating 
System</span>"</h3>

		
<ol>
  <li id="comment-35">
    Pingback: <a href="#AddingNewNoteToCluster"> Adding a New Node to Cluster - DIY Big Data</a></li>
</ol><!-- .comment-list -->

		
	
	

<aside id="recent-posts-2">		
<h4>Recent Posts</h4>		

<ul>
  <li> <a href="http://diybigdata.net/2016/11/quantcast-file-system-1-2-for-arm71/">Quantcast File System 1.2 for ARM71</a></li>
  <li> <a href="http://diybigdata.net/2016/10/using-custom-hive-udfs-with-pyspark/">Using Custom Hive UDFs With PySpark</a></li>
  <li> <a href="http://diybigdata.net/2016/09/airline-flight-data-analysis-part-2-analyzing-on-time-performance/">Airline Flight Data Analysis - Part 2 - Analyzing On-Time Performance</a></li>
  <li> <a href="#AirlineFlightDataPart1">Airline Flight Data Analysis - Part 1 - Data 
       Preparation</a></li>
  <li> <a href="#AddingNewNoteToCluster">Adding a New Node to the ODROID XU4 
        Cluster</a></li>
</ul>
		</aside>

<a name="AddingNewNoteToCluster"></a>
<h3>Adding a New Node to the ODROID XU4 Cluster 
<a href="http://diybigdata.net/2016/08/adding-a-new-node-to-the-odroid-xu4-cluster/" 
target="_b">(Source Origin)</a></h3>

<P>August 20, 2016 by michael

<p>I recently acquired another ODROID XU4 device (and a 64 GB MicroSD card for bulk 
storage) to add it to my <a href="http://diybigdata.net/odroid-xu4-cluster/">XU4 
cluster</a>.  This new node brings my node count to five. Adding a new node to the
cluster is relatively straight forward, but there are a lot of details. In the modern 
datacenter, this operation would be accomplished through a package manager, which would 
build the new node according to an image. However, I haven't set up package management 
on my cluster so I will need to set up the new node manually.</p>

<p>In the original cluster configuration, I used a 5-port ethernet switch for the 
internal network. Given that there was an open port, no additional hardware beyond the 
new node is needed. However, if I were to add a sixth node (or beyond), I would need to 
update my ethernet switch to something such as an 8-port switch. I will also note that 
using the 40 mm PCB spacers I originally ordered makes the top node in a 5 node stack 
too far away to reach the ethernet switch with the original 6 inch ethernet cables I 
ordered, so I ordered an 8 inch cable</a> to connect the top node to the switch.</p>

<h4>Setup Up a New Node</h4>

<p>The first step in adding a new XU4 node is to set it up as a slave node as described 
in my original post on <a href="#ConfiguringXU4OperatingSystem">setting up the node's 
operating system</a>.  After doing this, you will need to set up the cluster's internal 
network such that the new node get's a predictable IP address. Edit the 
<code>/etc/dhcp/dhcpd.conf</code> file on the master node and add the following subblock 
such as this to the <code>subnet 10.10.10.0</code> block:</p>

<PRE>
       host slave4 {
                option host-name "slave4";
                hardware ethernet 00:1e:06:30:55:63;
                fixed-address 10.10.10.5;
        }
</PRE>

<p>Of course, update the host name, assign IP address and ethernet MAC address as 
appropriate. After doing this, log into each node and update the <code>/etc/hosts</code> 
file to include an entry for the new node. One the new node itself, be sure to fully 
populate the <code>/etc/hosts</code> file for all the nodes in the cluster. Now restart 
the entire cluster to ensure the new networking configuration takes effect.</p>

<p>On the master node, add the new node to the cluster configuration files we originally 
created at <code>~odroid/cluster/all.txt</code> and 
<code>~odroid/cluster/slaves.txt</code>. then, as the odroid user on the master node, 
copy the master node SSH keys to the new node.</p>

<PRE>
ssh-copy-id odroid@slave4
ssh-copy-id root@slave4
</PRE>

<p>After do this, set up the MicroSD card for the new node as described in my post for 
<a href="http://diybigdata.net/2016/06/adding-the-microsd-data-drives-to-the-odroid-xu4-cluster/" 
target="_b">formatting and adding a MicroSD card as bulk storage for a node</a>.</p>

<h4>Installing Hadoop on a New Node</h4>

<p>To set up the new node for Hadoop, first log into it as user <code>odroid</code> to 
create the <code>hduser</code> user and install Java</p>

<PRE>
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser
sudo adduser hduser sudo
sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer
sudo apt-get install rsync
</PRE>

<p>On the master node as user <code>hduser</code>, update the Hadoop configuration to 
see the new node, sync out the updated configuration to the existing slaves, then sync 
out Hadoop to the new node.</p>

<PRE>
# copy hduser SSH keys to new node
ssh-copy-id hduser@slave4

# update Hadoop configuration to see new node by adding the slave to teh slaves file
cd /usr/local/hadoop/etc/hadoop/
vi slaves

# sync out new configuration to existing nodes
rsync -avxP /usr/local/hadoop/etc/hadoop/ hduser@slave1:/usr/local/hadoop/etc/hadoop/
rsync -avxP /usr/local/hadoop/etc/hadoop/ hduser@slave2:/usr/local/hadoop/etc/hadoop/
rsync -avxP /usr/local/hadoop/etc/hadoop/ hduser@slave3:/usr/local/hadoop/etc/hadoop/

# sync out hadoop to new node
sudo rsync -avxP /opt/hadoop-2.7.2/ root@slave4:/opt/hadoop-2.7.2/
</PRE>

<p>Back on the new node as <code>hduser</code>, perform the final configuration of the 
node:</p>

<PRE>
sudo mkdir -p /data/hdfs/tmp
sudo chown -R hduser:hadoop /data/hdfs
sudo chown -R hduser:hadoop /opt/hadoop-2.7.2
sudo ln -s /opt/hadoop-2.7.2 /usr/local/hadoop
</PRE>

<p>Now you can start HDFS on the master node as <code>hduser</code>:</p>

<PRE>
start-dfs.sh
</PRE>

<p>You will see the new node get an HDFS daemon launched. If you check out the HDFS 
monitor page at <code>http://&lt;cluster IP address&gt;:50070</code>, you can see that 
the new node does not have any data installed. It would be a good idea to balance your 
cluster. This will take a while:</p>

<PRE>
hdfs balancer
</PRE>

<h4>Installing Spark on the New Node</h4>

<p>The first step here is to update the configuration of Spark on the master node to be 
able to see the new node.</p>

<PRE>
cd /usr/local/spark/conf/
vi slaves
</PRE>

<p>Add the new node name to the slaves file. Sync out the new configuration to the 
existing slaves, and Spark to the new node.</p>

<PRE>
rsync -avxP /usr/local/spark/conf/ hduser@slave1:/usr/local/spark/conf/
rsync -avxP /usr/local/spark/conf/ hduser@slave2:/usr/local/spark/conf/
rsync -avxP /usr/local/spark/conf/ hduser@slave3:/usr/local/spark/conf/
sudo rsync -avxP /opt/spark-1.6.2-bin-hadoop2.6 root@slave4:/opt/
</PRE>

<p>Then on the new node, finalize the Spark set up:</p>

<PRE>
sudo chown -R hduser:hadoop /opt/spark-1.6.2-bin-hadoop2.6
sudo ln -s /opt/spark-1.6.2-bin-hadoop2.6 /usr/local/spark
sudo mkdir -p /data/spark
sudo chown hduser:hadoop /data/spark
sudo apt-get install python3
</PRE>

<p>To start Spark with the new node, on the master node as user <code>hduser</code>:</p>

<PRE>
/usr/local/spark/sbin/start-all.sh
</PRE>

<p>With that, you have fully added the new node to the ODROID XU4 cluster.</p>

	<footer>
<a href="http://diybigdata.net/category/low-cost-cluster/hardware/">Hardware</a>, 
<a href="http://diybigdata.net/category/low-cost-cluster/set-up/">Set Up</a>,
<a href="http://diybigdata.net/tag/hadoop/" rel="tag">Hadoop</a>, 
<a href="http://diybigdata.net/tag/hdfs/">HDFS</a>, 
<a href="http://diybigdata.net/tag/microsd/">MicroSD</a>, 
<a href="http://diybigdata.net/tag/odroid-xu4/">ODROID XU4</a>, 
<a href="http://diybigdata.net/tag/setup/">setup</a>, 
<a href="http://diybigdata.net/tag/spark/">Spark</a>
	</footer>

  
<h4>Post navigation</h4>

<UL>
  <LI><a href="http://diybigdata.net/2016/08/connecting-to-hdfs-from-an-computer-external-to-the-cluster/">Connecting to HDFS from an computer external to the cluster</a>
  <LI><a href="#AirlineFlightDataPart1">Airline Flight Data Analysis - Part 1 - Data 
      Preparation</a></div></div>
</UL>
	
 <h4>Recent Posts</h4>		

<ul>
  <li> <a href="http://diybigdata.net/2016/11/quantcast-file-system-1-2-for-arm71/">Quantcast File System 1.2 for ARM71</a></li>
  <li> <a href="http://diybigdata.net/2016/10/using-custom-hive-udfs-with-pyspark/">Using Custom Hive UDFs With PySpark</a></li>
  <li> <a href="http://diybigdata.net/2016/09/airline-flight-data-analysis-part-2-analyzing-on-time-performance/">Airline Flight Data Analysis - Part 2 - Analyzing On-Time Performance</a></li>
  <li> <a href="#AirlineFlightDataPart1">Airline Flight Data Analysis - Part 1 - Data Preparation</a></li>
  <li> <a href="#AddingNewNoteToCluster">Adding a New Node to the ODROID XU4 Cluster</a></li>
</ul>
	

<a name="AirlineFlightDataPart1"></a>
<h3>Airline Flight Data Analysis - Part 1 - Data Preparation 
<a href="http://diybigdata.net/2016/08/airline-flight-data-analysis-data-preparation/" 
target="_b">(Source Origin)</a></h3>

<P>August 28, 2016 by michael


<p>This data analysis project is to explore what insights can be derived from the 
Airline On-Time Performance data set collected by the United States Department of 
Transportation. The data can be downloaded in month chunks from 
<a href="http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;
DB_Short_Name=On-Time">the Bureau of Transportation Statistics website</a>. The data 
gets downloaded as a raw CSV file, which is something that 
<a href="http://spark.apache.org/">Spark</a> can easily load. However, if you download 
10+ years of data from the Bureau of Transportation Statistics (meaning you downloaded 
120+ one month CSV files from the site), that would collectively represent 30+ GB 
of data. For commercial scale Spark clusters, 30 GB of text data is a trivial task. 
However, if you are running Spark on the 
<a href="http://diybigdata.net/odroid-xu4-cluster/">ODROID XU4 cluster</a> or in local 
mode on your Mac laptop, 30+ GB of text data is substantial. So, before we can do any 
analysis of the dataset, we need to transform it into a format that will allow us to 
quickly and efficiently interact with it.</p>

<p>Fortunately, data frames and the <a href="https://parquet.apache.org/">Parquet file 
format</a> fit the bill nicely. Parquet is a compressed columnar file format. Columnar 
file formats greatly enhance data file interaction speed and compression by organizing 
data by columns rather than by rows. The two main advantages of a columnar format is 
that queries will deserialize only that data which is actually needed, and compression 
is frequently much better since columns frequently contained highly repeated values.</p>

<p>To explain why the first benefit is so impactful, consider a structured data table 
with the following format:</p>

<PRE>
FlightData
 |-- FlightDate: date (nullable = true)
 |-- UniqueCarrier: string (nullable = true)
 |-- AirlineID: long (nullable = true)
 |-- Carrier: string (nullable = true)
 |-- TailNum: string (nullable = true)
 |-- FlightNum: integer (nullable = true)
 |-- Origin: string (nullable = true)
 |-- Dest: string (nullable = true)
</PRE>

<p>And for the sake of discussion, consider this query against the table:</p>

<PRE>
SELECT Carrier, Year, COUNT( DISTINCT TailNum ) FROM FlightData GROUP BY Carrier, Year
</PRE>

<p>As you can see, there are only three fields from the original table that matter to 
this query, Carrier, Year and TailNum. In a traditional row format, such as CSV, in 
order for a data engine (such as Spark) to get the relevant data from each row to 
perform the query, it actually has to read the entire row of data to find the fields it 
needs. If the data table has many columns and the query is only interested in three, the 
data engine will be force to deserialize much more data off the disk than is needed. In 
any data operation, reading the data off disk is frequently the slowest operation. Doing 
anything to reduce the amount of data that needs to be read off the disk would speed up 
the operation significantly. Solving this problem is exactly what a columnar data format 
like Parquet is intended to solve.</p>

<p>A CSV file is a row-centric format. If you want to interact with a large data table 
backed by CSV files, it will be slow. You can, however, speed up your interactions with 
the CSV data by converting it to a columnar format. This will be our first goal with the 
Airline On-Time Performance data.</p>

<p><em>One thing to note with the the process described below: I am using <a target="_b" 
href="http://diybigdata.net/2016/07/quantcast-file-system-and-spark-on-odroid-cluster/">QFS 
with Spark</a> to do my analysis. If you prefer to use 
<a href="http://diybigdata.net/2016/07/installing-spark-onto-odroid-xu4-cluster/" 
target="_b">HDFS with Spark</a>, simply update all file paths and file system commands 
as appropriate.</em></p>

<h4>Getting Airline On-Time Performance Data</h4>

<p>As indicated above, the Airline Io-Time Performance data is available at 
<a href="http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;
DB_Short_Name=On-Time">the Bureau of Transportation Statistics website</a>. 
The challenge with downloading the data is that you can only download one month at a 
time. Therefore, to download 10 years worth of data, you would have to adjust the 
selection month and download 120 times. This is time consuming. But I went ahead and 
downloaded eleven years worth of data so you don't have to. You can download it here:</p>

<p style="text-align: center;"></p>
<a href="http://diybigdata.net/?smd_process_download=1&amp;download_id=392">Airline 
On-Time Performance Data 2005-2015</a><p></p>

<p>I have also made a smaller, 3-year data set available here:</p>

<p style="text-align: center;"></p>
<a href="http://diybigdata.net/?smd_process_download=1&amp;download_id=399">Airline 
On-Time Performance Data 2013-2015</a><p></p>

<p>Note that expanding the 11 year data set will create a folder that is 33 GB in size. 
If you are doing this on the master node of the ODROID cluster, that is far too large 
for the eMMC drive. You could expand the file into the MicroSD card found at the /data 
mount point, but I wouldn't recommend it as that is half the MicroSD card's space (at 
least the 64 GB size I originally specced). I would suggest two workable options: attach 
a sufficiently sized USB thumb drive to the master node (ideally a USB 3 thumb drive) 
and use that as a working drive, or download the data to your personal computer or 
laptop and access the data from the master node through a file sharing method.</p>

<p>I went with the second method. To "mount" my Mac laptop from the cluster's mast now, 
I used <code>sshfs</code> which simulates a mounted hard drive through behind-the-scenes 
SSH and SCP commands. This, of course, required my Mac laptop to have SSH connections 
turned on. To install  and create a mount point:</p>

<PRE>
sudo apt-get install sshfs
sudo mkdir /mnt/macbookpro
sudo sshfs -o allow_other myaccount@XX.XX.XX.XX:/ /mnt/macbookpro
</PRE>

<p>Update the name of the mount point, IP address of your computer, and your account on 
that computer as necessary.</p>

<p>Once you have downloaded and uncompressed the dataset, the next step is to place 
the data on the distributed file system. QFS has has some nice tools that mirror many 
of the HDFS tools and enable you to do this easily. The key command being the 
<code>cptoqfs</code> command. I prefer uploading the files to the file system one at a 
time. To do that, I wrote this script (update the various file paths for your set up):</p>

<PRE>
#!/bin/bash
for filename in /mmt/macbookpro/path/to/airline-data/*.csv; do
        echo "uploading - ${filename}"
        ( cptoqfs -s master -p 20000 -d "${filename}" -k /user/michael/data/airline -r 2 )
done
</PRE>

<p>This will take a couple hours on the ODROID Xu4 cluster as you are upload 33 GB of 
data.</p>

<h4>Converting Data to a Partitioned Parquet File</h4>

<p>The next step is to convert all those CSV files uploaded to QFS is to convert them 
to the Parquet columnar format. This will be challenging on our <a target="_b" 
href="http://diybigdata.net/odroid-xu4-cluster/">ODROID XU4 cluster</a> because there 
is not sufficient RAM across all the nodes to hold all of the CSV files for processing. 
Furthermore, the cluster can easily run out of disk space or the computations become 
unnecessarily slow if the means by which we combine the 11 years worth of CSVs requires 
a significant amount of shuffling of data between nodes. In general, shuffling data 
between nodes should be minimized, regardless of your cluster's size.</p>

<p>To minimize the need to shuffle data between nodes, we are going to transform each 
CSV file directly into a partition within the overall Parquet file. A partition is a 
subset of the data that all share the same value for a particular key. Parquet files 
can create partitions through a folder naming strategy. For example, if data in a 
Parquet file is to be partitioned by the field named year, the Parquet file's folder 
structure would look like this:</p>

<PRE>
data.paquet/
 |-- year=2005/
 |    |-- part-r-00000.parquet
 |    |-- part-r-00001.parquet
 |-- year=2006/
 |    |-- part-r-00002.parquet
 |    |-- part-r-00003.parquet
 |-- year=2007/
 |    |-- part-r-00004.parquet
 |    |-- part-r-00005.parquet
 .
 .
 .
 |-- year=2016/
      |-- part-r-00023.parquet
      |-- part-r-00024.parquet
</PRE>

<p>The advantage of partitioning data in this manner is that a client of the data only 
needs to read a subset of the data if it is only interested in a subset of the 
partitioning key values. As an example, consider this SQL query:</p>

<PRE>
SELECT Carrier, Year, COUNT( DISTINCT TailNum ) 
FROM FlightData
WHERE Year in (2006,2007,2008) 
GROUP BY Carrier, Year
</PRE>

<p>The <code>WHERE</code> clause indicates that the query is only interested in the 
years 2006 through 2008. This fact can be taken advantage of with a data set partitioned 
by year in that only data from the partitions for the targeted years will be read when 
calculating the query's results. As a result, the partitioning has greatly sped up the 
query bit reducing the amount of data that needs to be deserialized from disk.</p>

<p>The other property of partitioned Parquet files we are going to take advantage of is 
that each partition within the overall file can be created and written fairly 
independently of all other partitions. What this means is that one node in the cluster 
can write one partition with very little coordination with the other nodes, most notably 
with very little to no need to shuffle data between nodes. For 11 years of the airline 
data set there are 132 different CSV files. Since those 132 CSV files were already 
effectively partitioned, we can minimize the need for shuffling by mapping each CSV file 
directly into its partition within the Parquet file.</p>

<p>The way to do this is to map each CSV file into its own partition within the Parquet 
file. Since each CSV file in the Airline On-Time Performance data set represents exactly 
one month of data, the natural partitioning to pursue is a month partition.</p>

<h4>The Code</h4>

<p>So now that we understand the plan, we will execute own it. The first step is to 
lead each CSV file into a data frame. An important element of doing this is setting 
the schema for the data frame. Create a notebook in Jupyter dedicated to this data 
transformation, and enter this into the first cell:</p>

<PRE>
from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import DoubleType, IntegerType, StringType, DecimalType, LongType

air_schema = StructType([
    StructField("Year", IntegerType()),
    StructField("Quarter", IntegerType()),
    StructField("Month", IntegerType()),
    StructField("DayofMonth", IntegerType()),
    StructField("DayOfWeek", IntegerType()),
    StructField("FlightDate", StringType()),
    StructField("UniqueCarrier", StringType()),
    StructField("AirlineID", LongType()),
    StructField("Carrier", StringType()),
    StructField("TailNum", StringType()),
    StructField("FlightNum", IntegerType()),
    StructField("OriginAirportID", IntegerType()),
    StructField("OriginAirportSeqID", IntegerType()),
    StructField("OriginCityMarketID", IntegerType()),
    StructField("Origin", StringType()),
    StructField("OriginCityName", StringType()),
    StructField("OriginState", StringType()),
    StructField("OriginStateFips", IntegerType()),
    StructField("OriginStateName", StringType()),
    StructField("OriginWac", IntegerType()),
    StructField("DestAirportID", IntegerType()),
    StructField("DestAirportSeqID", IntegerType()),
    StructField("DestCityMarketID", IntegerType()),
    StructField("Dest", StringType()),
    StructField("DestCityName", StringType()),
    StructField("DestState", StringType()),
    StructField("DestStateFips", IntegerType()),
    StructField("DestStateName", StringType()),
    StructField("DestWac", IntegerType()),
    StructField("CRSDepTime", StringType()),
    StructField("DepTime", StringType()),
    StructField("DepDelay", DoubleType()),
    StructField("DepDelayMinutes", DoubleType()),
    StructField("DepDel15", DoubleType()),
    StructField("DepartureDelayGroups", IntegerType()),
    StructField("DepTimeBlk", StringType()),
    StructField("TaxiOut", DoubleType()),
    StructField("WheelsOff", StringType()),
    StructField("WheelsOn", StringType()),
    StructField("TaxiIn", DoubleType()),
    StructField("CRSArrTime", StringType()),
    StructField("ArrTime", StringType()),
    StructField("ArrDelay", DoubleType()),
    StructField("ArrDelayMinutes", DoubleType()),
    StructField("ArrDel15", DoubleType()),
    StructField("ArrivalDelayGroups", IntegerType()),
    StructField("ArrTimeBlk", StringType()),
    StructField("Cancelled", DoubleType()),
    StructField("CancellationCode", StringType()),
    StructField("Diverted", DoubleType()),
    StructField("CRSElapsedTime", DoubleType()),
    StructField("ActualElapsedTime", DoubleType()),
    StructField("AirTime", DoubleType()),
    StructField("Flights", DoubleType()),
    StructField("Distance", DoubleType()),
    StructField("DistanceGroup", IntegerType()),
    StructField("CarrierDelay", DoubleType()),
    StructField("WeatherDelay", DoubleType()),
    StructField("NASDelay", DoubleType()),
    StructField("SecurityDelay", DoubleType()),
    StructField("LateAircraftDelay", DoubleType()),
    StructField("FirstDepTime", StringType()),
    StructField("TotalAddGTime", StringType()),
    StructField("LongestAddGTime", StringType()),
    StructField("DivAirportLandings", StringType()),
    StructField("DivReachedDest", StringType()),
    StructField("DivActualElapsedTime", StringType()),
    StructField("DivArrDelay", StringType()),
    StructField("DivDistance", StringType()),
    StructField("Div1Airport", StringType()),
    StructField("Div1AirportID", StringType()),
    StructField("Div1AirportSeqID", StringType()),
    StructField("Div1WheelsOn", StringType()),
    StructField("Div1TotalGTime", StringType()),
    StructField("Div1LongestGTime", StringType()),
    StructField("Div1WheelsOff", StringType()),
    StructField("Div1TailNum", StringType()),
    StructField("Div2Airport", StringType()),
    StructField("Div2AirportID", StringType()),
    StructField("Div2AirportSeqID", StringType()),
    StructField("Div2WheelsOn", StringType()),
    StructField("Div2TotalGTime", StringType()),
    StructField("Div2LongestGTime", StringType()),
    StructField("Div2WheelsOff", StringType()),
    StructField("Div2TailNum", StringType()),
    StructField("Div3Airport", StringType()),
    StructField("Div3AirportID", StringType()),
    StructField("Div3AirportSeqID", StringType()),
    StructField("Div3WheelsOn", StringType()),
    StructField("Div3TotalGTime", StringType()),
    StructField("Div3LongestGTime", StringType()),
    StructField("Div3WheelsOff", StringType()),
    StructField("Div3TailNum", StringType()),
    StructField("Div4Airport", StringType()),
    StructField("Div4AirportID", StringType()),
    StructField("Div4AirportSeqID", StringType()),
    StructField("Div4WheelsOn", StringType()),
    StructField("Div4TotalGTime", StringType()),
    StructField("Div4LongestGTime", StringType()),
    StructField("Div4WheelsOff", StringType()),
    StructField("Div4TailNum", StringType()),
    StructField("Div5Airport", StringType()),
    StructField("Div5AirportID", StringType()),
    StructField("Div5AirportSeqID", StringType()),
    StructField("Div5WheelsOn", StringType()),
    StructField("Div5TotalGTime", StringType()),
    StructField("Div5LongestGTime", StringType()),
    StructField("Div5WheelsOff", StringType()),
    StructField("Div5TailNum", StringType())
])
</PRE>

<p>That's a lot of lines, but it's a complete schema for the Airline 
On-Time Performance data set. In order to leverage this schema to create
 one data frame for each CSV file, the next cell should be:</p>

<PRE>
import itertools
year_list = ['2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']
month_list = ['1','2','3','4','5','6','7','8','9','10','11','12']

air_df_dict = {}

print('Gathering files ...')
for (year_str,month_str) in list(itertools.product(year_list,month_list)):
    year_month_str = '%s_%s'%(year_str,month_str)
    print('%s, '%(year_month_str), end="")
    air_df_dict[year_month_str] = spark.read.csv( 
        'qfs://master:20000/user/michael/data/airline/On_Time_On_Time_Performance_%s.csv'%(year_month_str), 
        header=True, 
        schema=air_schema,
        escape='"')
print('Done!')
</PRE>

<p>What this cell does is iterate through every possible year-month 
combination for our data set, and load the corresponding CSV into a data
 frame, which we save into a dictionary keyed by the year-month. 
However, these data frames are not in the final form I want. There are a
 number of columns I am not interested in, and I would like the date 
field to be an actual date object.</p>

<PRE>
from datetime import datetime
from pyspark.sql.functions import col, udf, unix_timestamp
from pyspark.sql.types import DateType
from pyspark import StorageLevel

convertToDate = udf(lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())

airline_data_parts = []

# Should really coalesce to 1 here, but that strains the ODROID XU4 cluster too
# much.
print('Processing ', end="")
for year_month_str, air_df in air_df_dict.items():
    print('%s, '%(year_month_str), end="")
    airline_data = air_df.select(
            "Year","Quarter","Month","DayofMonth","DayOfWeek","FlightDate","UniqueCarrier","AirlineID",
            "Carrier","TailNum","FlightNum","OriginAirportID","OriginAirportSeqID","OriginCityMarketID",
            "Origin","OriginCityName","OriginState","OriginStateFips","OriginStateName","OriginWac",
            "DestAirportID","DestAirportSeqID","DestCityMarketID","Dest","DestCityName","DestState",
            "DestStateFips","DestStateName","DestWac","CRSDepTime","DepTime","DepDelay","DepDelayMinutes",
            "DepDel15","DepartureDelayGroups","DepTimeBlk","TaxiOut","WheelsOff","WheelsOn","TaxiIn","CRSArrTime",
            "ArrTime","ArrDelay","ArrDelayMinutes","ArrDel15","ArrivalDelayGroups","ArrTimeBlk","Cancelled",
            "CancellationCode","Diverted","CRSElapsedTime","ActualElapsedTime","AirTime","Flights","Distance",
            "DistanceGroup","CarrierDelay","WeatherDelay","NASDelay","SecurityDelay","LateAircraftDelay"
        ).withColumn(
            'FlightDate', convertToDate(col('FlightDate'))
        )
    
    airline_data_parts.append(airline_data)

print('Done!')
</PRE>

<p>Finally, we need to combine these data frames into one partitioned 
Parquet file. The way to do this is to use the union() method on the 
data frame object which tells spark to treat two data frames (with the 
same schema) as one data frame. This method doesn't necessarily shuffle 
any data around, simply logically combining the partitions of the two 
data frames together. Since we have 132 files to union, this would have 
to be done incrementally. Once we have combined all the data frames
 together into one logical set, we write it to a Parquet file 
partitioned by Year and Month. Note that this is a two-level 
partitioning scheme. Since the sourcing CSV data 
is effectively already partitioned by year and month, what this 
operation effectively does is pipe the CSV file through a data frame 
transformation and then into it's own partition in a larger, combined 
data frame. No shuffling to redistribute data occurs.</p>

<PRE>
master_data = airline_data_parts[0]

print('Unionizing data frames 0, ', end="")
for i in range(1,len(airline_data_parts)):
    print('%d, '%(i), end="")
    master_data = master_data.union(airline_data_parts[i])
print(" Done!")
print('Starting export to HDFS...')
master_data.write.partitionBy(
        "Year","Month"
    ).parquet(
        'qfs://master:20000/user/michael/data/airline_data',
        mode='overwrite'
    )
print('Done!')
</PRE>

<p>On my ODROID XU4 cluster, this conversion process took a little under
 3 hours. However, the one-time cost of the conversion significantly 
reduces the time spent on analysis later. While we are certainly jumping
 through some hoops to allow the small XU4 cluster to handle some 
relatively large data sets, I would assert that the methods used here 
are just as applicable at scale. You always want to minimize the 
shuffling of data; things just go faster when this is done. Therein lies
 why I enjoy working out these problems on a small cluster, as it forces
 me to think through how the data is going to get transformed, and in 
turn helping me to understand how to do it better at scale.</p>

<p>The last step is to convert the two meta-data files that pertain
 to airlines and airports into Parquet files to be used later. These 
files were included with the either of the data sets above.</p>

<PRE>
from pyspark.sql import Row

def mapAirlineIdRow(r):
    airline_id = int(r.Code)
    airline_name_parts = r.Description.split(':')
    airline_name = airline_name_parts[0].strip()
    iata_carrier = airline_name_parts[1].strip()
    out = Row(
        AirlineID=airline_id,
        AirlineName=airline_name,
        Carrier=iata_carrier
    )
    return out;

airline_id_csv = spark.read.csv(
    'qfs://master:20000/user/michael/data/airline/airline-id-lookup-table.csv',
    header=True,
    escape='"'
)

airline_id_df = airline_id_csv.rdd.map(mapAirlineIdRow).toDF().coalesce(1)
airline_id_df.write.parquet(
        'qfs://master:20000/user/michael/data/airline_id_table',
        mode='overwrite'
    )
    
airline_id_df.take(1)

airport_schema = StructType([
    StructField("Code", StringType()),
    StructField("Description", StringType()),
])

def mapAirportIdRow(r):
    airport_id = r.Code
    airport_city = ''
    airport_name = ''
    airport_name_parts = r.Description.split(':')
    if len(airport_name_parts) is 2:
        airport_city = airport_name_parts[0].strip()
        airport_name = airport_name_parts[1].strip()
    elif len(airport_name_parts) is 1:
        airport_city = airport_name_parts[0]
        airport_name = r.Code
    
    out = Row(
        AirportID=airport_id,
        City=airport_city,
        Name=airport_name
    )
    return out;

airport_id_csv = spark.read.csv(
    'qfs://master:20000/user/michael/data/airline/airport-information.csv',
    header=True,
    escape='"',
    schema=airport_schema
)

airport_id_df = airport_id_csv.rdd.map(mapAirportIdRow).toDF().coalesce(1)
airport_id_df.write.parquet(
        'qfs://master:20000/user/michael/data/airport_id_table',
        mode='overwrite'
    )

airport_id_df.take(1)
</PRE>

<p>All this code can be found in <a href="https://github.com/DIYBigData/spark-data-analysis-projects/blob/master/airline-data/airline-data-to-parquet.ipynb">my Github repository here</a>.</p>

<p>Next I will be walking through some analyses f the data set.</p>
			</div><!-- .entry-content -->

	<footer>
<a href="http://diybigdata.net/category/general/data-analysis-general/">Data 
   Analysis</a>, 
<a href="http://diybigdata.net/tag/airline-data/">Airline Data</a>, 
<a href="http://diybigdata.net/tag/big-data/">Big Data</a>, 
<a href="http://diybigdata.net/tag/parquet/">parquet</a>, 
<a href="http://diybigdata.net/tag/spark/">Spark</a> 
        </footer><!-- .entry-footer -->

  
</article><!-- #post-## -->

      
<h3>Post navigation</h3>

<UL>
  <LI><a href="#AddingNewNoteToCluster">Adding a New Node to the ODROID XU4 Cluster</a>
  <LI><a href="http://diybigdata.net/2016/09/airline-flight-data-analysis-part-2-analyzing-on-time-performance/">Airline Flight Data Analysis - Part 2 - Analyzing On-Time Performance</a>
</UL>


			
<div id="comments">

	
<h3>One thought on "<span>Airline Flight Data Analysis - Part 1 - 
Data Preparation</span>"</h3>

		
<ol>
  <li id="comment-39">
      Pingback: <a href="#AirlineFlightDataPart2">Airline 
    Flight Data Analysis - Part 2 - Analyzing On-Time Performance - DIY Big Data</a></li>
</ol><!-- .comment-list -->

		
	
	
<h4>Recent Posts</h4>		

<ul>
  <li><a href="http://diybigdata.net/2016/11/quantcast-file-system-1-2-for-arm71/">Quantcast File System 1.2 for ARM71</a></li>
  <li><a href="http://diybigdata.net/2016/10/using-custom-hive-udfs-with-pyspark/">Using Custom Hive UDFs With PySpark</a></li>
  <li><a href="#AirlineFlightDataPart2">Airline Flight Data Analysis - Part 2 - Analyzing On-Time Performance</a></li>
  <li><a href="#AirlineFlightDataPart1">Airline Flight Data Analysis - Part 1 - Data 
         Preparation</a></li>
  <li><a href="#AddingNewNoteToCluster">Adding a New Node to the ODROID XU4 Cluster</a></li>
</ul>

<a name="AirlineFlightDataPart2"></a>
<h3>Airline Flight Data Analysis - Part 2 - Analyzing On-Time 
Performance 
<a href="http://diybigdata.net/2016/09/airline-flight-data-analysis-part-2-analyzing-on-time-performance/">(Source Origin)</a></h3>

	<span>September 3, 2016 michael</span>

<p>In <a target="_b" 
href="http://diybigdata.net/2016/08/airline-flight-data-analysis-data-preparation/">my 
last post on this topic</a>, we loaded the&nbsp;Airline On-Time Performance data set 
collected by the United States Department of Transportation into a Parquet file to 
greatly improve the speed at which the data can be analyzed. Now, let's take a first 
look at the data by graphing the average airline-caused flight delay by airline. This 
is a rather straightforward analysis, but is a good one to get started with the data 
set.</p>

<p>Open a Jupyter python notebook on the cluster in the first cell indicate that we will 
be using <a href="http://matplotlib.org/">MatPlotLib</a> to do graphing:</p>

<PRE>
%matplotlib inline
</PRE>


<p>Then, in the next cell load data frames for the airline on time activity and airline 
meta data based on the parquet files built in the last post. Note that I am using 
<a href="https://github.com/quantcast/qfs">QFS</a> as my distributed file system. If you 
are using HDFS, simply update the file URLs as needed.</p>

<PRE>
air_data = spark.read.parquet('qfs://master:20000/user/michael/data/airline_data')
airlines = spark.read.parquet('qfs://master:20000/user/michael/data/airline_id_table')
</PRE>


<p>Now we are ready to process the data. The next cell will essential do a group by and 
average type query, but it does some important things first for efficiency. First, we 
immediately select one those columns we care about from the data frame, specifically 
<code>Carrier</code>, <code>Year</code>, <code>Month</code>, and <code>ArrDelay</code>. 
This give Spark and Parquet a chance to create efficiencies by only reading the data 
that pertains to those columns. The second step is to filter out those rows that don't 
pertain to the airlines we want to analyze. The next two <code>groupBy</code> and 
<code>agg</code> steps find the average delay for each airline by month. Then the query 
creates a new column <code>YearMonth</code> which is a display string for year and 
month, and drops the now extraneous  <code>Year</code> and <code>Month</code> 
columns.</p>


<PRE>
from pyspark.sql.functions import avg, udf, col
from pyspark.sql.types import StringType
def getYearMonthStr(year, month):
    return '%d-%02d'%(year,month)
udfGetYearMonthStr = udf(getYearMonthStr, StringType())
airline_delay = air_data.select(
    'Carrier',
    'Year',
    'Month',
    'ArrDelay'
).filter(
    col('Carrier').isin('AA','WN','DL','UA','MQ','EV','AS','VX')
).groupBy(
    'Carrier',
    'Year',
    'Month'
).agg(
    avg(col('ArrDelay')).alias('average_delay')
).withColumn(
    'YearMonth', udfGetYearMonthStr('Year','Month')
).drop(
    'Year'
).drop(
    'Month'
).orderBy(
    'YearMonth','Carrier'
).cache()

</PRE>

<p>The final step is to create of each airline's average delay over time. Here I want to 
use the human readable names of each airline, so the first task is to create a name 
dictionary for the airlines keyed by the airline ID. Then we create the graph. In both 
of these steps, the Spark data frame is converted to a pandas data frame, which has the 
effect of collecting all of the data in theSpark data frame to the master node. 
Generally, you have to be careful about doing this as the master node has limited 
memory. Due to the concise pull of data with the initial filter step on the data frame, 
the data set quite easily fits within the master node's RAM.</p>

<p>To create the graph, I use the fairly standard matplotlib line graph. Since I have 
multiple airlines to graph, you will note I have to plot each airline separately into 
the graph. The x-axis are the year &amp; month pairs, and these need to be converted 
into actual <code>datetime</code> objects before creating the airline specific graph 
line.</p>

<PRE>
import numpy as np
import matplotlib.pyplot as plt
import datetime as dt
airline_delay_pd = airline_delay.toPandas()
carriers = ['AA','WN','DL','UA','AS','VX']
name_dict = airlines.filter(
    col('Carrier').isin('AA','WN','DL','UA','MQ','EV','AS','VX')
).select(
    'Carrier',
    'AirlineName'
).toPandas().set_index('Carrier')['AirlineName'].to_dict()
fig, ax = plt.subplots()
for carrier in carriers:
    carrier_data = airline_delay_pd[airline_delay_pd['Carrier']==carrier]
    dates_raw = carrier_data['YearMonth']
    dates = [dt.datetime.strptime(date,'%Y-%m') for date in dates_raw]
    ax.plot(dates, carrier_data['average_delay'], label = "{0}".format(name_dict[carrier]))
fig.set_size_inches(16,12)
plt.xlabel("Month")
plt.ylabel("Average Delay")
ax.legend(loc='lower left')
plt.show()
</PRE>


<p>Running this notebook should yield the following inline graph:</p>

<p><a href="http://diybigdata.net/wp-content/uploads/2016/08/average-airline-delay.png">
<img src="http://diybigdata.net/wp-content/uploads/2016/08/average-airline-delay.png" 
alt="Average Airline Delay" height="718" width="947"></a></p>

<p>What is interesting to me about this graph is that the (selected) airlines look to 
have correlated trends in their average delay. Rather than drawing conclusions about 
correlation by eye-balling it, we can use a statistical measure of correlation called 
the <a href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson 
Correlation Coefficient.</a> Th correlation coefficient will give a measure of just how 
well correlated two data series are. Apache Spark has 
<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions.corr">a 
native data frame method to calculate the correlation coefficient</a>, but to use it, we 
need to reformat the data some. Specifically, we need to create a data frame where there 
is a column for each distinct airline with each row being a specific month's average 
delay, as the correlation function compares two columns from a data frame. To accomplish 
this, I used a code pattern 
<a href="http://stackoverflow.com/questions/33732346/spark-dataframe-transform-multiple-rows-to-column">described 
in a Stack Overflow question</a>:</p>


<PRE>
from pyspark.sql.functions import col, when, max
# convert the airline rows to columns
airlines = sorted(airline_delay.select(
        'Carrier'
    ).distinct().rdd.map(lambda row: row[0]).collect())
cols = [when(col('Carrier') == a, col('average_delay')).otherwise(None).alias(a) 
        for a in airlines]
maxs = [max(col(a)).alias(a) for a in airlines]
airline_delay_reformed = airline_delay.select(
        col("YearMonth"), 
        *cols
    ).groupBy(
        "YearMonth"
    ).agg(*maxs).na.fill(0).orderBy(
        'YearMonth'
    ).cache()
</PRE>


<p>This will produce a data frame with the following schema:</p>


<PRE>
airline_delay_reformed
 |-- YearMonth
 |-- AA
 |-- AS
 |-- DL
 |-- EV
 |-- MQ
 |-- UA
 |-- VX
 |-- WN

</PRE>


<p>Now, to calculate the correlations in flight delays between all the airlines:</p>

<PRE>
from pyspark.sql import DataFrameStatFunctions
import pandas as pd
import numpy as np
corr_data = dict(
        (
            a,
            [airline_delay_reformed.stat.corr(a,b) if a > b else np.nan for b in airlines]
        ) for a in airlines
    )
correlations_df = pd.DataFrame(corr_data, index=airlines)
print(correlations_df)
</PRE>


<p>Note that by using the <code>if</code>  statement in the list comprehension we limit 
our computational work to calculating the correlation score between two airlines only 
once. One calculated, you should see a result that looks like this:</p>

<PRE>
    AA        AS        DL        EV        MQ        UA        VX        WN
AA NaN  0.468001  0.580626  0.551466  0.718490  0.778896  0.029129  0.503715
AS NaN       NaN  0.526844  0.443933  0.345010  0.522625 -0.137774  0.192667
DL NaN       NaN       NaN  0.715151  0.571382  0.509096 -0.052562  0.480522
EV NaN       NaN       NaN       NaN  0.594915  0.600003  0.056209  0.485396
MQ NaN       NaN       NaN       NaN       NaN  0.647358  0.166137  0.656206
UA NaN       NaN       NaN       NaN       NaN       NaN  0.171475  0.517596
VX NaN       NaN       NaN       NaN       NaN       NaN       NaN  0.275750
WN NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN

</PRE>


<p>The way to interpret correlation scores is that values close to 1 or -1 indicate 
strong correlation, but values close to 0 indicate little correlation.</p>

<p>You might note is that Virgin Airlines (VX) flight delay has poor correlation with 
most other airlines (if you processed the same 11 year data set I did). This is 
primarily due to VX data only being present in the data set for the more recent years. 
The strongest correlation found is between American Airlines (AA) and United Airlines 
(UA), and the second strongest correlation is between American Airlines and Envoy Air 
(MQ). It is important to keep in mind that <a href="https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">correlation 
is does not imply causation.</a> That doesn't mean there isn't a structural relationship 
between the onetime performance of two airlines, just that correlation isn't sufficient 
evidence say the two airlines' on-time performance records have a link. In a future post,
we will search for clues as to what that structural relationship might be (if it does 
exist).</p>

<p>The Jupyter notebook behind this post can be viewed 
<a href="https://github.com/DIYBigData/spark-data-analysis-projects/blob/master/airline-data/average-airline-delay.ipynb">at 
my Github repository</a>.</p>


	<footer>
<span><a href="http://diybigdata.net/category/general/data-analysis-general/">Data 
Analysis</a></span><span><a href="http://diybigdata.net/tag/airline-data/">Airline 
Data</a>, <a href="http://diybigdata.net/tag/spark/" rel="tag">Spark</a></span>	
        </footer><!-- .entry-footer -->

  
</article><!-- #post-## -->

      
	<nav>
<h4>Post navigation</h4>

<a href="http://diybigdata.net/2016/08/airline-flight-data-analysis-data-preparation/">Airline Flight Data Analysis - Part 1 - Data Preparation</a><br>
<a href="http://diybigdata.net/2016/10/using-custom-hive-udfs-with-pyspark/">Using 
Custom Hive UDFs With PySpark</a></div></div>
	</nav>

</body></html>
