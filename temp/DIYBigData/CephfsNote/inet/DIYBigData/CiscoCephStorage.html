<!DOCTYPE html>
<html><!--<![endif]-->
<head><title>Cisco UCS C3160 high Density Rack Server with Red Hat Ceph Storage&nbsp; - 
             Cisco</title>
    
</head>

<body style="" id="wcq" class="fw-res cdc-support cdc-eot cdc-high-density">

<h3 id="fw-pagetitle2" class="" data-owner="ID">Cisco UCS C3160 high Density Rack 
Server with Red Hat Ceph Storage&nbsp; 
<a href="http://www.cisco.com/c/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.html">(Source Origin)</a></h3>
		
<div class="updatedDate"><span>Updated:</span>May 31, 2016 &nbsp;&nbsp;Document 
ID: 1464745958294515</a></div>


 <h4><a name="_Toc422695052">Executive Summary</a></h4>  

<p>Object storage resolves the challenges of storing 
massive amounts of data: in particular, unstructured data. Object 
storage provides the infrastructure to store files along with their 
metadata, together called objects. Object storage can be accessed 
through applications that use Representational State Transfer (REST) 
APIs. The widespread use of big data has led to massive growth in 
storage requirements, scalability challenges for traditional file- and 
block-based systems, and the need for simpler and easier maintenance. 
These factors together have led to a rapid increase in object storage 
hardware and software solutions.</p>  

<p>Object storage allows you to store volumes of 
unstructured data. Online web services, cloud backup, file sharing, 
cloud archives: all are among the many use cases for object storage. 
Although there are many commercial solutions on the market, Ceph 
open-source software is gaining predominance. Ceph not only offers 
object storage, but also block and file system open-source and free 
software solutions.</p>  

<p>The Cisco UCS C3160 and C240 M4 Rack 
Servers hardware are well suited for object storage solutions like Ceph.
 The Cisco UCS C3160 is a modular server with high storage density and 
is directed particularly at the use cases mentioned in this document. It
 combines industry-leading performance and scalability and is well 
suited for OpenStack, Ceph-based storage, and other software-defined 
distributed storage environments.</p>  

<p>This document provides an overview of the use of a 
Cisco UCS C3160 server with Ceph in a scaling multinode setup with 
petabytes (PB) of storage. It demonstrates the suitability of the Cisco 
UCS C3160 in object and block&nbsp;storage environments, and the 
server's dense storage capabilities, performance, and scalability as you
 add&nbsp;more&nbsp;nodes.</p>  

<h4><a name="_Toc422695053">Ceph Object Storage on Cisco UCS C3160 High-Density Rack 
Server</a></h4>  

<p>The Cisco UCS C3160 Rack Server (Figure 1) is a 
modular, high-density rack server well suited for service providers, 
enterprises, and industry-specific environments seeking to deploy 
software-defined storage solutions. The Cisco UCS C3160 addresses the 
need for highly scalable computing with high-capacity local storage at 
lower&nbsp;cost-per-gigabyte prices that is essential for an object 
storage deployment. Designed for a new class of 
cloud-scale&nbsp;applications, it is simple to deploy and excellent for 
unstructured data repositories, media streaming, and 
content&nbsp;distribution.</p>  

<p>The server uses a modular server architecture that 
takes advantage of Cisco's blade technology expertise, allowing you to 
upgrade the computing and network nodes in the system without requiring 
data migration from one system to another. Its modular architecture 
reduces the total cost of ownership (TCO) by allowing you to upgrade 
individual components over time and as use cases evolve, without having 
to replace the entire system.</p>  


<p>The server has a standard 4-rack-unit (4RU) form 
factor, with a depth of less than 32 inches, and fits into standard 
racks. It delivers:</p>  

<UL>
  <li> Up to 360 terabytes (TB) of local storage with 60 large-form-factor (LFF) drives, 
       an option to mix up to 14 Solid State Drives, plus two solid-state-disk (SSD) 
       boot drives</li>  

  <li> Two Intel Xeon processor E5-2660, E5-2660, and E5-2695 v2 CPUs up to 512 GB of 
       memory</li>  

  <li> 12-Gbps RAID throughput with multiple RAID options including JBOD and 
       Serial-Attached Small Computer System Interface (SCSI; SAS) host bus 
       adapters (HBAs)</li>  

  <li> Up to a 4-GB RAID cache</li>  

  <li> Support for 12-Gbps SAS drives</li>  

  <li> Modular LAN-on-motherboard (mLOM) slot on the system I/O controller for 
       installing a next-generation Cisco virtual interface card (VIC) or third-party 
       network interface card (NIC)</li>  

  <li> High-reliability, high-availability, and serviceability features with tool-less 
       server nodes, system I/O controller, easy-to-use latching lid, and hot-swappable 
       and hot-pluggable components</li>  
</UL>

<h4><a name="_Toc422695054">Cisco UCS C3160 Server Specifications</a></h4>  

<p>Here is a summary of the Cisco UCS C3160 specifications:</p>  

<UL>
  <li> Up to two 120GB or 480GB SATA SSDs for boot and caching, four power supply units 
       (PSUs), and one rail&nbsp;kit</li>  

  <li> Two Intel Xeon processor E5 series sockets, with 12 cores per socket</li>  

  <li> Up to 4-GB RAID cache</li>  

  <li> Up to sixty 4-TB 7200-rpm LFF drives or 6-TB 7200-rpm SAS3 drives with 4096-byte 
       (4K) sectors</li>  

  <li> Up to 512 GB of RAM</li>  

  <li> Up to fourteen 400GB SAS SSD drives for a Caching</li>  

  <li> System I/O controller with Cisco mLOM: Two Cisco UCS VIC 1227 adapters with 10
       Gigabit Ethernet, with a total of four ports at 40 Gbps each.</li>  
</UL>

<h4><a name="_Toc422695055">Cisco UCS C3160 Benefits</a></h4>  

<p>The Cisco UCS C3160 offers these main benefits:</p>  

<UL>
  <li> Fully modular chassis</li>  
  <li> Innovative airflow design for a compact 4RU, 31.8-inch-deep, industry-standard 
       rack server</li>  
  <li> Ease of upgradeability</li>  
</UL>

<p>&nbsp;</p> 
 <div class=" pDefault"> 
  <b>Figure 1.&nbsp;&nbsp;&nbsp;&nbsp;</b> Cisco 3160 Modular Architecture 
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_0.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 1" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_0.jpg" 
height="385" width="776"></a></p>  

<h4><a name="_Toc422695056">Ceph Features</a></h4>  

<p>Ceph offers the following features:</p>  

<UL>
  <li>Exabyte scalability</li>  
  <UL>
    <li> <b>Scale-out architecture:</b> Expand a cluster from one node to thousands.</p> 

    <li> <b>Automatic rebalancing:</b> Use a peer-to-peer architecture to add capacity 
         at any time with little operational effort. You no longer need major equipment 
         upgrades or data migration projects.</p>  

    <li> <b>Hot or phased software upgrades:</b> Upgrade clusters in phases with little 
         or no downtime.</li>  
  </UL>

  <li>API</li>  

  <UL>
    <li> <b>Amazon Web Services (AWS) S3 and OpenStack Swift API:</b> Support object 
         storage.</p>  

    <li> <b>REST API: </b>Manage all cluster and object storage functions.</li>  
  </UL>

  <li>Storage-aware web-scale applications (object only)</li>  

  <UL>
    <li> <b>Development libraries for direct application integration:</b> Access 
         advanced functions such as triggers, message passing, and in-place data 
         transformations.</p>  

    <li> <b>High-performance native protocol:</b> Eliminate the overhead of the REST 
         interface for performance-sensitive software that needs low-latency and 
         high-throughput I/O.</li>  
  </UL>

  <li>Security</p>  

  <UL>
    <li> <b>Access control lists:</b> Exert detailed control over object storage user- 
         and bucket-level permissions.</p>  

    <li> <b>Quotas:</b> Prevent abuse with pool and object user storage limits.</li>  
  </UL>

  <li>Reliability and availability</p>  

  <UL>
    <li> <b>Dynamic block resizing:</b> Expand or shrink Ceph block devices with little 
         or no downtime.</p>  

    <li> <b>Striping, erasure coding, and replication across nodes:</b> Achieve data 
         durability, high availability, and high performance.</p>  

    <li> <b>Storage policies:</b> Configure placement to reflect service-level 
         agreements (SLAs), performance requirements, and failure domains.</p>  

    <li> <b>Data placement:</b> Use the Controlled Replication Under Scalable Hashing 
         (CRUSH) algorithm to allow every client to calculate where data is located 
         without needing lookup tables, increasing performance.</p>  

    <li> <b>Automatic failover:</b> Prevent server and disk failures from affecting data 
         integrity, availability, or&nbsp;performance.</li>  
  </UL>

  <li>Performance</li>  

  <UL>
    <li> <b>Copy-on-write function:</b> Provision virtual machine images quickly (block 
         only).</li>  

    <li> <b>In-memory client-side caching:</b> Cache both the kernel and the hypervisor 
         (block only).</li>  

    <li> <b>Improved parallelism for data I/O:</b> Use a client-cluster model instead of 
         a client-server model.</li>  

    <li> <b>Cache tiering:</b> Promote hot data to SSDs with expiration policies.</li>  

    <li> <b>Flash journals:</b> Enhance the write performance of data.</li>  

    <li> <b>Customizable stripe sizes:</b> Configure optimal system performance, whether 
         storing multiple-gigabyte video files or small pictures.</li>  
  </UL>

  <li>Support and disaster recovery for multiple data centers</li>  

  <UL>
    <li> <b>Zones and region support:</b> Deploy topologies similar to AWS S3, and 
         others, with a global name space (object only).</li>  

    <li> <b>Read affinity:</b> Serve local copies of data to local users (object 
         only).</li>  

    <li> <b>Data center synchronization:</b> Back up full or partial sets of data 
         between regions (object only).</li>  

    <li> <b>Export of snapshots to geographically dispersed data centers:</b> Implement 
         disaster recovery (block&nbsp;only).</li>  

    <li> <b>Export of incremental snapshots:</b> Reduce network bandwidth (block 
         only).</li>  
  </UL>

  <li>Cost effectiveness</p>  

  <UL>
    <li> <b>Thin provisioning:</b> Allow overprovisioning (block only).</li>  

    <li> <b>Standard x86 hardware:</b> Tailor the price-to-performance mix to the 
         workload.</li>  

    <li> <b>Heterogeneous hardware:</b> Avoid having to replace older nodes as newer 
         ones are added.</li>  

    <li> <b>Erasure coding:</b> Gain the benefits of a cost-effective data durability 
         option.</li>  
  </UL>

  <li>Web-based management</li>  

  <UL>
    <li> <b>Ceph management platform:</b> Get a dashboard for cluster operations.</li>  

    <li> <b>Per-disk and per-pool performance statistics:</b> Identify bottlenecks 
         quickly and easily.</li>  

    <li> <b>Diagnostics workbench:</b> Accelerate troubleshooting.</li>  
  </UL>
</UL>

<h4><a name="_Toc422695057">Ceph Architecture</a></h4>  

<p>Ceph uniquely delivers object, block, and file storage in one unified 
system. Red Hat Ceph Storage provides full enterprise support for object and block 
storage. Ceph Filesystem (FS) is continuing to be actively refined in the Ceph 
community. Ceph delivers extraordinary 
scalability: with thousands of clients accessing petabytes 
to&nbsp;exabytes of data. A Ceph node uses x86 server hardware and 
intelligent daemons, and a Ceph storage cluster&nbsp;accommodates large 
numbers of nodes, which communicate with each other to replicate and 
redistribute&nbsp;data&nbsp;dynamically.</p>  

<p>Scalability concerns arise in traditional 
client-server file systems as a result of inherent centralization. Ceph 
decouples data and metadata operations by eliminating file allocation 
tables and replaces them with its own CRUSH algorithm. Ceph uses object 
storage devices not just for data access, but for serialization, 
replication, and&nbsp;failure detection. All these features make Ceph a 
compelling choice over traditional storage.</p>  

<p>Figure 2 provides an overview of the Ceph architecture.</p> 
 <div class=" pDefault"> 
  <b>Figure 2.&nbsp; &nbsp;&nbsp;&nbsp; </b> Ceph Architecture
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_1.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 27" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_1.jpg" 
height="507" width="776"></a></p>  

<h4><a name="_Toc422695058">Ceph Components</a></h4>  

<p>Ceph includes the following components:</p>  

<UL>
  <li><b>Data storage:</b> The Ceph storage cluster receives data from Ceph clients. 
      This data may come through a Ceph block device, Ceph object storage, Ceph FS, 
      or a custom implementation you create using <b>librados</b>. The cluster then
      stores the data as objects. Each object corresponds to a file in a file system, 
      which is stored on an object storage device (OSD). Ceph OSD daemons handle the 
      read-write operations on the storage disks.</li>  

  <li><b>Pools:</b> The Ceph storage cluster stores data objects in logical partitions 
      called pools. You can create pools for particular data repositories, such as for 
      block devices and object gateways, or simply to separate user groups. From the 
      perspective of a Ceph client, the storage cluster is very simple. When a Ceph 
      client reads or writes data (called an I/O context), it always connects to a 
      storage pool in the Ceph storage cluster.</p>  

      <p>In a replicated storage pool, Ceph defaults to making three copies of an object 
      with a minimum of two clean copies for write operations. If drives containing two 
      of the three copies fail, data will be preserved, but write operations will be 
      interrupted.</p>  

      <p>In an erasure-coded storage pool, objects are divided into chunks using the 
      n = k + m equation:</p></li>  

  <UL>
    <li> <b>k:</b> This is the number of data chunks that will be created.</li>  

    <li> <b>m:</b> This is the number of coding chunks that will be created to provide 
         data protection.</li>  

    <li> <b>n:</b> This is the total number of chunks created after the erasure-coding 
         process.</li>  
  </UL>

  <li><b>Monitors:</b> Before Ceph clients can read or write data, they must contact a 
      Ceph monitor to obtain the most recent copy of the cluster map. A Ceph storage 
      cluster can operate with a single monitor; however, this approach introduces a 
      single point of failure (if the monitor fails, Ceph clients cannot read or write 
      data). For added reliability and fault tolerance, Ceph supports a cluster of 
      monitors.</li>  

  <li><b>Ceph RADOS gateway:</b> The Ceph RADOS gateway (RGW) provides the REST API, 
      compatible with S3 and Swift.</li>  

  <li><b>Ceph RADOS block device:</b> The Ceph RADOS block device (RBD) provides block 
      storage for virtual machines and bare-metal kernels and supports features such as 
      snapshots.</li>  

  <li><b>CRUSH algorithm:</b> The clients and the OSDs use the CRUSH algorithm to get 
      information about data. CRUSH maps are specified per pool. This algorithm helps 
      ensure that replicas do not end up on the same host, disk, etc.</li>  

  <li><b>Placement groups:</b>  Placement groups maintain a static mapping between 
      objects and physical disks (Figure 3). Ceph places objects into placement groups, 
      and the placement groups are further mapped to OSDs. An increase in the number 
      of placement groups will reduce the variance per OSD. However, this may increase 
      CPU and memory on monitors and OSD servers. Objects map as follows:</li>  

  <UL>
    <li> Objects map to one placement group.</li>  

    <li> Each object maps to exactly one placement group.</li>  

    <li> One placement group maps to a list of OSDs (primary OSDs and replicas).</li>  

    <li> Many placement groups can map to one OSD.</li>  
  </UL>
</UL>

<p>&nbsp;</p> 
 <div class=" pDefault"> 
  <b>Figure 3.&nbsp; &nbsp;&nbsp;&nbsp; </b> Ceph Placement Groups and Mapping
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_2.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 10" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_2.jpg" height="475" width="776"></a></p>  

<h4><a name="_Toc422695059">Design Criteria for Running Ceph</a></h4 

<p>This document provides some recommendations for 
running Ceph on the Cisco UCS C3160 Rack Server. However, to understand 
the overall hardware and software requirements for running Ceph and 
designing clusters, you should also address the following high-level 
questions:</p>  

<UL>
  <li>What are the throughput and bandwidth requirements for the storage?</li>  

  <li>What reliability level are you seeking? Will you be using replication or erasure 
      coding? Reliability level helps you to calculate the raw disk capacity required 
      from usable storage capacity.</li>  

  <li>Should the cluster be optimized for performance or capacity? Is this a dense 
      storage requirement?</li>  

  <li>What percentage of the cluster's total data should reside on a single node?</li>  

  <li>Does the failure domain consist of racks and do you need to distribute the OSDs 
      across physical racks?</li>  

  <li>Is the application latency sensitive, and does your budget allow you to use SSDs 
      for journals for better&nbsp;throughput?</li>  
</UL>

<p>&nbsp;</p>  

<p>The answers to these questions can help you establish a
 platform for which you can plan the hardware and software 
configurations. The tests described in this document were conducted on 
1-node, 3-node, and 8-node clusters of Cisco UCS C3160 servers, and 
performance data was collected over a 32RU rack space for the storage 
nodes. These tests provide some useful guidance for designing Ceph 
clusters on Cisco hardware.</p> 
 <span style="font-size:12.0pt;font-family:&quot;Times&quot;,&quot;serif&quot;"><br style="page-break-before:always" clear="all"> </span>  


<h4>Test-Bed Architecture and Bill of Materials</h4>  

<p>Figure 4 shows the test-bed architecture, and Tables 1 and 2 list the hardware and software used in the tests.</p> 
 <div class=" pDefault"> 
  <b>Figure 4.&nbsp; &nbsp;&nbsp;&nbsp; </b>   Test-Bed Architecture
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_3.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 9" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_3.jpg" height="575" width="777"></a></p>  

<p>&nbsp;</p>  

<p class="pTableCaptionCMT"><b>Table 1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </b>Hardware Components</p> 
 <div style="overflow-x:auto !important"> 
  <table cellpadding="6" cellspacing="0" border="1" bordercolor="#ADADAD" width="100%"> 
   <tbody> 
    <tr align="left" valign="top"> 
     <td> <p>Component</p> </td> 
     <td> <p>Model</p> </td> 
     <td> <p>Number</p> </td> 
     <td> <p>Description</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>Storage nodes</p> </td> 
     <td> <p>Cisco UCS C3160 Rack Server</p> </td> 
     <td> <p>8</p> </td> 
     <td> <p>2
 Intel Xeon processor E5-2695 v2 CPUs, 256 GB of memory, 12 cores per 
socket, two 100GB SSD boot drives, 2 Cisco UCS VIC 1227 adapters with a 
total of 40 Gbps of network capacity each, 56 x 6 TB, 12-Gbps SAS3 
7200-rpm LFF Seagate drives, and Cisco MegaRAID 12-Gbps 
SAS&nbsp;controller</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>Chassis</p> </td> 
     <td> <p>Cisco UCS 5108 Blade Server Chassis</p> </td> 
     <td> <p>3</p> </td> 
     <td> <p>&nbsp;</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>I/O modules (IOMs)</p> </td> 
     <td> <p>Cisco UCS 2208 Fabric Extender</p> </td> 
     <td> <p>6</p> </td> 
     <td> <p>&nbsp;</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>Fabric interconnects</p> </td> 
     <td> <p>Cisco UCS 6296UP 96-Port Fabric&nbsp;Interconnect with expansion&nbsp;modules</p> </td> 
     <td> <p>2</p> </td> 
     <td> <p>&nbsp;</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>Switches</p> </td> 
     <td> <p>Cisco Nexus 9396PX Switch</p> </td> 
     <td> <p>2</p> </td> 
     <td> <p>&nbsp;</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>Additional clients</p> </td> 
     <td> <p>Cisco UCS C220 M4 Rack Server</p> </td> 
     <td> <p>16</p> </td> 
     <td> <p>Not shown in Figure 5</p> </td> 
    </tr> 
   </tbody> 
  </table> 
 </div>  

<p>&nbsp;</p>  

<p class="pTableCaptionCMT"><b>Table 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </b>Software Components</p> 
 <div style="overflow-x:auto !important"> 
  <table cellpadding="6" cellspacing="0" border="1" bordercolor="#ADADAD" width="100%"> 
   <tbody> 
    <tr align="left" valign="top"> 
     <td> <p>Operating system</p> </td> 
     <td> <p>Red Hat Enterprise Linux Release 7.1</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>Ceph</p> </td> 
     <td> <p>Red Hat Ceph Storage Release 1.2.3 (Ceph 0.80 Firefly)</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>Tools</p> </td> 
     <td> <p>Ceph Benchmarking Tool (CBT) and FIO</p> </td> 
    </tr> 
   </tbody> 
  </table> 
 </div>  

<p>&nbsp;</p>  

<h4><a name="_Toc422695061">Hardware Summary and Recommendations</a></h4>  

<p>This section summarizes the hardware used in the test and provides best-practice recommendations.</p>  

<h4><a name="_Toc422695062">Cisco Nexus 9396PX Switch</a></h4>  

<p>The Cisco Nexus 9396PX (Figure 5) is a 2RU switch that
 delivers comprehensive line-rate Layer 2 and 3 features. It supports 
line-rate 1/10/40 Gigabit Ethernet with 960 Gbps of switching capacity. 
It is excellent for top-of-rack (ToR) and middle-of-row (MoR) 
deployments in both traditional and Cisco Application Centric 
Infrastructure (Cisco ACI) enterprise, service provider, and cloud environments.</p>  

<p>Here is an overview of the switch's specifications:</p>  

<UL>
  <li>Forty-eight 1/10 Gigabit Ethernet Enhanced Small Form-Factor Pluggable (SFP+) 
      nonblocking ports</li>  

  <li>Twelve 40 Gigabit Ethernet Quad SFP+ (QSFP+) nonblocking ports</li>  

  <li>Low latency (approximately 2 microseconds)</li>  

  <li>50 MB of shared buffer space</li>  

  <li>Line-rate Virtual Extensible (VXLAN) bridging, routing, and gateway support</li>  

  <li>Fibre Channel over Ethernet (FCoE) capability</li>  

  <li>Front-to-back or back-to-front airflow</li>  
</UL>

<p>&nbsp;</p> 
 <div class=" pDefault"> 
  <b>Figure 5.&nbsp; &nbsp;&nbsp;&nbsp; </b>   Cisco Nexus 9396PX Switch
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_4.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 2" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_4.jpg" height="159" width="484"></a></p>  

<p>&nbsp;</p>  

<h5>Best Practice</h5>  

<p>The test bed used Cisco Nexus switches for connecting 
Cisco UCS C3160 servers to the fabric interconnect. It is recommended to
 use quality-of-service (QoS) policies on VLAN configurations on the 
switches to increase bandwidth utilization. The test used a standard 
configuration of the switch; no specific changes were made. Although the
 test bed used the Cisco Nexus 9396PX, a Cisco Nexus switch with fewer 
ports can also be used. Please refer to the <a href="#QOS">QOS</a> Setup in this document for information about how to setup QoS on Cisco Nexus 9396PX switch with Ceph.</p>  

<h4><a name="_Toc422695063">Client Infrastructure: Cisco UCS Blades, IOMs, and Fabric Interconnects</a></h4>  

<h5>Cisco UCS Manager</h5>  

<p>Cisco UCS Manager provides unified, embedded 
management of all software and hardware components of the Cisco Unified 
Computing System (Cisco UCS) through an intuitive GUI, a 
command-line interface (CLI), or an XML API. Cisco UCS Manager provides a
 unified management domain with centralized management capabilities and 
controls multiple chassis and thousands of virtual machines.</p>  

<h5>Cisco UCS 6296UP 96-Port Fabric Interconnect</h5>  

<p>Fabric interconnects provide a single point for 
connectivity and management for the entire system. Typically deployed as
 an active-active pair, the system's fabric interconnects integrate all 
components into a single, highly available management domain controlled 
by Cisco UCS Manager. The fabric interconnects manage all I/O 
efficiently and securely at a single point, resulting in deterministic 
I/O latency regardless of the server's or virtual machine's topological 
location in the system.</p>  

<p>Cisco UCS 6200 Series Fabric Interconnects support the
 system's 10-Gbps unified fabric. They provide low-latency, lossless, 
cut-through switching that supports IP, storage, and management traffic 
using a single set of cables. The fabric interconnects have virtual 
interfaces that terminate both physical and virtual connections 
equivalently, establishing a virtualization-aware environment in which 
blade and rack servers and virtual machines are interconnected using the
 same mechanisms.</p>  

<p>The Cisco UCS 6296UP (Figure 6) is a 2RU fabric 
interconnect with up to 96 universal ports that can support 10 Gigabit 
Ethernet, FCoE, and native Fibre Channel connectivity. The Cisco UCS 
6296UP has high port density, with up to 96 universal ports in a 2RU 
configuration, including three expansion modules with 16 unified ports 
each. The fabric interconnect supports reduced port-to-port latency of 
from 3.2 to 2 microseconds.</p> 
 <div class=" pDefault"> 
  <b>Figure 6.&nbsp; &nbsp;&nbsp;&nbsp; </b>   Cisco UCS 6296UP Fabric Interconnect
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_5.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_5.gif" border="0" height="125" width="434"></a></p>  

<h5>Cisco UCS 2208XP Fabric Extender</h5>  

<p>The Cisco UCS 2208XP Fabric Extender (Figure 7) has 
eight 10 Gigabit Ethernet, FCoE-capable, SFP+ ports that connect the 
blade chassis to the fabric interconnect. Each Cisco UCS 2208XP has 
thirty-two 10 Gigabit Ethernet ports connected through the midplane to 
each half-width slot in the chassis. Typically configured in pairs for 
redundancy, two fabric extenders provide up to 160 Gbps of I/O to the 
chassis.</p> 
 <div class=" pDefault"> 
  <b>Figure 7.&nbsp; &nbsp;&nbsp;&nbsp; </b>   Cisco UCS 2208XP Fabric Extender
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_6.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 313" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_6.jpg" border="0" height="122" width="434"></a></p>  

<p>&nbsp;</p>  

<h5>Cisco UCS 5100 Series Blade Server Chassis</h5>  

<p>The Cisco UCS 5100 Series Blade Server Chassis is a 
crucial building block of Cisco UCS, delivering a scalable and flexible 
blade server chassis.</p>  

<p>The Cisco UCS 5108 Blade Server Chassis (Figure 8) is 
6RU high and can mount in an industry-standard 19-inch rack. A single 
chassis can house up to eight half-width Cisco UCS B-Series Blade 
Servers and can accommodate both half-width and full-width blade form 
factors. Four single-phase, hot-swappable power supplies are accessible 
from the front of the chassis. These power supplies are 92 percent 
efficient and can be configured to support nonredundant, N+ 1 redundant,
 and grid-redundant configurations. The rear of the chassis contains 
eight hot-swappable fans, four power connectors (one per power supply), 
and two I/O bays for Cisco UCS 2208XP Fabric Extenders.</p>  

<p>A passive midplane supports up to two 40 Gigabit 
Ethernet links to each half-width blade slot, or up to four 40 Gigabit 
Ethernet links to each full-width slot. It provides eight blades with 
1.2 terabits (Tb) of available Ethernet throughput for future I/O 
requirements.</p>  

<p>The chassis is capable of supporting future 80 Gigabit Ethernet standards.</p> 
 <div class=" pDefault"> 
  <b>Figure 8.&nbsp; &nbsp;&nbsp;&nbsp; </b>   Cisco UCS 5108 Blade Server Chassis 
     (Front and Back Views)
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_7.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 457" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_7.gif" border="0" height="110" width="211"></a>&nbsp;&nbsp; <a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_8.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 312" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_8.jpg" border="0" height="155" width="276"></a></p>  

<h5>Best Practice</h5>  

<p>Cisco UCS blades were used as client load generators.</p>  

<h4><a name="_Toc422695064">Cisco UCS C3160 Rack Server</a></h4>  

<p>The Cisco UCS C3160 is a dense, modular 4RU server with 360 TB of raw capacity (Figure 10).</p> 
 <div class=" pDefault"> 
  <b>Figure 9.&nbsp; &nbsp;&nbsp;&nbsp; </b>  Cisco UCS C3160 Rack Server
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_9.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 15" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_9.gif" border="0" height="158" width="202"></a></p>  

<p>&nbsp;</p>  

<p>The system consists of a base chassis, a server node 
with two sockets, memory and a system I/O controller, and an optional 
drive expansion node. For more information, refer to the <a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c3160-spec-sheet.pdf">Cisco UCS C3160 specification sheet</a>.</p> 
 <span style="font-size:12.0pt;font-family:&quot;Times&quot;,&quot;serif&quot;"><br style="page-break-before:always" clear="all"> </span>  


<h5>CPU and Memory</h5>  

<p>The server has two CPU sockets, 16 DIMM slots, and a RAID controller (Figure 10).</p> 
 <div class=" pDefault"> 
  <b>Figure 10.&nbsp; &nbsp; </b>   CPU and Memory Slots
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_10.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 25" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_10.jpg" border="0" height="266" width="606"></a></p>  

<p>Each socket has four DIMM channels, and each DIMM 
channel has two slots. When populating memory, follow the instructions 
in the Cisco UCS <a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c3160-spec-sheet.pdf">C3160 specification sheet</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </p>  

<h5>Best Practice</h5>  

<p>DIMMS in the same slot should have the same capacity. 
For optimal performance, all the DIMMS in the same slot should be 
identical. The tests were conducted with sixteen 16-Gbps DIMMs and a 
total of 256 GB of memory per server.</p>  

<p>Ceph OSDs run the RADOS service, calculate data 
placement with CRUSH, and replicate data, and each maintains its own 
copy of the cluster map. Hence, a reasonable amount of CPU is needed for
 OSD servers. In contrast, monitoring servers simply maintain a master 
copy of the cluster map. The monitoring servers are not CPU intensive. 
However, if the nodes run other services or have virtual machines, 
additional CPU requirements may be needed.</p>  

<p>The tests were conducted with Intel Xeon processor 
E5-2695 v2 CPUs, with 12 cores per socket on OSD nodes. Additional tests
 were performed on a similar test bed with Intel Xeon processor E5-2660 
v2 CPUs, with 10 cores per socket, though on a subset of use cases. For 
most of the workloads, a 10-core E5-2660 performed well enough with 
replicated architectures. Only while running erasure-coded tests did the
 E5-2695 provide better headroom than the E5-2660 processors.</p>  

<p>Monitor servers maintain the map and should respond to
 client requests as soon as possible. The greater the memory, the better
 the response time is. OSDs do not need much RAM. No memory issues were 
observed on OSD nodes. Monitoring of the active and inactive columns 
from /proc/meminfo revealed that Ceph on OSD nodes consumed more memory 
during recovery operations than during normal operations.</p>  

<h5>System I/O Controller</h5>  

<p>The Cisco 12-Gbps SAS MegaRAID System I/O Controller 
(SIOC) was used in the test bed. The RAID controller comes with 4 GB of 
RAID cache, has a SuperCap backup, has options to configure the disks in
 different RAID configurations, and provides JBOD support. An industry 
standard, and a recommendation for Ceph, is the use of JBOD for disks. 
JBOD mode was used with the SIOC.</p>  

<p>Optionally, an HBA-mode RAID controller is also available.</p>  

<h5>Disks</h5>  

<p>The system has 56 4 SAS-2 512n or 6TB SAS-3 4kn, 
3.5-inch HDDs that can be installed on the top accessible drive bay. The
 system can accommodate 4 more 3.5-inch disks on the rear panel as well.
 Up to 14 400-GB 2.5-inch SSDs can be installed. The tests were 
conducted with both 56 and 28-disk configurations using the 6TB SAS-3 
4kn drives.</p>  

<h5>Best Practice</h5>  

<UL>
  <li>Always install the same type and size of disks.</li>  

  <li>A maximum of 14 SSDs can be installed. However, the tests did not include SSDs. 
      The use of SSDs as journals on another test bed revealed that SSDs reduced 
      latency, but did not translate much to bandwidth improvement. Hence, the 
      recommendation is to use SSDs when an application needs lower latency, 
      considering the cost factor.</li>  

  <li>The tests were run with 56 drives and with a half configuration: that is, with 28 
      drives. The performance data for both 56 and 28 drives is presented in this 
      document. The 56-drive setup is suitable for dense storage requirements, where 
      lots of raw disk capacity is needed. The 28-drive configuration offers a smaller 
      failure domain for Ceph, though with a reduced raw disk capacity. You should 
      select one of these configurations based on your capacity and performance 
      requirements.</li>  
</UL>

<h5>Interfaces</h5>  

<p>The SIOC has an mLOM slot for the Cisco VIC adapter. 
Up to two SIOCs can be accommodated in a base Cisco UCS C3160 chassis. 
For these tests, the Cisco UCS VIC 1227 dual-port 10-Gbps SFP+ adapter 
was used, providing a total of 40 Gbps of bandwidth per server.</p>  

<h5>Best Practice</h5>  

<p>As a best practice create multiple virtual NICs 
(vNICs) across each adapter, bond and distribute them across pair of 
Cisco Nexus 9000 Series Switches for high availability and bandwidth 
sharing with QoS. For more information about this configuration, <a href="#NIC_Bonding">please check the bonding section on vNIC's</a>.</p>  

<h4><a name="_Toc422695065">Software Summary and Recommendations</a></h4>  

<h4><a name="_Toc422695066">Ceph</a></h4>  

<p>Ceph ICE 1.2.3, which is based on the Firefly release,
 was installed. The general recommendations and parameters for Ceph 
configurations vary depending on the reliability, high-availability, 
capacity, and application throughput and bandwidth requirements, etc.</p>  

<h5><b>Reliability and Pools</b></h5>  

<p>Reliability depends on failure domains. A failure can 
occur on the underlying OS, disks, or OSDs; from a power or network 
outage; etc. When planning your system, you may need to choose a 
compromise between cost and availability.</p>  

<p>A replicated pool provides reliability at the OSD, 
host, rack, etc. Although you can configure the number of copies of 
replicas used, it is recommended to have a minimum of two replicas. The 
OSD pool default size is three, which means that it maintains three 
copies. Replicated pools provide greater redundancy, but the usable disk
 capacity is reduced to one-third of the raw disk capacity in this 
configuration.</p>  

<p>Erasure-coded pools are suitable for large volumes of 
data stores. The data is split into k + m, where m is the parity chunk 
and k is the data chunk. Erasure coding provides better utilization of 
space.</p>  

<p>If you have 100 TB of raw space, with k = 6 and m = 2,
 you have 6/8 of 100 TB, or 75 percent of usable space, compared to 33 
percent in replicated mode. However, erasure coding entails performance 
overhead. With rule-set domain set as host, each element has to be 
pulled from the network for read operations, causing performance 
overhead. This overhead increases the CPU, memory, and network 
requirements.</p>  

<h5><b>Placement Groups</b></h5>  

<p>A general recommendation for pools is 50 to 100 
placement groups per OSD. Because every replicated or erasure pool has 
placement groups associated with it, the sum of the placement groups for
 all the pools together should be close to the recommendations presented
 in this section.</p>  

<h4><a name="_Toc422695067">Installing Ceph on Cisco UCS C3160 Servers</a></h4>  

<p>This section describes how to install Ceph on Cisco UCS C3160 servers.</p>  

<h4><a name="_Toc422695068">Preinstallation Steps</a></h4>  

<p>Perform the following steps before starting the installation.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </p>  

<h5>Connect the Switches</h5>  

<p>Connect the two uplink ports on each adapter to the 
pair of Cisco Nexus 9000 Series Switches. Connect each port on the 
adapter to each of these switches. This setup provides high availability
 in the even case of a switch failure (Figure 11).</p> 
 <div class=" pDefault"> 
  <b>Figure 11.&nbsp; &nbsp; </b>   Nexus Switches to Cisco C3160 connectivity
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_11.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 464" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_11.jpg" border="0" height="312" width="567"></a></p>  

<p>&nbsp;</p>  

<h5>Connect the Management IP Interface</h5>  

<p>Connect the keyboard, video, and mouse (KVM) dongle. 
Connect the management port to the Cisco Nexus 9000 Series Switch or any
 other ToR switch and set the management interface. </p>  

<h5>Upgrade to the Latest Firmware and Patches</h5>  

<p>Log in to <a href="http://software.cisco.com/">http://software.cisco.com</a>
 and navigate to Downloads Home &gt; Products &gt; Servers-Unified 
Computing &gt; UCS C-Series Rack-Mount Standalone Server Software &gt; 
UCS C3160 Rack Server Software. Upgrade to the latest and the suggested 
firmware. </p>  

<h5>Create Additional vNIC Interfaces&nbsp;&nbsp; </h5>  

<p>Log in to the Cisco Integrated Management Controller 
(CIMC) user interface and open the Inventory tab under Server and Cisco 
VIC adapters. Add interfaces to each adapter. Two VLANs are recommended:
 one for the Ceph client or public network (VLAN 20), and one for the 
Ceph cluster network (VLAN 30).</p> 
 <div class=" pDefault"> 
  <b>Figure 12.&nbsp; &nbsp; </b>   Adding vNIC's on Adapter 1
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_12.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 466" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_12.jpg" border="0" height="249" width="602"></a></p> 
 <div class=" pDefault"> 
  <b>Figure 13.&nbsp; &nbsp; </b>   Adding vNIC's on Adapter 2
 </div>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_13.jpg" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 467" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_13.jpg" border="0" height="240" width="608"></a></p>  

<p>&nbsp;</p>  

<p>In Figures 12 and 13:</p>  

<UL>
  <li>eth1 on Adapter Card 1 (uplink port 0) and eth0 on Adapter Card 2 (uplink port 1) 
      are configured with a maximum transmission unit (MTU) value of 1500 on VLAN1.</li>  

  <li>Two interfaces with MTU values of 9000 are created on each of the adapter uplink 
      ports.</li>  

  <li>Each adapter and its uplink ports carry both the VLANs (cluster and public Ceph 
      traffic).</li>  

  <li>A class-of-service (CoS) value of 2 is defined for VLAN20, and a CoS value of 3 
      is defined for VLAN30.</p> 

  <li>These interfaces are bonded together to bond0 (VLAN1), bond1 (VLAN20), and bond2 
      (VLAN30) after the OS installation.</li>  

  <li>eth0 on Adapter Card 1 was used only for preboot execution environment (PXE) boot 
      in the setup.</li>  
</UL>

<h5>Best Practices for Cisco UCS C3160 Setup</h5>  

<UL>
  <li>Connect the uplink port across the switches as shown in Figure 11. This setup 
      will provide high availability in the event of a switch failure and will help 
      maintain business continuity.</li>  

  <li>Upgrade to the suggested and latest firmware at <a target="_b" 
      href="http://software.cisco.com/">Software.cisco.com</a>. Install the utilities, 
      such as storcli, which can be handy for routine operations.</li>  

  <li>Create multiple interfaces and distribute them across uplink ports. Let both Ceph 
      public and cluster traffic flow through each port, but define the CoS and QoS on 
      northbound switches.</li>  
</UL>

<h4><a name="_Toc422695069">Install the Operating System</a></h4>  

<p>Install a certified operating system. For information 
about certified operating systems on the Cisco UCS C3160, refer to the 
Cisco <a href="http://www.cisco.com/web/techdoc/ucs/interoperability/matrix/matrix.html">interoperability matrix</a>. The Cisco UCS C3160 comes with SSD boot drives. For more information, refer to the Cisco UCS C3160 <a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c3160-spec-sheet.pdf">specifications sheet</a>.
 Install the OS on these boot drives. Only software mirroring across 
these two boot disks is supported. After installing the OS, make sure 
that all HDD drives on the Cisco UCS C3160 are visible in 
/proc/partitions before proceeding.</p>  

<h5>Upgrade Ethernet NIC Drivers</h5>  

<p>Go to <a href="http://software.cisco.com/download/navigator.html">http://software.cisco.com/download/navigator.html</a> and upgrade to the recommended Ethernet NIC (eNIC) drivers for the operating system installed on the system.</p>  

<h5><a name="NIC_Bonding">Perform NIC Bonding</a></h5>  

<p>Create bond0 for public traffic (Secure Shell [SSH], 
etc.), bond1 for Ceph public and client traffic, and bond2 for the Ceph 
cluster network. Here are the steps for NIC bonding:</p>  

<OL>
  <LI> &nbsp;&nbsp;&nbsp;&nbsp; Use the adaptive load balancing (alb, or bond mode 6) 
       bonding option with all other bonding options at their default settings.</p>  

<PRE>
   BONDING_OPTS="updelay=0 resend_igmp=1 use_carrier=1 miimon=100 downdelay=0 xmit_hash_policy=0 \
   primary_reselect=0 fail_over_mac=0 arp_validate=0 mode=balance-alb \
   arp_interval=0 ad_select=0"
</pre>  

  <LI> Set up QoS on both Cisco Nexus 9000 Series Switches.</p>  

<PRE>
     class-map type qos match-all COS-Ceph-Pub  
     &nbsp;match cos 2  
     class-map type qos match-all COS-Ceph-Cluster  
     &nbsp;match cos 3  
     policy-map type qos Ceph-Network  
     &nbsp;class COS-Ceph-Pub  
     &nbsp;set qos-group 2  
     &nbsp;class COS-Ceph-Cluster  
     &nbsp;set qos-group 3  
     policy-map type queuing Ceph-traffic  
     &nbsp;class type queuing c-out-q3  
     &nbsp;bandwidth percent 94  
     &nbsp;class type queuing c-out-q2  
     &nbsp;bandwidth percent 4  
     &nbsp;class type queuing c-out-q-default  
     &nbsp;bandwidth percent 1  
     &nbsp;class type queuing c-out-q1  
     &nbsp;bandwidth percent 1  
     system qos  
     &nbsp;service-policy type queuing output Ceph-traffic  
     vlan configuration 20,30  
     &nbsp;service-policy type qos input Ceph-Network</p>  
</PRE>

<p>&nbsp;</p></li>  

  <li>  &nbsp;&nbsp;&nbsp;&nbsp; After interface bonding is up, query from the /proc 
        file system.</p>  

<PRE>
     # cat /proc/net/bonding/bond0  
     Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)  
     <b>Bonding Mode: adaptive load balancing</b>
</PRE></li>
</OL>

<h4><a name="_Toc422695070">Install Ceph on the Monitor and OSD Nodes</a></h4>  

<p>Follow the instructions in the Red Hat Ceph Storage installation guide to install Ceph 1.2.3 (0.80 Firefly): <br> <a href="https://access.redhat.com/documentation/en/red-hat-ceph-storage/version-1.2.3/red-hat-ceph-storage-123-installation-guide-for-rhel-x86-64">Red Hat Ceph Storage 1.2.3 Installation Guide for Red Hat Enterprise Linux (x86 64)</a></p>  

<p>The following procedure is suggested:</p>  

<OL>
  <LI> &nbsp;&nbsp;&nbsp;&nbsp; Install the Ceph software and set up the monitors. A 
       minimum of three monitors is strongly recommended.</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Install the Ceph software on the OSD nodes and push 
       ceph.conf from the monitor to the OSD nodes.</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Before adding the OSDs (Chapter 11 on Add OSD's in 
       <a href="https://access.redhat.com/webassets/avalon/d/Red_Hat_Ceph_Storage-1.2.3-Installation_Guide_for_RHEL_x86_64-en-US/Red_Hat_Ceph_Storage-1.2.3-Installation_Guide_for_RHEL_x86_64-en-US.pdf">Ceph ICE 1.2.3 installation guide above</a>), prepare the disks:</li>  

<UL>
  <LI> a.&nbsp;&nbsp; Destroy any existing partitions.</p>  

<PRE>
parted -s /dev/sd[xxx] rm &lt;partition #&gt;
</PRE>

<p>&nbsp;</P></li>  

  <li> b.&nbsp;&nbsp; Nullify or wipe off any blocks (optional; use this step just to 
       help ensure that the disks are clean).</p>  

<PRE>
dd if=/dev/zero of=/dev/sd[xxx]
</PRE>

<p>&nbsp;</p></li>  

  <li> c.&nbsp;&nbsp; Delete (zap) the disks from your ceph-admin or ceph-deploy 
       node.</p>  

<PRE>
/usr/bin/ceph-deploy -v disk zap &lt;c3160 server&gt;:/dev/&lt;sd[xxx]&gt;
</PRE>

<p>&nbsp;</p></li>  

  <li> d.&nbsp;&nbsp; Create the aligned partitions. The example shown here was created 
       for 20-GB journal partitions on the 6-TB Seagate drives used in the test bed. 
       Make changes as needed for your configuration. This step assumes that you are 
       creating the partitions in a loop for all the disks.</p>  

<PRE>
parted --align optimal -s /dev/$i mklabel gpt
parted --align optimal /dev/$i unit MiB mkpart ceph-journal 1 19081
parted --align optimal /dev/$i unit MiB mkpart ceph-data 19082 5704082
</PRE>

<p>&nbsp;</p></li>  

  <li> e.&nbsp;&nbsp; At this point, a global unique identifier (GUID), which can be 
         queried with <b>sgdisk</b>, will be created on the partitions. Ceph has its 
         own GUID, and it should be in place for these partitions.</li>  

  <li> f.&nbsp;&nbsp;&nbsp; Reinitialize the GUID. The /usr/sbin/ceph-disk-udev script 
       has the Ceph GUID values. You can write script for all the OSDs as shown here.</p>

<PRE>
     osd_uuid=$(uuidgen)
     ptype1=45b0969e-9b03-4f30-b4c6-b4b80ceff106
     # GUID taken from ceph-disk-udev.
     sgdisk --change-name="1:ceph journal" --partition-guid="1:${osd_uuid}" --typecode="1:${ptype1}" /dev/${i}
     osd_uuid=$(uuidgen)
     ptype2=4fbd7e29-9d25-41b8-afd0-062c0ceff05d
     sgdisk --change-name="2:ceph data" --partition-guid="2:${osd_uuid}" --typecode="2:${ptype2}" /dev/${i}
</PRE>

<p>&nbsp;</p></li>  
</UL>

<p class="pBullet2IndentCMT">This should complete the preparatory steps on OSD disks. Once done you may proceed to Chapter 11 for adding OSD's.</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Enter the script to run the <b>osd create</b> command 
       for all the OSDs and nodes.</p>  

<PRE>
/usr/bin/ceph-deploy -v osd create `hostname -s`:${i}2:${i}1 (data partition:journal partition)
</PRE>

<p>&nbsp;</p></li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; After installing Ceph, place the journal and data 
       partitions in ceph.conf as shown here.</p>  

<PRE>
     [osd.400]
     host = colusa1-ceph
     devs=/dev/disk/by-id/scsi-35000c500630ba08f-part2
     osd_journal=/dev/disk/by-id/scsi-35000c500630ba08f-part1
</PRE></li>
</OL>

<p>&nbsp;</p>  

<p>This placement will tie the journal and data 
partitions to the SCSI ID, and any Linux device name changes after 
reboots will be transparent.</p>  

<h4><a name="_Toc422695071">Install Ceph on Client Nodes</a></h4>  

<p>Install Ceph software on the client nodes. Make sure 
that the VLAN configured for Ceph client traffic on the OSD nodes is 
available on these nodes as well.</p>  

<h4><a name="_Toc422695072">Install Ceph Benchmarking Tool (CBT) on Client Nodes</a></h4>  

<p>Download the CBT from github.</p>  

<PRE>
git clone https://github.com/ceph/ceph-tools.git
</PRE>

<p>&nbsp;</p>  

<p>The tool uses pdsh packages. Download the pdsh packages from the Fedora library.</p>  

<p>You may need to adjust the number of client nodes used
 depending on the expected bandwidth from the cluster. Also, the 
adaptive load-balancing mode is observed to work well with a larger 
number of clients.</p>  

<p>A sample YAML configuration file load generation is provided in Appendix C.</p> 
 <span style="font-size:12.0pt;font-family:&quot;Times&quot;,&quot;serif&quot;"><br style="page-break-before:always" clear="all"> </span>  


<h4>Performance Testing</h4>  

<p>Performance tests were conducted using CBT. Both RADOS
 and RBD tests were attempted. RADOS tests were performed on bare-metal 
clients, and RBD tests were performed on Kernel-based Virtual Machine 
(KVM) clients (two per blade in the client infrastructure: a few tests 
with kernel RBD, and a few with LIBRBD) presents a subset of the 
performance data. Most of the parameters for the BIOS, OS, and Ceph were
 the default. The purpose of these tests was to validate the reference 
architecture, check the scalability for different configurations and to 
see how a multinode cluster could scale and the how values can be 
extrapolated upward; it was not intended as a benchmarking exercise.</p>  

<p class="pTableCaptionCMT"><b>Table 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </b>Test Cases</p> 
 <div style="overflow-x:auto !important"> 
  <table cellpadding="6" cellspacing="0" border="1" bordercolor="#ADADAD" width="100%"> 
   <tbody> 
    <tr align="left" valign="top"> 
     <td> <p>Block/Object</p> </td> 
     <td> <p>Test #</p> </td> 
     <td> <p>Block Sizes</p> </td> 
     <td> <p>Type of Pools</p> </td> 
     <td> <p>Cluster Configurations</p> </td> 
     <td> <p>No of Disks</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>RADOS</p> <p>tests with CBT</p> </td> 
     <td> <p>Test 1</p> </td> 
     <td> <p>Collect performance metrics for block sizes from 4 KB to 4 MB.</p> </td> 
     <td> <p>Replication and erasure coding</p> </td> 
     <td> <p>1 node, 3 node, and 8 nodes</p> </td> 
     <td> <p>56-disk and 28-disk configurations</p> </td> 
    </tr> 
    <tr align="left" valign="top"> 
     <td> <p>RBD FIO tests with&nbsp;CBT</p> </td> 
     <td> <p>Test 2</p> </td> 
     <td> <p>Collect performance metrics for block sizes from 4 KB to 4 MB.</p> </td> 
     <td> <p>Replication </p> </td> 
     <td> <p>3 node and 8 nodes</p> </td> 
     <td> <p>56-disk and 28-disk configurations</p> </td> 
    </tr> 
   </tbody> 
  </table> 
 </div>  

<h4><a name="_Toc422695074">Iperf Tests</a></h4>  

<p>Before starting the actual tests, iperf tests were run
 between the clients and all the servers. It is a best practice to run 
iperf tests to understand the network limit between the clients and the 
servers. Each server (OSD) node had 40 Gbps of network capacity. Each 
blade had 20 Gbps of network capacity.</p>  

<p>Running iperf tests with 9000 MTU provided a total 
bandwidth of 26,000 MBps. Hence, it was concluded that a maximum of 
26,000 MBps of bandwidth can be expected from the test bed. The results 
that follow support the results of the iperf tests.</p>  

<h4><a name="_Toc422695075">Single-Node Tests</a></h4>  

<p>Single-node tests were performed with the rule-set domain set to OSD. Both replicated and erasure-coding tests were attempted.</p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_14.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 30" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_14.gif" border="0" height="277" width="776"></a></p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_15.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 449" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_15.gif" border="0" height="276" width="776"></a></p>  

<h5>Summary</h5>  

<UL>
  <li>With a replication factor of 3, read operations on a single node peaked at 3700 
      MBps, and write operations peaked at 1100 MBps with erasure coding.</li>  

  <li>Erasure-coding reads appear to peak lower over replicated reads.</li>  

  <li>System performance improved with increase in block size.</li>  

  <li>Write and read operations per OSD at a given block size were observed to be higher 
      with 28 disks.</li>  

  <li>The pattern and performance impact of erasure coding compared to replication is 
      almost the same for both 28- and 56-disk configurations.</li>  
</UL>

<h4><a name="_Toc422695076">Three-Node Tests</a></h4>  

<p>Both RADOS and RBD tests were conducted on a 3-node configuration</p>  

<p class="MsoCaption">RADOS Tests</p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_16.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 460" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_16.gif" border="0" height="201" width="606"></a></p>  

<p>&nbsp;</p> 
 <span style="font-size:12.0pt;font-family:&quot;Times&quot;,&quot;serif&quot;"><br style="page-break-before:always" clear="all"> </span>  


<p class="MsoCaption">RBD Tests (RBDFIO)</p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_17.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 454" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_17.gif" border="0" height="201" width="606"></a></p>  

<p>&nbsp;</p>  

<p class="MsoCaption">CPU Utilization-Write Operations While Running RADOS Tests</p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_18.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 463" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_18.gif" border="0" height="201" width="305"></a></p>  

<h5><b>RBDFIO and LIBRBDFIO Tests</b></h5>  

<p>CBT provides both RBDFIO and LIBRBDFIO tools for 
testing the RADOS block devices RBDFIO mounts the RBDs very similar to 
kernel RBD and runs the tests. LIBRBD uses FIO with an I/O engine 
supporting direct access to Ceph RADOS block devices through LIBRBD 
without the need to use the kernel RBD driver.</p>  

<p>Setting the RBD cache as true or false alters the 
throughput values, particularly for sequential write operations. The use
 of client-side RBD caching is application dependent too.</p>  

<p>Although all of these combinations were not tested, 
snippets of RBDFIO and LIBRBDFIO are shown here. KVM guests used 
write-through mode for the disk cache.</p>  

<UL>
  <li>LIBRBD with FIO (<b>ioengine=rbd</b>) was used.</li>  

  <li>The RBD cache was set to false.</li>  
</UL>

<p>&nbsp;</p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_19.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 6" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_19.gif" border="0" height="201" width="606"></a></p>  

<p>&nbsp;</p>  

<h5>Summary</h5>  

<UL>
  <li>The behavior of erasure-coding pools compared to replicated pools was similar to 
      that seen on a single node.</li>  

  <li>Write operations reached about 2500 MBps, and read operations, taking full 
      advantage of QoS, reached about 9000 MBps.</li>  

  <li>The scalability from a single node to three nodes observed to be anywhere between 
      2.5 to 3.</li>  

  <li>Both RADOS and RBD tests performed better at higher blocks sizes. RBD read 
      operations performed better between 512 KB and 1M.</li>  

  <li>The peak values of bandwidth and I/O operations per second (IOPS) dropped between 
      50 and 60 percent with caching disabled, compared to their values while using the 
      RBD cache.</li>  

  <li>CPU utilization was greater with erasure coding. The difference in CPU utilization 
      between replication and erasure coding increased at higher block sizes.</li>  
</UL>

<p>&nbsp;</p>  

<p>Below is a snippet of bandwidth comparison per OSD on a
 3-node Cisco UCS C3160 cluster. The 28-disk configuration offers better
 bandwidth per OSD and a smaller failure domain. However, the overall 
MBps rate for a 56-disk configuration is still higher (Per-OSD bandwidth
 x Number of disks).</p>  

<p class="MsoCaption">56- and 28-Disk Configurations</p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_20.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 468" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_20.gif" border="0" height="175" width="606"></a></p>  

<p>&nbsp;</p> 
 <span style="font-size:12.0pt;font-family:&quot;Times&quot;,&quot;serif&quot;"><br style="page-break-before:always" clear="all"> </span>  


<p class="MsoCaption">Eight-Node Tests</p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_21.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 26" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_21.gif" border="0" height="173" width="606"></a></p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_22.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 450" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_22.gif" border="0" height="175" width="606"></a></p>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_23.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 451" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_23.gif" border="0" height="175" width="303"></a></p>  

<h5>Summary</h5>  

<UL>
  <li>Replicated read operations peak at about 20000 MBps in the 8-node 
      configuration.</li>  

  <li>The behavior of erasure-coded pools compared to replicated pools was similar to 
      what was seen on a single node.</li>  

  <li>Both 56- and 28-disk configurations were close in their bandwidth values for read 
      operations; write operations were higher with 56 disks.</li>  
</UL>

<p>&nbsp;</p>  

<h4><a name="_Toc422695078">Analysis of Multinode Ceph Cluster Results</a></h4>  

<p><a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_24.gif" class="show-image-alone" title="Related image, diagram or screenshot."><img id="Picture 452" src="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/whitepaper-C11-735004.doc/_jcr_content/renditions/whitepaper-C11-735004_24.gif" border="0" height="176" width="606"></a></p>  

<UL>
  <li>Joining the three points shows that almost a linear bandwidth improvement occurs 
      as nodes are added.</li>  

  <li>The results from the above show that a 16-node Cisco UCS C3160 should scale to 
      about 36000 MBps for read operations.</li>  

  <li>Write operations benefit from the additional disks in the 56-disk configuration. 
      However, the bandwidth is not twice the number of disks.</li>  

  <li>There isn't much difference between the two disk configurations for read 
      operations.</li>  
</UL>

<h4><a name="_Toc422695079">Conclusions</a></h4>  

<p>The following points can be inferred about running Ceph on the Cisco UCS C3160:</p>  

<UL>
  <li>The Cisco UCS C3160 is a dense server that can offer a total raw disk capacity of 
      360 TB in a 4RU rack space. These characteristics help reduce the overall data 
      center space required at a very competitive price per gigabyte and per rack 
      unit.</li>  

  <li>The Cisco UCS C3160 is offered in both 56- and 28-disk configurations. The 28-disk 
      configuration reduces the overall failure domain of the Ceph cluster with industry 
      leading throughput or bandwidth per OSD. The fewer disks in this configuration 
      enable customers to start small and grow over time with just additional 
      drives.</li>  

  <li>The Cisco UCS C3160 VIC adapter allows you to configure multiple vNICs. This 
      unique feature allows you to configure multiple NICs (the operating system sees 
      them as separate interfaces) on the same physical adapter ports. When used with 
      the Cisco Nexus 9000 Series Switches, this feature allows you to define QoS on 
      both Ceph public and Ceph cluster traffic. There is no cluster traffic during 
      read operations for replicated pools, allowing read operations to take advantage 
      of the complete network capacity of the host.</li>  

  <li>Multiple vNICs bonded together allow you to send traffic to both switches at the
      same time, providing bandwidth sharing and also high availability in the event of 
      switch failure.</li>  

  <li>The existing hardware is compatible for upgrades to the next releases, and you can 
      upgrade with little or no interruption of business.</li> 
 </UL>


<h4>Summary</h4>  

<p>The rapid growth of unstructured data in backup, 
archival, video-streaming, analytics, and cloud environments require a 
dense storage solution as data center space becomes increasingly 
expensive. Organizations need to reduce the cost of both hardware and 
software, but at the same time need to help ensure the reliability and 
availability of the overall solution.</p>  

<p>The Cisco UCS C3160 Rack Server and Ceph together 
offer a viable solution. This solution is well suited for most of the 
workloads, particularly for object workloads at higher block sizes.</p>  

<p>The Cisco UCS C3160 with its 28 and 56-disk 
configurations offers both capacity-optimized and throughput-optimized 
solutions for Ceph.</p>  

<h5>Acknowledgements</h5>  

<p>The author is thankful to Cisco colleagues and Cisco's
 partner Red Hat for their continuous guidance and valuable suggestions 
while executing this project.</p>  

<h5>Cisco Systems</h5>  

<OL>
  <LI> &nbsp;&nbsp;&nbsp;&nbsp; RamaKrishna Nishtala</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Vijay Durairaj</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Raj Govindaiah</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Paula Patel</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Aniket Patankar</li>  
</OL>

<h5>Red Hat</h5>  

<OL>
  <LI> &nbsp;&nbsp;&nbsp;&nbsp; Brent Compton</li>  

  <li> &nbsp;&nbsp;&nbsp;&nbsp; Kyle Bader</li>  
</OL>

<h4><a name="_Toc422695081">Appendix A: Operating System Parameters</a></h4>  

<PRE>
     kernel.pid_max = 4194303</acronym>
     * soft nofile 327680</acronym>
     * hard nofile 327680</acronym>
</PRE>

 <span style="font-size:12.0pt;font-family:&quot;Times&quot;,&quot;serif&quot;"><br style="page-break-before:always" clear="all"> </span>  


<h4>Appendix B: Sample ceph.conf File</h4>  

<PRE> 
   [global]  
   fsid = 94f87f4d-4219-4cbb-8536-ec017b963d76  
   mon_initial_members = ceph-deploy, ceph-deploy1, ceph-deploy2  
   mon_host = 192.168.20.133,192.168.20.134,192.168.20.135  
   auth_cluster_required = cephx  
   auth_service_required = cephx  
   auth_client_required = cephx  
   filestore_xattr_use_omap = true  
   public_network = 192.168.20.0/24  
   cluster_network = 192.168.30.0/24  
   osd_journal_size = 20000  
   osd_pool_default_size = 3  
   osd_pool_default_min_size = 2  
   osd_pool_default_pg_num = 4096  
   rbd_cache = true  
   mon_pg_warn_max_per_osd = 32768  
   mon_pg_warn_max_object_skew = 100000  
   mon_pg_warn_min_per_osd = 0  
   mon_clock_drift_allowed = .15  
   mon_clock_drift_warn_backoff = 30  
   mon_osd_down_out_interval = 300  
   mon_osd_report_timeout = 300  
   &nbsp;  
   [osd]  
   osd_mkfs_type = xfs  
   osd_mkfs_options = -f -i size=2048 -n size=64k  
   osd_mount_options_xfs = inode64,noatime,logbsize=256k  
   filestore_xattr_use_omap = true  
   osd_op_threads = 24  
   osd_max_backfills = 1  
   &nbsp;  
   [osd.0]  
   host = colusa8-ceph  
   devs=/dev/disk/by-id/scsi-35000c500633de9ab-part2  
   osd_journal=/dev/disk/by-id/scsi-35000c500633de9ab-part1  
   ....  
   [osd.447]  
   host = colusa1-ceph  
   devs=/dev/disk/by-id/scsi-35000c500630ba813-part2  
   osd_journal=/dev/disk/by-id/scsi-35000c500630ba813-part1 
</PRE> 

<span style="font-size:12.0pt;font-family:&quot;Times&quot;,&quot;serif&quot;"><br style="page-break-before:always" clear="all"> </span>  


<h4>Appendix C: Sample YAML Configuration File for CBT</h4>  

<PRE> 
   cluster:  
   &nbsp; head: "root@ceph-deploy"  
   &nbsp;  
   &nbsp; clients: 
["root@ceph-deploy","root@ceph-deploy1","root@ceph-deploy2","root@compute4",
 "root@compute5", "root@compute6", "root@compute7", "root@compute8", 
 "root@compute9", "root@compute10", "root@compute11", "root@compute12", 
 "root@compute13", "root@compute14", "root@compute15", "root@compute16", 
 "root@compute17","root@compute18","root@compute19","root@compute20",
 "root@compute21","root@compute22","root@compute23","root@compute24",
 "root@compute25","root@compute26","root@compute27","root@compute28",
 "root@compute29","root@compute30","root@compute31","root@compute32"]  
   # VM's for RBD Tests.  
   &nbsp; osds: 
["root@colusa8-ceph","root@colusa7-ceph","root@colusa6-ceph","root@colusa5-ceph",
 "root@colusa4-ceph","root@colusa3-ceph","root@colusa2-ceph","root@colusa1-ceph"]  
   &nbsp; mons: ["root@ceph-deploy","root@ceph-deploy1","root@ceph-deploy2"]  
   &nbsp; osds_per_node: 56  
   &nbsp; fs: xfs  
   &nbsp; mkfs_opts: -f -i size=2048 -n size=64k  
   &nbsp; mount_opts: -o inode64,noatime,logbsize=256k  
   &nbsp; conf_file: /home/ceph/downloads/ceph-tools/cbt/example/ceph.conf  
   &nbsp; ceph.conf: /home/ceph/downloads/ceph-tools/cbt/example/ceph.conf  
   &nbsp; iterations: 3  
   &nbsp; rebuild_every_test: True  
   &nbsp; tmp_dir: "/tmp/cbt"  
   &nbsp; clusterid: 944689be-e842-407c-a9dc-13cbb76bfce1  
   &nbsp; pool_profiles:  
   &nbsp;&nbsp; erasure:  
   &nbsp;&nbsp;&nbsp;&nbsp; pg_size: 16384  
   &nbsp;&nbsp;&nbsp;&nbsp; pgp_size: 16384  
   &nbsp;&nbsp;&nbsp;&nbsp; replication: 'erasure'  
   &nbsp;&nbsp;&nbsp;&nbsp; erasure_profile: 'colusaec'  
   &nbsp;&nbsp; replicated:  
   &nbsp;&nbsp;&nbsp;&nbsp; pg_size: 16384  
   &nbsp;&nbsp;&nbsp;&nbsp; pgp_size: 16384  
   &nbsp;&nbsp;&nbsp;&nbsp; replication: 'replicated'  
   &nbsp; erasure_profiles:  
   &nbsp;&nbsp; colusaec:  
   &nbsp;&nbsp; erasure_k: 2  
   &nbsp;&nbsp; erasure_m: 1  
   &nbsp;  
   # Adjust PG's. Rados tests create one pool for each client while RBD tests create only one pool.  
   &nbsp;  
   # rbdfio:  
   # rbdadd_mons: "192.168.20.133:6789,192.168.20.134:6789,192.168.20.135:6789"  
   # rbdadd_options: "noshare"  
   # time: 300  
   # pgs: 4096  
   # use_existing: True  
   # client_ra: 0  
   # concurrent_procs: [1, 2, 4, 8, 16]  
   # vol_size: 65536 --This is too much for 32 vm's  
   # op_size: [ 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192,4096 ]  
   # op_size: [ 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192,4096 ]  
   # op_size: [ 4194304, 65536, 4096 ]  
   # iodepth: [1, 2, 4, 8, 16]  
   # numjobs: [1]  
   # mode: [write, read]  
   # mode: [randwrite, randread, write, read]  
   # op_size: [ 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192,4096 ]  
   # op_size: [ 4096 ]  
   # concurrent_procs: [1]  
   # vol_size: 16384  
   # pool_profile: replicated  
   # iodepth: [ 16 ]  
   &nbsp;  
   # librbdfio:  
   # rbdadd_mons: "192.168.20.133:6789,192.168.20.134:6789,192.168.20.135:6789"  
   # rbdadd_options: "noshare"  
   # time: 120  
   # pgs: 4096  
   # use_existing: True  
   # concurrent_procs: [1, 2, 4, 8, 16]  
   # vol_size: 65536 --This is too much for 32 vm's  
   # op_size: [ 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192,4096 ]  
   # op_size: [ 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192,4096 ]  
   # op_size: [ 4194304, 65536, 4096 ]  
   # iodepth: [1, 2, 4, 8, 16]  
   # numjobs: [1]  
   # mode: [randwrite, write]  
   # op_size: [ 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192,4096 ]  
   # op_size: [ 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192 ]  
   # op_size: [ 4194304 ]  
   # concurrent_procs: [1]  
   # vol_size: 16384  
   # pool_profile: replicated  
   # iodepth: [ 16 ]  
   &nbsp;  
   &nbsp; radosbench:  
   &nbsp;&nbsp; op_size: [ 8388608, 4194304, 2097152, 1048576, 524288,262144,131072,65536,32768,16384,8192,4096 ]  
   &nbsp;&nbsp; op_size: [ 4194304, 2097152, 1048576 ]  
   &nbsp;&nbsp; write_only: False  
   &nbsp;&nbsp; time: 600  
   &nbsp;&nbsp; concurrent_ops: [ 128 ]  
   &nbsp;&nbsp; concurrent_procs: 1  
   &nbsp;&nbsp; use_existing: True  
   &nbsp;&nbsp; pool_profile: replicated  
   # pool_profile: erasure
</PRE> 

<p>&nbsp;</p>  

<h5><b>About Red Hat</b></h5>  

<p>Red Hat is the world's leading provider of open source
 software solutions, using a community-powered approach to reliable and 
high-performing cloud, Linux, middleware, storage, and virtualization 
technologies. Red Hat also offers award-winning support, training, and 
consulting services. As a connective hub in a global network of 
enterprises, partners, and open source communities, Red Hat helps create
 relevant, innovative technologies that liberate resources for growth 
and prepare customers for the future of IT.</p>  

<p>&nbsp;</p> 

</body></html>