<!DOCTYPE html>
<html>
  <head><title>Why is Ceph so rare for home use? Even among technically inclined people, the most common setup seems to be ZFS. : DataHoarder</title>
  </head>
  <body class="single-page comments-page">

<h3>Why is Ceph so rare for home use? 
<a href="https://www.reddit.com/r/DataHoarder/comments/4gzpxi/why_is_ceph_so_rare_for_home_use_even_among/" target="_b">(Source Origin)</a></h3> 

<b>Even among technically inclined people, the most common setup seems to be ZFS.</b> 
 

<p>Although
 Ceph really shines with huge installations, it can still be used on a 
single host with any number of drives. It can create any amount of data 
redundancy and you can start with just 3 drives on a single host and add
 as many drives as you want and change the parity to anything you want 
on the fly. You can also mix drive sizes all you want. After a few years
 when you want to upgrade your NAS, most people always run into the 
problem copying all the old files to the new NAS and what to do with the
 old hardware. With Ceph you can just add as many hosts as you want and 
scale as quickly/slowly as you want. </p>

<p>Is there a major downside I'm not seeing for the average datahoarder 
that has 3+ drives on a single host? Seems like you could run everything
 on a single host and run the monitor on a VM or a raspberryPi. </p>

<p>With CephFS you can even mount storage directly to any other 
computer. So for example you could run a Plex machine that mounts block 
storage from Ceph or just run Cephfs and mount a drive. </p>

<p>Is there a disadvantage I am missing?</p>

<p>EDIT:</p>

<p>Things I learned from this thread</p>

<ol>
  <li>Complexity is the major issue. It is and likely will always be more 
complicated than setting up freenas/rockstor type systems. There are 
writeups for setting up multiple nodes, but I haven't seen any writeups 
for single node systems that a home user would want to try. <a href="http://palmerville.github.io/2016/04/30/single-node-ceph-install.html">This write up has a basic setup with virtualbox simulating a single host with 3 OSDs.</a> </li>
  <li>CephFS doesn't have proper FSCK tools for data integrity. This 
however has improved with the newest release (Jewel) although it has 
only been a week. Not production ready. <a href="http://usabuzz.net/page/watch/vid2016GbdHxL0vc9I">Latest info discussed here.</a>. Stick to mounting blocks for now. </li>
  <li>Current setup requires dedicated journal drives. This model is 
changing with the new release of bluestore. Dedicated journal drives are
 likely not needed, however this was first released in Jewel last week 
so not production ready yet. <a href="http://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/">Read more here</a>. </li>
  <li>bluestore doesn't appear to have any protection against bitrot. The 
underlying btrfs used to provide some protection but they seem to be 
moving away from that. Bluestore does not save checksums like zfs or 
btrfs. </li>
  <li>Home users will want to change the default setting from replication 
to erasure codes. ECs will allow you to setup raid5 like redundancy 
instead of raid1 like redundancy. It allows you to be more efficient 
with your raw storage space. </li>
  <li>Dream setup: use openstack to scale across all the CPUs I have for 
VMs. Provision block storage to each VM using Cinder. Connect cinder to a
 Ceph data store for the actual storage. Lots of tutorials on setting 
this all up, however definitely not for beginners of either Ceph or 
Openstack. </li>
</ol>

<h4>all 105 comments</h4>

<p>I have ceph for home use. you have to realize that with just a few osd's ceph is 
actually not that great. the objects that are stored are by default 4MB is size, that 
means any transaction from 512 bytes to 4MB is a full 4MB read, and if you modify it, 
its a 4MB right as well, except its not, if you have replication greater than 1, its 
increased by a factor of that. now you can make this number smaller by setting the 
order size of your RBD, the smallest it goes is order 12, thats 2<sup>12</sup> or 4096. 
at 4k per object its much easier to do normal transactions but now when you spend a lot 
of cycles generating the objects themselves. its a trade off. you either build big or 
go slow. i recommend and order size of 16 or 64K it seems good for RBD use.</p>

<p>With bluestores this should all change.</p>

<p> thanks for the write up, for what it's worth bluestore was released last week with 
Jewel</p>

<p> 
<a href="http://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/" 
target="_b">http://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/</a>
</p>

<p> hopefully it gets some good testing in the coming months. </p>

<p> 
<a href="https://www.reddit.com/user/TheSov">TheSov</a>4.4PB Ceph(work) 9TB Ceph(home) 
</p>

<p> i understand but its still experimental. I wouldn't use that in production yet.</p>

<p> Where did you get the impression that reads less than 4MB will require 4MBs of data 
to be transferred?</p>


<p> On atomic reads that's not a problem but if u don't use atomic reads the object in 
its entirety is read modified in memory and written back.</p>

<p><b>What's bluestores?</b> </p>


<p> very nice write up here:</p>

<p> 
<a href="http://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/" 
target="_b">http://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/</a>
</p>

<p> tl;dr: right now we have</p>

<pre>
  HDD-&gt;XFS/btrfs/ext4-&gt;OSD-&gt;RAD
</pre>

<p> with the new system</p>

<pre>
  HDD-&gt;bluestore-&gt;OSD-&gt;RAD
</pre>

<p> the existing system basically uses hacks to store objects (ceph) on a regular file 
system (xfs\btrfs\ext4). The new system is designed from the ground up to store objects 
directly to the HDD with minimal overhead. Should provide lots of benefit for writing 
(less journaling) and reading.</p>


<p> Effectively turning ceph into it's own filesystem?</p>


<p> almost, you can see BlueFS on their block diagrams. You still need an OS on the 
machine itself, but the HDD is raw and managed by it's own 'OS'</p>

<p>Because ZFS is so incredibly easy to setup.</p>

<p>sudo apt-get install zfsutils-linux &amp;&amp; sudo zpool create data mirror /dev/sda 
/dev/sdb mirror /dev/sdc /dev/sdd</p>

<p>Done, simple, ready to go. Takes care of formatting, and everything, will 
automatically mount to /data, works perfectly. ZFS is also tried and true, very stable, 
and performs great.</p>

<p>How do you install Ceph? Is it as easy as two commands that are super memorable? If 
not, I'm probably not that interested in it. Plus, I've heard the data integrity on 
Ceph is a bit iffy but I'm not sure about that. It just seems like it's not for me. If 
you could address all of my complaints or potential concerns, then and only then would 
I consider running it.</p>


<p> I've spent the last year getting my ceph homelab cluster up. So, no you can't do it 
with 2 commands. yet. we'll get there.</p>

<p> 
edit: I just got 5 more 2TB HDD's in the mail today actually. If you run Ceph at home, 
you will always have a reason to buy "just one more node", and "a few more drives". </p>

<p> sounds nice. I'm sure many would be interested in a write up of your 
experiences/setup if you have the time.</p>

<p> it will be titled: Don't Try This At Home</p>


<p> haha, nice.</p>

<p>I see the main problem for home ceph, and ceph in general as follows.</p>

<p>In the ceph philosophy in my head the ideal setup is 1 Node, 1 HDD, 1 OSD. </p>

<p>Thankfully ceph needs only weak nodes, however, often times the 'lowest unit of 
computer' (in the environments where you might find ceph) is a dual processor 1 to 
4 U behemoth. Which is completely unnecessary, and why ceph is incompatible with 
this world.  </p>

<p>For years I've followed advancements in x86 SBC's and only now are we starting to 
see native SATA and GbE support. x86 is expensive compared to ARM but it makes it so 
much easier to deploy. </p>

<p>Which is why I am so excited that <a 
href="http://ceph.com/community/500-osd-ceph-cluster/" 
target="_b">http://ceph.com/community/500-osd-ceph-cluster/</a> was announced this week. 
Now the HDD is the node, and we can ditch the power hungry Intel CPU's. </p>

<p>Add in a re-introduction of 5.25" HDD's as per Google's request, and we have 
ourselves a homelab ceph cluster in a atx tower.</p>

<blockquote><p>In the ceph philosophy in my head the ideal setup is 1 Node, 1 HDD, 1 
OSD. </p></blockquote>

<p>that seems counter to what I've read. Seems like the ideal setup is </p>

<p>1 node, many HDDs, one OSD per HDD</p>

<p>Scroll to bottom of this: </p>

<p><a href="http://docs.ceph.com/docs/hammer/start/hardware-recommendations/" 
target="_b">http://docs.ceph.com/docs/hammer/start/hardware-recommendations/</a></p>

<p>One quad core CPU can easily handle ~10 HDDs.</p>

<p>The limiting factor is often not the CPU at all, it's the network. If you can spring 
for better network gear to you easily put 40HDDs with one quadcore CPU</p>

<p><a href="http://www.mellanox.com/blog/2016/02/making-ceph-faster-lessons-from-performance-testing/" 
target="_b">making-ceph-faster-lessons-from-performance-testing/</a></p>

<p>Yes, you are correct, that is the reference architecture. Which is why I used the 
preface: "in my head...".</p>

<p>The reason becomes clear when you start to consider High Availability (or HA for 
short) deployments.  HA Ceph deployments are....i forget the word...topology aware? 
rack aware?, you configure the underlying algorithm to make the redundant writes to 
disks that are on different failure 'zones'?.</p>

<p>Zones are hierarchical, and we loath a single point of failure, never more so than 
in a HA Ceph deployment, so the idea of putting multiple OSD's into a single node, or 
a single point of failure is an inherent concern of ceph sysadmins.</p>

<p>There's like a ton of reasons a node can fail, and it only has to fail slightly for 
it to fail completely, these managed Linux deployments are fragile afterall. If a single 
node failure leads to 6-8 disks/OSD's missing, you are gonna saturate your internal 
storage network and really max out the spinning rust, shutting down your external 
storage network.</p>

<p>But all that's over now. <a href="http://ceph.com/community/500-osd-ceph-cluster/" 
target="_b">http://ceph.com/community/500-osd-ceph-cluster/</a></p>

<p>I think the idea is to setup Crushmaps such that data parity is stored across 
multiple nodes and multiple racks, etc as you grow. If you have multiple nodes, a loss 
of one node should make no difference even if it takes out 8 OSDs, if you setup the 
crushmap properly.</p>

<p>Having a single node per OSD would create the ultimate redundancy, but I don't think 
there are any installations anywhere in the world that does this because the cost would 
be astronomical in network gear alone.</p>

<p>check out this diagram for an idea of house to setup crush buckets as you scale.</p>

<p><a href="http://docs.ceph.com/docs/master/_images/ditaa-91dff8176c752894890e24c5e8844d0fdfb8a890.png" target="_b">http://docs.ceph.com/docs/master/_images/ditaa-91dff8176c752894890e24c5e8844d0fdfb8a890.png</a></p>

<p>this article has a good writeup of how to scale parity from multiple OSDs in one node 
to multiple nodes to multiple racks.</p>

<p><a href="http://docs.ceph.com/docs/master/rados/operations/crush-map/" 
target="_b">http://docs.ceph.com/docs/master/rados/operations/crush-map/</a></p>

<p>Thank you for that link.</p>

<p>Yes I agree.  But to achieve even RAID comparable redundancy and HA for the home user 
is impacticle at this time because many, many nodes are required. </p>

<p>The future is <a href="http://ceph.com/community/500-osd-ceph-cluster/" 
target="_b">http://ceph.com/community/500-osd-ceph-cluster/</a></p>

<p>Everyone knows that ceph requires many nodes, but to get HA you need many, many 
nodes. And HA Ceph is when you can start to compare it to raid, since Ceph is so 
inherently fragile. </p>

<p>I was thinking about this a couple days ago, but with Raspberry PIs.</p>

<p>In my mind, ceph's osd/mon model would be perfect for this. Just get RasPis and those 
WD PiDrives, and each set runs exactly one OSD. Right now, about 120 USD buys you a 1Tb 
OSD on this format. The next obvious step would be to have the OSD running inside the 
drive's own firmware.</p>

<p>Apparently, I've been beat to it by the pros</p>

<blockquote><p><b>If you could address all of my complaints</b></p></blockquote>

<p>I can't, it's definetly more complicated. The trade off is being able to add drives 
of different sizes to the same host and adding an entire new host in a few years when 
the NAS needs upgrading. You can also upgrade your parity down the line to anything you 
need. That flexibility comes with more complexity. </p>

<p>Data integrity is only iffy if you are using CephFS, which doesn't have a proper fsck 
yet (devs don't consider cephfs production ready). If you use block provisioning, the 
data integrity is well protected by design. </p>

<p>Great news, as of the Jewel release CephFS has been marked stable with one active MDS 
(you can have any many standby as you want). It has "fsck" tools.</p>

<p>oh nice, I haven't seen any info on it but it's only been a week. Hopefully people 
write about their experiences and stability.</p>

<p>Well, I suppose you do have your answer as to why perhaps not everyone uses it.</p>

<p>For me, I don't use parity, as you could probably tell from my zfs commands, I use 
mirror vdevs. I already expanded my zpool from 10TB usable to 16TB usable, and the 
performance is much better across the board as well as of course more storage. ZFS does 
everything I need it to, and more. I'm perfectly happy with ZFS.  :D</p>

<blockquote><p>The trade off is being able to add drives of different sizes to the same 
host</p></blockquote>

<p>I can do that with Btrfs</p>

<blockquote>
<p>adding an entire new host in a few years when the NAS needs upgrading</p>
</blockquote>

<p>I dont' have that much data yet.</p>

<blockquote><p>I can do that with Btrfs</p></blockquote>

<p>fair enough, one of the things I really like about btrfs. I wanted to use btrfs over 
zfs but a year ago when I built my NAS it was not ready. </p>

<blockquote>
<p>I dont' have that much data yet.</p>
</blockquote>

<p>for my situation, which is probably similar to a lot of other home users, I built my 
first proper NAS last year. It has 5xdrives and runs freenas. I also have two other 
systems which are really just a hodge podge of drives running windows. They were 
collected over the years before I knew/had the money to build a proper NAS. Even if I 
had Ceph now, I may not be using them but the thing is that the barrier to upgrade was 
very high. I would have liked to upgrade components in the existing system but instead 
I had to get a whole new system all at once. So instead of dropping money over time I 
had to do it all at once, configure a new system and move everything over in a huge 
process. If I follow the same plan, I will likely build out another completely new NAS 
in 4-5 years and migrate everything there. Instead if I had a Ceph system today I could 
just be upgrading stuff over time and growing it slowly. When old stuff/drives are no 
longer cost effective I can just pull them out over time instead of the entire system. 
</p>

<p>Ceph might not be more efficient compared to the new NAS I have now, but if I take 
into account my normal upgrade patterns, I think it would be better over 3-5+ years. </p>

<p><b>How does ceph compare to BTRFS?</b></p>

<p>BTRFS has raid 5/6 so max of 2 drive redundency. Realistically you don't want more 
than 10 drives per vdev, but I'd probably not use more than 7. You can add drives and 
increase from raid 5 to 6, but the drives have to be the same size. The raid 5/6 tool 
are very new and not well tested yet.</p>

<p>Basically btrfs works well for small builds where all the drives are the same.</p>

<p>Ceph can work for small build, but better with large builds. Ceph can also use any 
drive size. </p>

<p>BTRFS is working okay for me ATM with different drive sizes, but only in 
"single"/JBOD mode, not RAID.</p>

<p>that's fair. Some people are moving towards no parity just 3x backup type setups. </p>

<p>I'm going to have a RAID eventually, (when I can afford more than I "need" to store) 
so that's why I'm looking into this stuff. It just doesn't affect me <em>yet</em>.</p>

<p>.</p>

<p>So it's not a filesystem itself, at least not a low-level FS?</p>

<p>shit, sorry. I deleted my comment because it was sooo wrong. Last month it would have 
been reasonable. Let me explain.</p>

<p>Yes, ceph is not a filesystem, infact, it relies on a filesystem. Ceph stores parts 
of objects as files on a regular linux filesystem. Whichever filesystem you choose. I 
used to think (I also thought other people thought this too) that Ceph was going to, 
in the future, prefer to use BTRFS as its underlying filesystem. Now I'm reading it 
was just announced <a href="http://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/" 
target="_b">http://www.sebastien-han.fr/blog/2016/03/21/ceph-a-new-store-is-coming/</a> 
we are going to bypass filesystems all together? I don't understand.</p>

<p>Okay, then. Interesting.</p>

<p><b>Could you share a detailed guide? I would like to try a new Linux  installation 
using ZFS</b></p>

<p>you can run it natively like this: </p>

<p><a href="https://github.com/zfsonlinux/pkg-zfs/wiki/HOWTO-install-Ubuntu-to-a-Native-ZFS-Root-Filesystem" 
target="_b">https://github.com/zfsonlinux/pkg-zfs/wiki/HOWTO-install-Ubuntu-to-a-Native-ZFS-Root-Filesystem</a></p>

<p>if not there are plenty of guides on using fuse to get it running. </p>

<p>I would not suggest using ZFS for your Root/OS drive. I put my Linux OS on an SSD, 
and ZFS is used for all my data drives.</p>

<blockquote>
<p>Is it as easy as two commands that are super memorable? If not, I'm probably not 
that interested in it.</p>
</blockquote>

<p>I just hope you don't work as SysAdmin or similar.</p>

<p>Yes, because I'm clearly incapable of understanding more complex Linux commands and situations, and writing scripts:</p>

<p><a href="https://www.reddit.com/r/linuxmasterrace/comments/4ac1u9/and_they_say_linux_is_hard/d0zj9mm" rel="nofollow">https://www.reddit.com/r/linuxmasterrace/comments/4ac1u9/and_they_say_linux_is_hard/d0zj9mm</a></p>

<p>Fuck right off. The two responses I have gotten so far today are from
 two complete moronic jackasses, and both happen to be in this 
subreddit. Go to hell, dipshit.</p>

<p>Just because I don't want something to be difficult doesn't mean I can't handle difficult things.</p>

<p>I
 just stated non-offensive opinion and you totally abuse me right now. 
You can't handle critic mate and reply with full offensive response. GJ 
with life.</p>

<p>Right as if what you said wasn't abusive, rofl. That's why you got downvoted to hell.</p>

<p>You made an incorrect, rude assumption based off of basically no 
information and I told you (rightly so) to fuck off. If you don't want 
to be a dick, don't treat others like one.</p>

<p>Getting your hopes of something isn't making assumptions lol.</p>

<p>Still, getting poked where it hurts makes you scream like little kid,
 right? You can't handle any ounce of critic - that's sums it up. In 2 
replies you called me "jackass", "dipshit", "dick" and whatever which 
just shows up what kind of human being you are.</p>

<p>Again. GJ with life.</p>

<p>Guess
 what? You're the one who started it. You assume I'm a bad sysadmin 
because I didn't want an overly complex home setup, and you assume I 
cannot write scripts because I don't want an overly complex home setup? 
ZFS works fine for me. I don't want to waste my time learning something 
else and migrating to it when ZFS is fine for my needs.</p>

<p>You came off as a dick, and thus you were treated as one. You started
 it, and you got downvoted to hell for it. People agree with me, not 
with you so just drop your attitude.</p>

<p>I'm 22 and own my own house, so I would say I'm doing plenty fine 
with life. I don't need your passive aggressive "GJ with life" (I'm 
pretty sure that's supposed to be GL with life), so you can shove it up 
your ass. The only one acting like a kid here is you by assuming I'm not
 a sysadmin based on VERY limited information - certainly not enough to 
even make an educated guess.</p>

<p>You overuse "assume" word and put words I didn't even say in my mouth. Ok. 22 and can't even hold proper conversation.</p>

<p>Let's end it here, not worth my time.</p>

<p>It's
 mainly complexity.  I've been considering it since I am running 
openstack at home but haven't gotten around to doing it, mainly because 
the cost is high for little benefit.</p>

<p>so
 you manage all your VMs and storage via openstack? ie. to spin up a VM 
you use the openstack interface? I've never used openstack, just checked
 out a few videos.</p>

<p>Yep, I do distro dev work and the quick /easy spin up is awesome. </p>

<p>Agree.
 I have thought this over soooooo many times. I'd love to be able  to 
scale on a platform like this but the best I could come up with is j1900
 boards. There are issues with arm one because of limited RAM,  network 
rebuild speed and I've seen some mention of bugs with xfs on arm. </p>

<p>Things to note.  Repair can be usually be fine, but having gone 
through a major production outage do to bad pg sizing combined with a 
network outage.</p>

<p>Ceph can be intensive on CPU and Memory when things are not working 100% </p>

<p>Hope this helps I'm always looking for a way to do this also cost effectively.  </p>

<p>I've
 been looking into a cheap AMD FM2+ board with 8 SATA ports and a quad 
core processor, 16GB ram. Something like this, but start with only 3 
drives and scale up over time</p>

<p><a href="https://secure.newegg.com/WishList/PublicWishDetail.aspx?WishListNumber=22153594">https://secure.newegg.com/WishList/PublicWishDetail.aspx?WishListNumber=22153594</a></p>

<p>I
 was looking at Supermicro X10SBA. Just trying to keep my power costs 
down. Adding nodes would ramp up power usage pretty quickly. With some 
builds. Then I start to just go back and say meh I'll just do zfs. </p>

<p>nice
 board. I wanted a celeron board but I want to get enough sata ports to 
not need a pcie expander. I wanted to save the pcie for a future 10GBe 
card. 6 ports per node (1 used up for the journal) might not be enough 
for $190.</p>

<p>Although the TDP is 65w it has integrated gpu which is not used so the actual power use of probably very little. </p>

<p>Then again with cooler, my setup is only $20 cheaper</p>

<p>For
 me it's because the FSCK on CephFS isn't reliable. If something does go
 wrong, your data is hosed and you're up a famous creek without a 
paddle. I've been trialing ceph as a potential replacement for our 
production ZFS machines at work, and whilst the scale out features are 
just amazing, the data integrity can fail badly under extreme (but not 
implausible) situations.</p>

<p>Another consideration is that in a small installation with only a few
 OSD's you're going to be taking a serious performance hit for the sake 
of scalability that you'll probably never actually use. </p>
</div></form></div><div class="child"><div id="siteTable_t1_d2m68k6" class="sitetable listing"><div class=" thing id-t1_d2m8xt5 noncollapsed   comment " id="thing_t1_d2m8xt5" onclick="click_thing(this)" data-fullname="t1_d2m8xt5" data-type="comment" data-subreddit="DataHoarder" data-subreddit-fullname="t5_2x7he"><p class="parent"><a name="d2m8xt5"></a></p><div class="midcol unvoted"><div class="arrow up login-required archived access-required" data-event-action="upvote" role="button" aria-label="upvote" tabindex="0"></div><div class="arrow down login-required archived access-required" data-event-action="downvote" role="button" aria-label="downvote" tabindex="0"></div></div><div class="entry unvoted"><form action="#" class="usertext warn-on-unload" onsubmit="return post_form(this, 'editusertext')" id="form-t1_d2m8xt51pc"><input name="thing_id" value="t1_d2m8xt5" type="hidden"><div class="usertext-body may-blank-within md-container "><div class="md"><blockquote>
<p>For me it's because the FSCK on CephFS isn't reliable. </p>
</blockquote>

<p>just checked, seems like they are working on it but it could be a 
while before that happens. Didn't know this was a problem. Thanks for 
the heads up. </p>

<blockquote>
<p>taking a serious performance hit</p>
</blockquote>

<p>any idea how bad that would be? A basic FreeNAS zfs raidz2 system 
with 7 drives can pretty much saturate a gigE nic. Would the same 
hardware (single host, 7 drives) with Ceph 2x redundancy be 
significantly worse?</p>

<p>I've
 never actually tried ceph in a situation with fewer than 3 physical 
nodes and 3 OSD's per node, but in terms of like-for-like against the 
same 9 disks in a single chassis, the ZFS solution was easily twice the 
speed. </p>

<p>That said, this is back of an envelope stuff. I'm working with 
cast-off 5 year old consumer grade kit here trying to get some rough 
idea if it's worth throwing real money at it. So far the answer seems to
 be a qualified "No" which I expect to change to a "Yes" once CephFS 
matures and I'm not forced to do XFS-&gt;RBD-&gt;XFS for filesystems.</p>

<p>You need to think in terms of where the bottlenecks are going to be. </p>

<p>With RaidZ2 your maximum write speed is going to depend on how you 
have your write-through set up, and how fast your intent log devices 
are. If you're not using a dedicated intent log then your maximum write 
speed is going to be limited to the write speed of a single disk inside 
the array. You can be pretty much sure that this will be less than the 
capacity of your network link unless you're really IO bound on the link.</p>

<p>Ceph write performance scales almost linearly with the number of 
OSD's you use, so in theory it can be massively better, but in practice 
it's actually a lot worse unless you have your journal on separate disks
 and really fast network links on each node hosting the actual physical 
OSD's</p>

<p>You can obviously tune the hell out of both so it's hard to give a 
definitive answer as to which is actually better, but Ceph is a lot more
 complex so in terms of the "out of the box" configuration I'd expect 
you to see similar results to what I got. You'd want to spend some time 
reading things written by actual experts to work out how best to tune 
for your workload (There is a sizable difference between the throughput 
you'll get with 10,000 1meg writes vs a single 10G write for example, on
 either system)</p>

<p>thanks for the writeup, appreciate it. </p>

<blockquote>
<p>it's actually a lot worse unless you have your journal on separate disks</p>
</blockquote>

<p>from what I've read, it;s highly discouraged to do anything but have the journal on a separate disk.</p>

<p>My theoretical setup would be two hosts with cheap consumer hardware,
 probably just using onboard 8xSATA. I would configure each host with 3 
HDDs and one SSD for journal then expand the number of drives on each 
host over time. </p>

<p>People
 have covered most of the rest, but I want to warn against using a 
Raspberry Pi in this scenario. The network port is a glorified USB 
network adapter, and as such runs across the USB bus. It's hard to get 
more than about 40MBps unidirectional across the Raspberry Pi's USB bus,
 and if you're connecting your drives via USB then that means 20MBps on 
reads across all drives, absolute max.</p>

<p>I wrote an article a month or two ago about <a href="https://aesospadez.com/single-node-ceph-cluster-your-home-storage-solution/">the benefits of Ceph even for the home user</a>, promising a follow-up on a Single Node Ceph install that should be out shortly (in fact I'm working on it as I type this).</p>

<p>I
 wanted to add that one of my biggest hesitations to running a single 
node ceph is the lack of guides from other people doing it. I think a 
write up would be very popular among <a href="https://www.reddit.com/r/homeserver" rel="nofollow">/r/homeserver</a> and <a href="https://www.reddit.com/r/DataHoarder" rel="nofollow">/r/DataHoarder</a> people </p>

<p>Yeah,
 since I promised the write-up I've had more than a couple people email 
me asking if it's still coming. Within a week I should have something 
posted here. :)</p>

<p>awesome, thanks. </p>

<p>awesome thanks</p>

<p>Please don't forget <a href="https://www.reddit.com/r/homelab" rel="nofollow">/r/homelab</a>.</p>

<p>for
 the Rpi, I was thinking of just using it as a monitor, no drives 
attached. Should have minimal IO and CPU overhead even during a rebuild.
 The SD card should be enough to store the needed data. </p>

<p>Thank for your writeup, looking forward to the single node setup. If 
you have similar hardware running a ZFS setup right now, it might be 
very beneficial to take a benchmark of ZFS vs ceph on the same single 
node hardware. </p>

<p>I'm a bot, <em>bleep</em>, <em>bloop</em>. Someone has linked to this thread from another place on reddit:</p>

<ul>
  <li>[<a href="https://www.reddit.com/r/ceph">/r/ceph</a>] <a href="https://np.reddit.com/r/ceph/comments/4h1b2o/why_is_ceph_so_rare_for_home_use_even_among/">Why is Ceph so rare for home use? Even among technically inclined people, the most common setup seems to be ZFS. : DataHoarder</a></li>
</ul>

<p><a href="#footer"></a><em><sup>If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.</sup> <sup>(<a href="https://www.reddit.com/r/TotesMessenger">Info</a></sup> <sup>/</sup> <sup><a href="https://www.reddit.com/message/compose?to=/r/TotesMessenger">Contact</a>)</sup></em></p>

<p><a href="#bot"></a></p>

<p><a href="http://docs.ceph.com/docs/master/cephfs/">Important:
 CephFS currently lacks a robust 'fsck' check and repair function. 
Please use caution when storing important data as the disaster recovery 
tools are still under development.</a></p>

<p>ZFS has a long, proven track record.  It's not perfect, and it has limitations, but I trust it a lot more than Ceph or Btrfs.</p>

<p>AFAIK
 that is only a problem if you are mounting via cephFS. If you mount 
block storage (similar to an iscusi setup) you don't need to worry about
 it. Also cephfs is not production ready yet anyways. </p>

<p>From what I understand, <a href="http://www.virtualtothecore.com/en/adventures-with-ceph-storage-part-6-mount-ceph-as-a-block-device-on-linux-machines/" rel="nofollow">most kernels can mount block devices natively</a> so you can work around cephfs. </p>

<p>Still good to know, cephfs would make things a lot easier but is still limited as of now.</p>

<p>Great,
 but I don't need network block storage.  I don't have diskless clients,
 and my large storage requirements are for files that need to be 
accessible from several clients (Linux, Windows, Mac) simultaneously.  
I'm currently serving ZFS over NFS and SMB.</p>

<p>I suppose I could mount ZFS/btrfs/whatever on top of network block 
devices.. but what does that buy me over using bare disk?  I don't need 
host redundancy or wicked speed.  Expansion isn't any easier using block
 devices, since I still have all the limitations of my filesystem (such 
as being unable to change a RAIDz2 from 6 disk to 10 disk).</p>

<p>The advantage of re-using existing machines to hold disks rather than
 attaching a dedicated drive cage is not that great, once you factor in 
electric costs to keep all those machines running.  Having to add a 
journal disk to every machine, plus a whole separate machine for the 
monitor, also drives up the cost.  You mentioned a RPi as a monitor, but
 I'm not going to trust the integrity of my data to a machine without 
ECC.</p>

<p>There's nothing wrong with Ceph and it seems really cool.  The two 
use cases they present make sense - backend storage for an app, or 
network block devices for a large KVM installation.  But I don't have 
those cases, and I suspect most home users don't either.  They don't 
need a SAN-type solution providing block storage; they just need a 
really big shared network filesystem.</p>

<p>How do you use Ceph?</p>

<p>all good points. </p>

<blockquote>
<p>since I still have all the limitations of my filesystem (such as being unable to change a RAIDz2 from 6 disk to 10 disk).</p>
</blockquote>

<p>I just wanted to point out that you're not supposed to use raid under
 ceph (if that's what you meant). Each drive should have it's own OSD 
that is managed by RADOS. If you want to grow your array you just add a 
drive. If you want to specify how much redundancy you have between 
drives and hosts (if you have more than one) you do it with a CrushMap. </p>

<blockquote>
<p>How do you use Ceph?</p>
</blockquote>

<p>I don't have a ceph setup, I'm considering it for my next build. </p>

<p>for my situation, which is probably similar to a lot of other home 
users, I built my first proper NAS last year. It has 5xdrives and runs 
freenas. I also have two other systems which are really just a hodge 
podge of drives running windows. They were collected over the years 
before I knew/had the money to build a proper NAS. Even if I had Ceph 
now, I may not be using them but the thing is that the barrier to 
upgrade was very high. I would have liked to upgrade components in the 
existing system but instead I had to get a whole new system all at once.
 So instead of dropping money over time I had to do it all at once, 
configure a new system and move everything over in a huge process. If I 
follow the same plan, I will likely build out another completely new NAS
 in 4-5 years and migrate everything there. Instead if I had a Ceph 
system today I could just be upgrading stuff over time and growing it 
slowly. When old stuff/drives are no longer cost effective I can just 
pull them out over time instead of the entire system.</p>

<p>Ceph might not be more efficient compared to the new NAS I have now, 
but if I take into account my normal upgrade patterns, I think it would 
be better over 3-5+ years. </p>

<p>I'm about to setup gluster,  a popular ceph competitor, at home to host VM images for KVM.</p>

<p>I'd be interested in a writeup about your experiences if you get the chance. </p>

<p>I'll try to remember to do that. No idea exactly when it would be.</p>

<p>Any progress on this?</p>

<p>Unfortunately, not yet.</p>

<p>I
 started using CephFS on top of ZFS mostly as an experiment. I'm excited
 that it's going to be put into FreeNAS10 and will probably switch over 
to doing it that way. A lot of people only use a single machine for home
 fileserver use and just using ZFS on a single host is more than enough 
in those situations. Ceph adds another layer of complexity that you 
don't really get with ZFS since you have to worry about running all of 
the daemons.</p>

<p>wait, you have ZFS-&gt;OSD-&gt;RADOS? How does that work, I though Ceph can only work on btrfs, xfs and ext4?</p>

<p>You <em>can</em>
 put ceph OSD's on any base FS you like, but InkTank haven't tested it 
on other configurations and if you run into problems and ask for help 
they're just going to tell you that what you're doing is unsupported.</p>

<p>good
 to know, thanks. Out of curiosity, with btrfs you can enable parallel 
journalling so writes go to a journal drive and are written to the OSDs 
in parallel. Do you know if you have this setup on your ZFS system?</p>

<p>For home use, I'm fairly happy with btrfs.</p>

<p>Until recently I was using Fedora on my NAS, plus samba, netatalk, etc. I just migrated to <a href="http://rockstor.com/" rel="nofollow">Rockstor</a>, which seems decent.</p>

<p>how well do the docker rockons work? I tested it a year ago when building my nas and it really wasn't developed enough. </p>

<p>No
 idea. The plex one was out of date when I checked. Syncthing looks like
 it might need to be run per-user, which means a single docker isn't 
going to work for me.</p>

<p>It would be nice if they let you use generic dockers, instead of just their provided ones.</p>

<p>I'm still primarily relying on my VMs (run on another server, but 
with their storage hosted on rockstor's btrfs -- yeah, I'm keeping an 
eye on the fragmentation)</p>

<p>You can run your own Dockers by placing the json file in the /opt/rockstor/rockons-metastore directory of your Rockstor install.</p>

<p>if
 you are not using the rockons is the main advantage just the fact that 
you have a nice dashboard UI? Otherwise you could just run centOS 
yourself right?</p>

<p>How do you run plex? Do you have an iscsi target from the plex VM to 
the rockstor? Seems like you might get better performance running centOS
 and then running docker yourself to get plex and other services. </p>

<p>Pretty
 much the nice dashboard UI, and knowing somebody else is following up 
on btrfs happenings (presumably) better than I am. I was doing my own 
(well, I was using Fedora instead of CentOS). The fancy web interface 
also handles users, samba, nfs. There's scheduling for scrubs, etc. All 
that stuff I was doing manually or with home-grown scripts that <em>usually</em> worked, or with snapper. There's an advantage of being in a herd instead of being on my own, particularly if something breaks.</p>

<p>Currently, I run plex in a Fedora KVM VM, installed with their RPM. That VM mounts <code>export/media</code> via nfs from the rockstor nas.</p>

<p>The KVM host (still running Fedora, but I might try out centos-based 
ovirt at some point) has all VM's main backing store as skinny qcow2 
files (with TRIM enabled, but that was more as an experiment), also 
mounted from the rockstor via nfs.</p>

<p>Other than the brief look at rock-on based dockers, I haven't done 
any further investigation of docker. I'm still VM-based, due to having a
 convenient bootstrap script that will take a centos|fedora cloud image,
 create a cloud-init bootstrap image, and give me a fresh VM very 
quickly. I've got about seven or eight permanent VMs, and haven't begun 
reading about converting them to dockers</p>

<p>Not sure I will convert, either. I'm comfortable with VMs.</p>

<p>nice,
 thanks for the writeup. If you like having full VMs and using a 
dashboard interface, checkout openstack. Has a nice interface for 
spinning up VMs and can scale across multiple hardware hosts if you ever
 expand. </p>

<p>I'll check it out. How does it compare to ovirt (also on my list, but not something I've checked out yet)</p>

<p>Currently I'm just using virsh on the command line, and virt-manager if I require a GUI.</p>

<p>they all sort of interconnect, check out this:</p>

<p><a href="https://youtu.be/elEkGfjLITs?t=431" rel="nofollow">https://youtu.be/elEkGfjLITs?t=431</a></p>

<p>I went to an openstack event ~4 years ago and saw Ceph for the first time.</p>

<p>I loved the look of it...</p>

<p>Went home, tried to install it, couldn't get started... just kept failing and more.</p>

<p>Got drives on the shelf, but, work has got in the way and I just haven't had time to revisit.</p>

<p>I haven't got ZFS up either, but, I need something! There are 
certainly a lot more guides for ZFS than Ceph which I think is the main 
reason it is used so much as support is so much easier.</p>

<p>in
 theory you can run an openstack for computing/VMs and use it to manage 
your storage via cinder-&gt;ceph but I wouldn't try that for beginners. I
 haven't found any single node ceph guides so no idea how well it works 
but several people on <a href="https://www.reddit.com/r/HomeServer" rel="nofollow">/r/HomeServer</a> and <a href="https://www.reddit.com/r/selfhosted" rel="nofollow">/r/selfhosted</a> use openstack. You can ask around there if you want to give it another try. </p>

<p>I think that the real problem is that Ceph users don't have a space like <a href="https://www.reddit.com/r/homelab" rel="nofollow">/r/homelab</a> and <a href="https://www.reddit.com/r/datahorder" rel="nofollow">/r/datahorder</a> which are very focused on Plex NAS's. <a href="https://www.reddit.com/r/Ceph" rel="nofollow">/r/Ceph</a> is not a thing, nor is <a href="https://www.reddit.com/r/openstack" rel="nofollow">/r/openstack</a>
 good for ceph users, even Fuel (which is good for deploying ceph) just 
uses ask:opensource, anything deeper is running on mailing lists and IRC
 from 1991. I am very glad you asked this question. We are here, just 
not that vocal.</p>

<p>yup,
 there is a serious lack of tutorials and discussion for deploying ceph 
on smaller scales. A good start would simply be for people with home 
ceph setups posting their systems here with some thoughts/experiences. 
I'm not convinced it's as complicated as people seem to think it is, the
 problem is the lack of easy to follow resources. </p>

<p>I've
 used openstack before and I'm interested in Ceph or swift like you for 
home use. Zfs is not what I'm looking for, I like the expandability of 
Ceph, the replication, the ability to mount a disk image, to treat it 
like s3 in my home for object storage. I'll pit this together at home in
 the next few weeks and see how it goes. I'd be interested to hear 
others experience too. </p>

<p>nice, please write up a post with your experiences if you get the time.</p>

<p>Compression is one.  Hopefully bluestore will offer this.</p>

<p>Not to dis on Ceph but, I ask the same thing about MooseFS and/or LizardFS (fork of Moose).</p>

<p>Why would any home user use Ceph over this?</p>

<p>You get (most of) the same benefits (replication, redundancy speed 
increase with more clients)**, with MUCH less hassle or learning curve. 
 Plus a nifty looking GUI webpage (find some on google).  And all the 
daemons can work on Windows as well (minus the client part since no 
FUSE) via a cygwin compile of the code.  Then you connect from Windows 
to it via a Samba (or NFS directly if you have the enterprise version of
 Win7 or 10).</p>

<ul>
  <li><p>Linux install:   (setup apt repo), apt-get install, tweak configs, profit!</p></li>
  <li><p>Windows install:   download source from github, install Cygwin, 
(./configure &amp;&amp; make &amp;&amp; make install), tweak configs, 
profit!</p></li>
</ul>

<p>** Moose/Lizard do have some architectual diffs that make them 
impossible to scale to quite the same enterprise level as Ceph but that 
will never matter in a home/soho environment.</p>

<p>** Also no native block store either on Moose/Liz but ESXi doesnt do 
native Ceph anyway so you'll still have to hack an iSCSI/NFS solution to
 mount.  So you might as well just export an NFS share for your ESXi 
datastore (that's what I do).</p>

<p><strong>TL;DR</strong>   Ceph is CLEARLY better for the large 
enterprise, but Moose/Lizard seems much better suited to SoHo and home 
user environments.  And maayybe even medium size businesses too.</p>

<p>I'm
 running Ceph at home, I previously used FreeNAS which was great to use 
but eventually I had to sell off the box it was running on. I didn't 
want to be in that situation again where one box was running the entire 
show. I do however have some regrets with Ceph:</p>

<ol>
  <li><p>It hates latency. Absolutely hates it. So at the moment I've got 
about 10TB attached to a box running ESXI, the disks are used as 
datastores with virtual disks running on top of that. It's way slower 
than 4 drives should be, I get performance of about 60MB/s on a good day
 but 30MB/s average would be fair. </p></li>
  <li><p>Try and get it to run on ARM, I dare you. I've been trying and 
have been having a terrible experience. There are research projects from
 major drive manufacturers treading this path but they must have some 
magic or a whole lot of patience on their side because I just can't get 
it to work. The official packages have bugs, compiling it yourself 
doesn't work. x86 only.</p></li>
  <li><p>It doesn't distribute data perfectly evenly. I've got about 1.5TB
 free in my array and am still getting warnings about an OSD being full 
or near full because the distribution isn't perfect. There's one drive 
at 95% and others at 60 or 70%. Remember that when one OSD fills the 
entire cluster stops accepting writes.</p></li>
</ol>

<p>As
 a 40TB home datahorder with 7 years professional experience in data 
storage and backup development...I'm going with awareness as I've never 
heard of it.</p>

<p>yup,
 I'm hoping that this thread can convince someone to do a writeup of a 
single node ceph build. I think part of the reason no one does it is 
because no one has done it...at least bloged about it. </p>

<p>I have such a "Dream setup" at home and very happy with it. Here some points about my build and lessons learned:</p>

<ul>
  <li><p>I started with 3 Supermicro A1SAi-2750F mobos with 32GB ECC RAM each;</p></li>
  <li><p>Each mobo houses openstack hypervisor and ceph mon/osd daemons;</p></li>
  <li><p>4 ethernet interfaces are for openstack frontend, openstack backend, ceph frontend, ceph backend;</p></li>
  <li><p>it's very important to separate ceph front-net from back-net;</p></li>
  <li><p>don't even think of using non-ECC RAM for ceph.</p></li>
</ul>

<p>Valuable benefits I have:</p>

<ul>
  <li><p>openstack live migration rocks - I can migrate all VMs from one 
node and powerdown it for hardware-upgrade/rebuild or even exchange to 
another one, and all that without stopping my services (VMs);</p></li>
  <li><p>easy (personal quick installation scenario) extending or shrinking of cluster - from one OSD to whole node;</p></li>
  <li><p>desktop access to VMs from everywhere through novnc via standard 
https protocol - no need to install RDP/VNC-software or open exotic 
ports.</p></li>
</ul>

<p>wow, that sounds amazing. Thanks for posting.</p>

<p>One thing I've been concerned about is bitrot protection, especially 
with the new bluestore that doesn't using an underlying btrfs 
filesystem. In theory if you have 3x replication, the filesystem can go 
by consensus, if 2 blocks are the same and 1 block is off, the bad block
 can be recovered. AFAIK the system doesn't actually do this during a 
scrub. ECC ram will help avoid it, but the filesystem doesn't seem to be
 self healing like btrfs/zfs. Do you have any thoughts on the issue? 
Have you had any problems?</p>

<p>You are right, ceph ensures data integrity by <a href="http://docs.ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing" rel="nofollow">scrubbing</a>.
 It can determine incorrect data blocks not only by comparing it's 
replicas, but by using checksums. If it finds incorrect data block, it 
can be recovered from healthy one manually by "ceph pg repair" command. 
There is no need in self-healing filesystems or redundant RAID arrays, 
as ceph has it's own methods of data integrity protection.</p>

<p>I didn't test bluestore yet, but will try it when be adding new OSD. 
Had problems using btrfs when some OSDs were overfilled with btrfs 
snapshots and I have to manually clean them up to start osd daemons 
again. Now I'm using xfs with no problems.</p>

<p>thanks
 for the info. I'm surprised I haven't found very much discussion online
 about scrubbing and repairing. I haven't really seen any articles about
 people testing it by corrupting data on purpose and seeing if the 
repair works. There are many articles and test data of ZFS repair so 
it's a bit disconcerting to hardly find anything regarding ceph.</p>

<p>In any case, thanks for your thoughts.</p>

<p>Ceph
 gets better with more OSD's. Running a single node Ceph instance is 
almost silly unless you have a ton of OSD's, ZFS much better serves this
 purpose however there are cost issues. I have been very much enjoying 
Ceph across 3 Pi's to help with the performance constraints and to help 
keep costs down. </p>

<p>They are still looking aint making OSD's faster as well. With a 
limited number of OSD's or no plans to actually scale, there is no 
reason to use Ceph sorry to say. </p>

<P>I'm about to setup gluster, a popular ceph competitor, at home to host VM images for 
KVM.</p>


</body></html>
