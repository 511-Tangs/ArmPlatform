<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#" lang="en-US">
  <head><title>Building a Ceph Cluster on Raspberry Pi - Bryan Apperson</title>
  </head>
  <body>

<h3> The Definitive Guide: Ceph Cluster on Raspberry Pi 
<a target="_b" 
href="http://bryanapperson.com/blog/the-definitive-guide-ceph-cluster-on-raspberry-pi/" 
target="_b">(Source origin)</a></h3>

<h4>Bryan Apperson</a> 
From the Cloud to The Ocean</h4>

<footer><a href="http://bryanapperson.com/blog/the-definitive-guide-ceph-cluster-on-raspberry-pi/"><time>May 13, 2015</time></a> 
<span>15 minute read</span> <span>

<p>3 Node Ceph Cluster on Raspberry Pi</p>

</div> <span id="scroll-to-content">

<p>A Ceph cluster on Raspberry Pi is an awesome way to create a RADOS home storage 
solution (NAS) that is highly redundant and low power usage. It's also a low cost way 
to get into Ceph, which may or may not be the future of storage (software defined 
storage definitely is as a whole). Ceph on ARM is an interesting idea in and of itself. 
I built one of these as a development environment (playground) for home.  It can be 
done on a relatively small budget. Since this was a spur of the moment idea, I 
purchased everything locally. I opted for the 
<a href="https://www.raspberrypi.org/products/raspberry-pi-2-model-b/">Raspberry Pi 2 
B</a> (for the 4 cores and 1GB of RAM). I'd really recommend going with the Pi 2 B, 
so you have one core and 256MB RAM for each USB port (potential OSD). In this guide I 
will outline the parts, software I used and some options that you can use for 
achieving better performance. This guide assumes you have access to a Linux PC with 
an SD card reader. It also assumes you have a working knowledge of Linux in general 
and a passing familiarity with Ceph.</p>

<a name="Parts"></a>
<h4>Parts</h4>

<P> Also check: <a href="./DIYBigData.html#BOM" target="_b">Bits and Parts</a></P>

<p>Although I will explain many options in this guide, this is the minimum you will need 
to get a cluster up and running, this list assumes 3 Pi nodes.</p>


<PRE>
--------------------------------------------------------------------------------------
3 x 3ft Cat6 Cables
3 x Raspberry Pi 2 B
3 x Raspberry Pi 2 B Case
3 x 2 Amp Micro USB Power Supply
3 empty ports on a gigabit router
3 x Class 10 MicroSD (16GB or more) for OS drive
3-12 x USB 2.0 Flash Drives (at least 32GB, better drive for better performance)
--------------------------------------------------------------------------------------
Note: 1. 3 x SD card reader. 
      2. 128GB MicroSD will be better, since we need to host compute VM for computation 
         or data manipulation
      3. Pi 3 would be better, it can power one external (1T or 2T) usb harddisk.
         But 1M memory for 1GB storage, will 512G disk be the maximum?  Why 1M 
         memory for 1GB storage? From somewhere, I read Pi 3 needs a heat sink.
--------------------------------------------------------------------------------------
</PRE>

<P> Yes, I found it! <a target="_b" 
    href="http://docs.ceph.com/docs/master/start/hardware-recommendations/#ram">1M 
    memory for 1G storage</a>, but this is for MDS and MON. OSDs do not require as 
    much RAM for regular operations (e.g., 500MB of RAM per daemon instance); 
    however, during recovery they need significantly more RAM (e.g., ~1GB per 1TB 
    of storage per daemon).  

<P> Would <a href="http://www.digitaltrends.com/computing/c2-offers-competitive-specs-to-raspberry-pi/" 
    target="_b">ODROID-C2</a> be a better choice? 2GB DDR3 SDRAM and Gigabit Ethernet.
    But, it does not support native Wi-Fi and Bluetooth wireless connection standards 
    by default.  Also, there is one more candidate: ODROID-XU4

<h4><a href="http://forum.kodi.tv/showthread.php?tid=280865" target="_b">ODROID-C2 vs 
    Pi3</a> and Odroid-xu4</h4>

<OL>
  <LI> The C2 is very impressive and has the following benefits over the Pi 3 : GigE, 
       HEVC 10 bit, HDMI 2.0 2160/60p output, HD Audio bit streaming, and very swift.

  <LI> Pi 3 has two main advantages over the C2 : Integrated WiFi and Bluetooth, Full 
       3D MVC decode and frame packed 24p output and PCM 5.1/7.1 output (which should 
       be possible on the C2 but has yet to appear), much more up to date Linux kernel, 
       so far better support for current USB DVB tuners, WiFi dongles etc.

  <LI> The Pi 3 also has a much larger development team and is probably still better 
       supported. (I'm not dissing the C2 developers at all - just that as there are 
       fewer of them, some stuff takes longer to appear, and if they decide to 
       concentrate on other platforms in a few months then support will wane)

  <LI> ODROID-XU4 ($74, Cortex-A15, [8] Octacore, Samsung, that's bad, 4x Cortex-A15 @ 
       2.0GHz and 4x Cortex-A7 @ 1.4GHz) has two builtin USB3 ports. eMMC, stands for 
       embedded MultiMediaCard. <a  target="_b" 
       href="http://ameridroid.com/products/64gb-emmc-module-u-linux">64GB eMMC Module 
       U Linux</a> with Transcend USB 3.0 adapter $75.9. (Only red ones work with xu4) 
       Total: $171.59 Caveat: both Cortex-A15 and Cortex-A7 are 32-bit CPUs, am I right?

    <UL>
      <LI><a href="./OdroidXU4Review.html" target="_b">ODROID-XU4 Review</a>&nbsp;

        <P> 8 cores, 2 USB 3.0 ports, 1 USB 2.0 port.  2GB ram, GB ethernet.  With 1 
            (or 2) TB external USB 3.0 disk, 1G memory for ceph, we can have 2 more VMs 
            (via kvm?) for data processing.  &nbsp;&nbsp;
        <P> Do we need 128GB USB 3.0 flash disk? VM templates do need a lot of space.  
            If we do get this flash disk, our VMs will be placed in here, but no more 
            USB 3.0 port!  Also, do we have enough power supply for USB 3.0 TB disk and 
            USB 3.0 flash disk simultaneously?

        <P> USB3 to Ethernet Adaptor: This ethernet adapter will be attached to the head 
            node to allow external communications.  This way, head node will not be able 
            to have 128GB USB 3.0 flash disk.  Hence, no VMs can be created on it. By the
            way, USB 3.0 to Gigabit Ethernet Adapter adds a single RJ45 Ethernet port to 
            a USB-enabled computer system, with support for Gigabit network connections 
            at full bandwidth, unlike USB 2.0 adapters.

            <div><img src="USB31000S.main.jpg"></div>

        <P> <a href="http://www.hardkernel.com/main/products/prdt_info.php?g_code=G143556253995" 
            target="_b">XU4 Shifter Shield</a> ($18) is a must-have for I/O expansion 
            and electronics experiments.

      <LI>Partitions of 64GB eMMC:


<P><b>Partition Contents</b>


<P> Check <a href="http://odroid.com/dokuwiki/doku.php?id=en:xu3_building_kernel#linux1" 
target="_b">At least two partitions, first partition can be ext4</a></p>

<PRE>
Partition 1:

    Kernel Image (zImage)
    boot.scr
    exynos5422-odroidxu3.dtb
    uInitrd (if applicable)

Partition 2:

    rootfs (a.k.a. File System)
</PRE>



<UL>
  <LI>14GB for root

<P><b>Note:</b> Using <b>dd</b> command,  it would create two partitions in this logic 
   disk, am I right?
  <LI> 2GB for swap
  <LI> 8GB for tmp
  <LI> 8GB for var
  <LI>12GB for /usr/local 
  <LI>20GB for /src1

    <P> During ceph source compilation, we need more than 25GB disk space.  Hence, 
        cnfs (ceph nfs) VM has 40GB root filesystem.
</UL>

      <LI><a href="http://odroid.com/dokuwiki/doku.php?id=en:odroid-xu4" 
          target="_b">odroid-xu4 spec</a>  &nbsp;&nbsp;<a  target="_b" 
       href="http://odroid.com/dokuwiki/doku.php?id=en:xu4_release_debian_mainline">xu4
       release_debian</a>  &nbsp;&nbsp;<a  target="_b" 
       href="#FlashingLinuxImageToXu4">Flashing Linux image onto eMMC</a>
      <LI><a href="http://ameridroid.com/products/wifi-module-5" target="_b">WiFi Module 
          5</a>, USB 3.0, $21.95, (only master node needs wifi-module-5?)&nbsp;&nbsp;
          <a href="http://www.hardkernel.com/main/products/prdt_info.php?
             g_code=G141630348024" target="_b">WiFi Module 4</a>, USB 2.0 host interface 
          $14.00.  Only the Ceph master needs this gadget? Also, a GB switch or gateway 
          for this ceph cluster?&nbsp;&nbsp;

         <P>We only have two USB3 ports, one for 1 (or 2) TB USB3 hard disk, the other 
            for 128 GB USB 3.0 flash disk. The VM images are to be created in this 
            flash disk, I think. 
                     
      <LI><a href="https://github.com/ioft/linux" 
          target="_b">arch/arm/configs/odroidxu4_defconfig</a> linux 4.8 
          &nbsp;&nbsp;<a href="https://github.com/tobetter/linux/tree/odroidxu4-v4.8" 
          target="_b">linux 4.8</a>
      <LI><a href="https://github.com/umiddelb/armhf/wiki/How-To-compile-a-custom-Linux-kernel-for-your-ARM-device" 
          target="_b">compile a custom Linux kernel</a>

<PRE>
# Probably, may compile kernel by ourself.
$ find /src3/kernel/linux-source-4.8/ -name "*[Oo]droid*"
/src3/kernel/linux-source-4.8/arch/arm64/boot/dts/amlogic/meson-gxbb-odroidc2.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos4412-odroidu3.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos5422-odroidxu4.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/meson8b-odroidc1.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos54xx-odroidxu-leds.dtsi
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos5410-odroidxu.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos4412-odroid-common.dtsi
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos5422-odroidxu3-common.dtsi
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos4412-odroidx.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos5422-odroidxu3-lite.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos5422-odroidxu3.dts
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos5422-odroidxu3-audio.dtsi
/src3/kernel/linux-source-4.8/arch/arm/boot/dts/exynos4412-odroidx2.dts
</PRE>
     <LI>Build kernel: <a href="http://odroid.com/dokuwiki/doku.php?id=en:xu3_building_kernel" 
          target="_b">ODROID building linux kernel</a> (Linux 3.10, too old?) &nbsp;
         Especially, <a target="_b" 
href="http://odroid.com/dokuwiki/doku.php?id=en:xu3_building_kernel#kernel_rebuild_guide">Kernel Rebuild Guide</a><br> 
         <a href="http://odroid.com/dokuwiki/doku.php?id=en:xu3_release_linux_ubuntu" 
          target="_b">Ubuntu 16.04</a> LTS (Kernel 3.10.92)&nbsp;&nbsp;
      <LI><a href="https://github.com/igorpecovnik/lib/" 
          target="_b">build my own image or kernel</a>
      <LI><a href="http://forum.odroid.com/viewtopic.php?f=52&t=25207" 
          target="_b">use the u-boot mainline to boot mainline 4.x kernel.</a>
      <LI><a href="https://obihoernchen.net/1416/odroid-xu4-tune-network-and-usb-speed/" 
          target="_b">odroidxu4 tune network and usb</a>
      <LI><a href="http://forum.odroid.com/viewtopic.php?f=53&t=6173" 
          target="_b">eMMC Recovery Tool for XU3/XU4</a>
          <P> Wish we don't need this.  Just in case.
    </UL>

  <LI> So, we need 5xOdroid-XU4, 5x 64GB eMMC (with Transcend USB 3.0 adapter), 
       5x1TB-USB-3-external-Disk, 5xFlashDisk, 1xWiFi Module 4, 1xGB Switch.  Am I 
       missing anything else? Maybe one or two spare parts?<p></p>

  <LI> Software archives

    <UL>
      <LI><a href="https://image.armbian.com/" target="_b">armbian images</a>: 
          Armbian_5.20_Odroidxu4_Debian_jessie_3.10.103_desktop.7z&nbsp;&nbsp;
          Need <a href="#downloading" target="_b">7-zip</a> to extract.&nbsp;&nbsp;
          <a href="http://dn.odroid.com/5422/ODROID-XU3/Ubuntu/" 
          target="_b">ubuntu-16.04-mate-odroid-xu3-20161011.img.xz</a>

          <P><b>Note: (02/13/2017)</b> <a href="https://github.com/igorpecovnik/lib" 
          target="_b">Armbian build tools</a>, a package similar to buildroot?  
          I am sure Odroid-xu4 is supported.  
      <LI><a href="http://oph.mdrjr.net/odrobian/" target="_b">odrobian</a>, similar 
          to raspbian&nbsp;&nbsp;

          <P><a href="http://oph.mdrjr.net/odrobian/pool/5422/c/" 
          target="_b">Cloudshell</a>. Cloudshell for XU4 is an affordable DIY Network 
          Attached Storage (NAS) Solution
      <LI><a href="http://oph.mdrjr.net/odrobian/doc/" 
          target="_b">odrobian doc</a>
      <LI>Put the next line in the sources.list?

        <P><a href="http://oph.mdrjr.net/odrobian/apt-sources/public/odroid-5422.list" 
          target="_b">deb http://oph.mdrjr.net/odrobian/ odroid 5422</a>&nbsp;&nbsp;
          Does this refer to <a href="http://oph.mdrjr.net/odrobian/pool/5422/" 
          target="_b">odroidxu4</a>?
    </UL>
 

<P><a href="https://www.percona.com/blog/2016/07/13/using-ceph-mysql/" 
target="_b">Ceph On Odroid-XU4</a>&nbsp;&nbsp;
<a href="https://blogs.s-osg.org/install-ubuntu-run-mainline-kernel-odroid-xu4/" 
target="_b">Install Ubuntu on ODROID-XU4</a>&nbsp;&nbsp;
<a href="https://github.com/hardkernel/linux/tree/odroidxu4-v4.2-rc1" 
target="_b">odroidxu4-v4.2</a>, release candidate.&nbsp;&nbsp;

  <LI> <a href="http://www.jeffgeerling.com/blog/2016/review-odroid-c2-compared-raspberry-pi-3-and-orange-pi-plus" 
target="_b">odroid-c2 review</a>
</OL>


<p> I used 3 x 64GB flash drives, 3 x 32GB MicroSD and existing ports on my router. My 
cost came in at about $250. You can add to this list based on what you add to your 
setup throughout the guide, but this is pretty much the minimum for a fully functional 
Ceph cluster.</p>

<h4>Operating System</h4>

<p> Raspbian. The testing repository for Raspbian has the many packages of Ceph 0.80.9 
and dependencies pre-compiled. Everything you'll need for this tutorial and is the "de 
facto" OS of choice for flexibility on Raspberry Pi. You can download the Raspbian image 
here: <a href="http://downloads.raspberrypi.org/raspbian/images/" 
target="_b">Raspbian Download</a>.  Once you have the image, you'll want to put it on an 
SD card. For this application I recommend using at least a 16GB MicroSD card (Class 10 
preferably - OS drive speed matters for Ceph monitor processes). To transfer the image 
on Linux, you can use DD.  Run the <b>lsblk</b> command to display your devices once 
you've inserted the card into your card reader. Then you can use dd  to transfer the 
image to your SD. The command below assumes the image name is raspbian-wheezy.img  and 
that it lives in your present working directory. The above command also assumes that 
your SD card is located at /dev/mmcblk0 adjust these accordingly and make sure that your 
SD card doesn't contain anything important and is empty.</p>


<PRE>
--------------------------------------------------------------------------------------
sudo dd bs=4M if=raspbian-wheezy.img of=/dev/mmcblk0
--------------------------------------------------------------------------------------
</PRE>

<p>This command will take a few minutes to complete. Once it does, run <b>sync</b> 
to flush all cache to disk and make sure it is safe to remove the device. You'll 
then boot up into Raspbian, re-size the image to the full size of your MicroSD, set 
a memorable password, overclock if you want.</p>

<p>Once this is done there are a few modifications to make. We'll get into this in the 
installation section below. I don't recommend using too large of a MicroSD as later in 
this tutorial we will image the whole OS from our first MicroSD for deployment to our 
other Pi nodes.</p>

<h4>Hardware Limitations</h4>

<p> The first limitation to consider is overall storage space.  Ceph OSD processes 
require roughly 1MB of RAM per GB of storage.  Since we are co-locating monitor 
processes the effective storage limitation is 512GB per Pi 2 B (4 x 128GB sticks) RAW 
(before Ceph replication or erasure coding overhead).  

<P> Network speed is also a factor as discussed later in document. You will hit network 
speed limitations before you hit the speed limitations of the Pi 2 B's single USB 2.0 
bus (480Mbit).</p>

<h4>Network</h4>

<p> In this setup I used empty ports on my router. I run a local DNS server on my home 
router and use static assignments for local DNS. You may want to consider just using a 
flat 5 or 8 port (depending on number of nodes you plan to have) gigabit switch for the 
cluster network and WiPi modules for the public (connected to your router via WiFi). 
The nice thing about using a flat layer 2 switch is that if all the Pi nodes are in the 
same subnet, you don't have to worry about a gateway and it also keeps the cost down 
(compared to using router ports) while reducing the network overhead (for Ceph 
replication) on your home network. Using a dedicated switch for the cluster network will 
also increase your cluster performance, especially considering the 100Mbit limitations 
of the Pi 2 B's network port. By using a <a target="_b" 
href="http://thepihut.com/products/usb-wifi-adapter-for-the-raspberry-pi">BGN Dongle 
for Pi</a> and a dedicated switch for the cluster network, you will get a speedier 
cluster. This will use one of your 4 USB ports and thus, you will get one less OSD per 
Pi. Keep in mind, depending on if you use replication or erasure coding private traffic 
can be 1-X times greater then client IO (X being 3 in a standard replication profile) 
if that matters for your application. Of course this is all optional and for additional 
"clustery goodness". It really depends on budget, usage - etcetera.</p>

<h4>Object Storage Daemons</h4>

<p> In this guide, I co-located OSD journals on the OSD drives. For better performance, 
you can use a faster USB like the SanDisk Extreme 3.0 (keep in mind that you'll be 
limited by the 60MB/s speed of USB 2.0). Using a dedicated (faster) journal drive will 
yield much better performance. But you don't really need to worry about it unless you 
are using multiple networks as outlined above.  If you are not, 4 decent USB sticks  
will saturate your 100Mbit NIC per node. There is a lot more to learn about Ceph 
architecture that I cover in this article and I highly recommend you do so 
<a href="http://ceph.com/docs/master/" target="_b">here</a>.</p>

<h3>OSD Filesystem</h3>

<p> XFS is the default in Ceph Firefly. I prefer <a target="_b" 
href="https://btrfs.wiki.kernel.org/index.php/Main_Page">BTRFS</a> as an OSD filesystem 
for multi-fold reasons and I use it in this tutorial.</p>

<h4>Installation</h4>

<p> Assuming you have setup your network and operating system - have 3 nodes and the 
hardware you want to use - we can begin. The first thing to do is wire up power and 
network as you see fit. After that, you'll want to run through the initial raspi-config 
on what will become your admin node. Then it's time to make some changes. Once your admin
node is booted and configured, you have to edit <code>/etc/apt/sources.list</code>. 
Raspbian Wheezy has archaic versions of Ceph in the main repository, but the latest 
firefly version in the testing repository. Before we delve into this, I find it useful 
to install some basic tools and requirements. Connect via SSH or directly to terminal 
and issue this command from the Pi:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo apt-get install vim screen htop iotop btrfs-tools lsb-release gdisk
--------------------------------------------------------------------------------------
</PRE>


<p> From this point forward we will assume you are connecting to your Pi nodes via SSH. 
You've just installed BTRFS-tools, vim (better then vi) and some performance diagnostics 
tools I like. Now that we have <b>vim</b> it's time to edit our sources:</p>



<PRE>
--------------------------------------------------------------------------------------
vi /etc/apt/sources.list
--------------------------------------------------------------------------------------
</PRE>

<p>You'll see the contents of your sources file. Which will look like this:</p>


<PRE>
--------------------------------------------------------------------------------------
deb http://mirrordirector.raspbian.org/raspbian/ wheezy main contrib non-free rpi
# Uncomment line below then 'apt-get update' to enable 'apt-get source'
#deb-src http://archive.raspbian.org/raspbian/ wheezy main contrib non-free rpi
--------------------------------------------------------------------------------------
</PRE>

<p>Modify it to look like this:</p>


<PRE>
--------------------------------------------------------------------------------------
deb http://mirrordirector.raspbian.org/raspbian/ testing main contrib non-free rpi
# Uncomment line below then 'apt-get update' to enable 'apt-get source'
#deb-src http://archive.raspbian.org/raspbian/ testing main contrib non-free rpi
--------------------------------------------------------------------------------------
</PRE>

<p>We've replaced <code>wheezy</code> with <code>testing</code>. Once this is done, 
then issue this command:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo apt-get update
--------------------------------------------------------------------------------------
</PRE>

<p> Once this process has completed is time to start getting the OS ready for Ceph. 
Everything we do in this section up to the point of imaging the OS is needed for nodes 
that will run Ceph.</p>

<p> First we will create a ceph user and give it password-less sudo access. To do so 
issue these commands:</p>


<PRE>
--------------------------------------------------------------------------------------
ssh user@ceph-server
sudo useradd -d /home/ceph -m ceph
sudo passwd ceph
--------------------------------------------------------------------------------------
</PRE>

<p> Set the password to a memorable one as it will be used on all of your nodes in this 
guide. Now we need to give the ceph user sudo access</p>


<PRE>
--------------------------------------------------------------------------------------
echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
sudo chmod 0440 /etc/sudoers.d/ceph
--------------------------------------------------------------------------------------
</PRE>

<p> We'll be using ceph-deploy later and it's best to have a defult user to login as all 
the time. Issue this command:</p>


<PRE>
--------------------------------------------------------------------------------------
mkdir -p ~/.ssh/
--------------------------------------------------------------------------------------
</PRE>

<p>Then create this file using vi:</p>


<PRE>
--------------------------------------------------------------------------------------
vi ~/.ssh/config
--------------------------------------------------------------------------------------
</PRE>

<p> I assume 3 nodes in this tutorial and a naming convention of piY, where Y is the 
node number starting from 1.</p>


<PRE>
--------------------------------------------------------------------------------------
Host pi1  
   Hostname pi1  
   User ceph  
Host pi2  
   Hostname pi2  
   User ceph  
Host pi3  
   Hostname pi3  
   User ceph
--------------------------------------------------------------------------------------
</PRE>

<p> Save the file and exit. As far as hostnames, you can use whatever you want of 
course. As I mentioned, I run local DNS and DHCP with static assignments. If you do not, 
you'll need to edit <code>/etc/hosts</code> so that your nodes can resolve each-other. 
You can do this after the OS image, as each node will have a different IP.</p>

<p> Now it's time to install the ceph-deploy tool. Raspbian <b>wget</b>  can be strange 
with HTTPS so we will ignore the certificate (do so at your own peril):</p>


<PRE>
--------------------------------------------------------------------------------------
wget --no-check-certificate -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
echo deb http://ceph.com/debian-firefly/ wheezy main | sudo tee /etc/apt/sources.list.d/ceph.list
--------------------------------------------------------------------------------------
</PRE>

<p> Now that we've added the Ceph repository, we can install ceph-deploy:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo apt-get update &amp;&amp; sudo apt-get install ceph-deploy ceph ceph-common
--------------------------------------------------------------------------------------
</PRE>

<p> Since we are installing ceph from the Raspbian repositories, we need to change the 
default behavior of ceph-deploy:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo vi /usr/share/pyshared/ceph_deploy/hosts/debian/install.py
--------------------------------------------------------------------------------------
</PRE>


<p>Change</p>


<PRE>
--------------------------------------------------------------------------------------
 def install(distro, version_kind, version, adjust_repos):  
   codename = distro.codename  
   machine = distro.machine_type
--------------------------------------------------------------------------------------
</PRE>

<p>To</p>


<PRE>
--------------------------------------------------------------------------------------
 def install(distro, version_kind, version, adjust_repos):  
   adjust_repos = False
   codename = distro.codename  
   machine = distro.machine_type
--------------------------------------------------------------------------------------
</PRE>

<p> This will prevent ceph-deploy from altering repos as the Ceph armhf (Rasberry Pi's 
processor type) repos are mostly empty.</p>

<p> Finally, we should revert the contents of <code>/etc/apt/sources.list</code> :</p>


<PRE>
--------------------------------------------------------------------------------------
sudo vi /etc/apt/sources.list
--------------------------------------------------------------------------------------
</PRE>

<p>You'll see the contents of your sources file. Which will look like this:</p>


<PRE>
--------------------------------------------------------------------------------------
deb http://mirrordirector.raspbian.org/raspbian/ testing main contrib non-free rpi
# Uncomment line below then 'apt-get update' to enable 'apt-get source'
#deb-src http://archive.raspbian.org/raspbian/ testing main contrib non-free rpi
--------------------------------------------------------------------------------------
</PRE>

<p>Modify it to look like this:</p>


<PRE>
--------------------------------------------------------------------------------------
deb http://mirrordirector.raspbian.org/raspbian/ wheezy main contrib non-free rpi
# Uncomment line below then 'apt-get update' to enable 'apt-get source'
#deb-src http://archive.raspbian.org/raspbian/ wheezy main contrib non-free rpi
--------------------------------------------------------------------------------------
</PRE>

<p> </p>

<p>We've replaced <b>testing</b> with <b>wheezy</b>. Once this is done, then issue this 
command:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo apt-get update
--------------------------------------------------------------------------------------
</PRE>

<p> </p>

<h4>Kernel Tweaks</h4>

<p> We are also going to tweak some kernel parameters for better stability. To do so we 
will edit <code>/etc/sysctl.conf</code>.</p>


<PRE>
--------------------------------------------------------------------------------------
vi /etc/sysctl.conf
--------------------------------------------------------------------------------------
</PRE>

<p>At the bottom of the file, change add the following lines:</p>


<PRE>
--------------------------------------------------------------------------------------
vm.swappiness=1
vm.min_free_kbytes = 32768
kernel.pid_max = 32768
--------------------------------------------------------------------------------------
</PRE>

<p></p>

<h4>Imaging the OS</h4>

<p> Now we have a good baseline for deploying ceph to our other Pi nodes. It's time to 
stop our admin node and image the drive (MicroSD). Issue:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo halt
--------------------------------------------------------------------------------------
</PRE>

<p> Then unplug power to your Pi node and remove the MicroSD. Insert the microSD in your 
SD adapter, then the SD adapter into your Linux PC. You'll need at least as much free 
drive space on your PC as the size of the MicroSD card. Where <code>/dev/mmcblk0</code> 
is your SD card and ceph-pi.img is your image destination, run:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo dd if=/dev/mmcblk0 of=ceph-pi.img bs=4M
--------------------------------------------------------------------------------------
</PRE>

<p> This can take a vary long time depending on the size of your SD and you can compress 
it with gzip  or xz  for long term storage (empty space compresses really well it turns 
out). Once the command returns, run <b>sync</b>  to flush the cache to disk and make 
sure you can remove the MicroSD</p>

<h4>Imaging Your Nodes OS Drives</h4>

<p> Now that you have a good baseline image on your PC, you are ready to crank out 
"Ceph-Pi" nodes - without redoing all of the above. To do so, insert a fresh MicroSD 
into your adapter and then PC. Then assuming ceph-pi.img  is your OS image and 
<code>/dev/mmcblk0</code> is your MicroSD card, run:</p>


<PRE>
--------------------------------------------------------------------------------------
sudo dd if=ceph-pi.img of=/dev/mmcblk0 bs=4M
--------------------------------------------------------------------------------------
</PRE>

<p>Repeat this for as many nodes as you intend to deploy.</p>

<h4>Create a Ceph Cluster on Raspberry Pi</h4>

<p> Insert your ceph-pi MicroSD cards into your Pi nodes and power them all on. You've 
made it this far, now it's time to get "cephy". Deploying with ceph-deploy is a breeze. 
First we need to SSH to our admin node, make sure you have setup IPs, network and 
<code>/etc/hosts</code> on all Pi nodes if you are not using local DNS and DHCP with 
static assignments.</p>

<p> We need to generate and distribute an SSH key for password-less authentication 
between nodes. To do so, run (leave the password blank):</p>


<PRE>
--------------------------------------------------------------------------------------
ssh-keygen
Generating public/private key pair.
Enter file in which to save the key (/ceph-client/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /ceph-client/.ssh/id_rsa.
Your public key has been saved in /ceph-client/.ssh/id_rsa.pub.
--------------------------------------------------------------------------------------
</PRE>

<p> Now copy the key to all nodes (assuming 3 with the naming convention from above):</p>


<PRE>
--------------------------------------------------------------------------------------
ssh-copy-id ceph@pi1  
ssh-copy-id ceph@pi2  
ssh-copy-id ceph@pi3
--------------------------------------------------------------------------------------
</PRE>

<p> You will be prompted for the password you created for the ceph user each time to 
establish initial authentication.</p>

<p> Once that is done and you are connected to your admin node (1st node in the cluster) 
as the <b>pi</b> user you'll want to create an admin node directory:</p>


<PRE>
--------------------------------------------------------------------------------------
mkdir -p ~/ceph-pi-cluster
cd ~/ceph-pi-cluster
--------------------------------------------------------------------------------------
</PRE>

<p></p>

<h3>Creating an initial Ceph Configuration</h3>

<p> We are going to create an initial Ceph configuration assuming all 3 pi nodes as 
monitors. If you have more, keep in mind - you always want an odd number of monitors to 
avoid a <a href="http://en.wikipedia.org/wiki/Split-brain_%28computing%29" 
target="_b">split-brain</a> scenario. To to this run:</p>


<PRE>
--------------------------------------------------------------------------------------
ceph-deploy new pi1 pi2 pi3
--------------------------------------------------------------------------------------
</PRE>

<p> Now there are some special tweaks that should be made for best stability and 
performance within the hardware limitations of the Raspberry Pi 2 B. To apply these 
changes we'll need to edit the <code>ceph.conf</code> here on the admin node before it 
is distributed. To do so:</p>


<PRE>
--------------------------------------------------------------------------------------
vi ~/ceph-pi-cluster/ceph.conf
--------------------------------------------------------------------------------------
</PRE>

<p>After the existing lines add:</p>


<PRE>
--------------------------------------------------------------------------------------
  # Disable in-memory logs
  debug_lockdep = 0/0
  debug_context = 0/0
  debug_crush = 0/0
  debug_buffer = 0/0
  debug_timer = 0/0
  debug_filer = 0/0
  debug_objecter = 0/0
  debug_rados = 0/0
  debug_rbd = 0/0
  debug_journaler = 0/0
  debug_objectcatcher = 0/0
  debug_client = 0/0
  debug_osd = 0/0
  debug_optracker = 0/0
  debug_objclass = 0/0
  debug_filestore = 0/0
  debug_journal = 0/0
  debug_ms = 0/0
  debug_monc = 0/0
  debug_tp = 0/0
  debug_auth = 0/0
  debug_finisher = 0/0
  debug_heartbeatmap = 0/0
  debug_perfcounter = 0/0
  debug_asok = 0/0
  debug_throttle = 0/0
  debug_mon = 0/0
  debug_paxos = 0/0
  debug_rgw = 0/0
  osd heartbeat grace = 8

[mon]
  mon compact on start = true
  mon osd down out subtree_limit = host

[osd]
  # Filesystem Optimizations
  osd mkfs type = btrfs
  osd journal size = 1024

  # Performance tuning
  max open files = 327680
  osd op threads = 2
  filestore op threads = 2
  
  #Capacity Tuning
  osd backfill full ratio = 0.95
  mon osd nearfull ratio = 0.90
  mon osd full ratio = 0.95

  # Recovery tuning
  osd recovery max active = 1
  osd recovery max single start = 1
  osd max backfills = 1
  osd recovery op priority = 1

  # Optimize Filestore Merge and Split
  filestore merge threshold = 40
  filestore split multiple = 8
--------------------------------------------------------------------------------------
</PRE>

<p> </p>

<h3>Creating Initial Monitors</h3>

<p> Now we can deploy our spiffy ceph.conf, create our initial monitor daemons, deploy 
our authentication keyring and chmod it as needed. We will be deploying to all 3 nodes 
for the purposes of this guide:</p>


<PRE>
--------------------------------------------------------------------------------------
ceph-deploy mon create-initial
ceph-deploy admin pi1 pi2 pi3
for i in pi1 pi2 pi3;do ssh $i chmod 644 /etc/ceph/ceph.client.admin.keyring;done
--------------------------------------------------------------------------------------
</PRE>

<p></p>

<h3>Creating OSDs (Object Storage Daemons)</h3>

<p> Ready to create some storage? I know I am. Insert your USB keys of choice into 
your Pi USB ports. For the purposes of this guide I will be deploying 1 OSD (USB key) 
per Pi node. I will also be using the BTRFS filesystem and co-locating the journals on 
the OSDs with a default journal size of 1GB (assuming 2 * 40MB/s throughput max and a 
default filestore max sync interval of 5). This value is hard coded into our ceph-pi 
config above. The formula is:</p>


<PRE>
--------------------------------------------------------------------------------------
osd journal size = {2 * (expected throughput * filestore max sync interval)}
--------------------------------------------------------------------------------------
</PRE>

<p> So let's deploy our OSDs. Once our USBs are plugged in, use <b>lsblk</b> to display 
the device locations. To make sure our drives are clean and have a GPT partition table, 
use the <b>gdisk</b> command for each OSD on each node. Assuming <code>/dev/sda</code> 
as our OSD:</p>

<pre>
 gdisk /dev/sda
</pre>

<p> Create a new partition table, write it to disk and exit. Do this for each OSD 
on each node. You can craft a bash <b>for</b> loop if you are feeling "bashy" or 
programmatic.</p>

<p> Once all OSD drives have a fresh partition table you can use ceph-deploy to create 
your OSDs (using BTRFS for this guide) where pi1 is our present node and 
<code>/dev/sda</code> is the OSD we are creating:</p>


<PRE>
--------------------------------------------------------------------------------------
ceph-deploy osd create --fs-type btrfs pi1:/dev/sda
--------------------------------------------------------------------------------------
</PRE>

<p> Repeat this for all OSD drives on all nodes (or write a for loop). Once you've 
created at least 3 you are ready to move on.</p>

<h4>Checking Cluster Health</h4>

<p> Congratulations! You should have a working Ceph-Pi cluster. Trust, but verify. Get 
the health status of your cluster using this command:</p>


<PRE>
--------------------------------------------------------------------------------------
ceph -s
--------------------------------------------------------------------------------------
</PRE>

<p>and for a less verbose output</p>


<PRE>
--------------------------------------------------------------------------------------
ceph health 
--------------------------------------------------------------------------------------
</PRE>

<p></p>

<h4>What to do now?</h4>

<p> Use your storage cluster! Create an RBD, mount it - export NFS or CIFS. There is a 
lot of reading out there. Now you know how to deploy a Ceph cluster on Raspberry Pi.</p>

<h3>References</h3>

<p>http://millibit.blogspot.com/2014/12/ceph-pi-installing-ceph-on-raspberry-pi.html</p>

<p>http://ceph.com/docs/v0.80.5/start/</p>

<p>https://www.raspberrypi.org/</p>

<header>
<h3> <a href="http://bryanapperson.com/blog/oscon-2017-ha-file-services-ceph-samba/" 
target="_b"> OSCON 2017 Proposal: Architect HA File Services With Ceph and 
Samba </a></h3>
</header>

<footer> 

<header>
<h3> <a href="http://bryanapperson.com/blog/ceph-osd-performance/" 
rel="bookmark"> Ceph OSD Performance: Backends and Filesystems </a></h3>
</header>

<footer> 
<h4> 14 Comments</h4>


<ol>
  <li>Pingback: Distributed file storage with a Ceph cluster on Raspberry Pi | 
      Raspberry Pi Pod</li><p></p>

  <li>Mike Kelly

<p>Hi,</p>

<p> This looks like an interesting use of the Raspberry Pi, but I wonder if this is 
really that cost-effective of a solution?</p>

<p> When I crunched the numbers, it came out to about $1/GB of storage, if you maxed 
out your nodes with 4 128GB drives and had 3 replicas... but it seems like, once you 
need to scale above a TB or so of storage, it's more cost effective to just build 
"real" servers using spinning drives at a much higher capacity per node?</p>

  <ul>
    <li>Bryan Apperson

    <p>Of course, this is more of a proof-of-concept for learning ceph. Not meant to 
       be cheaper per GB, but cheaper for initial cost. A x86_64 ceph cluster with 
       10Gbit networking costs 5 figures. This is a 3 figure cost of entry way to begin 
       learning ceph.</p></li>
  </ul></li>

  <li>Thomas Bludau

<p> Hey, its working now on my 3 raspberrys 2 too with saltstack implementation and 
automatical installation script :)! Thanks for this documentation! Overread that you 
changed the source.list two times and only for the ceph installation on the first 
try.</p>

  <ul>
    <li>Bryan Apperson

    <p> Yeah, I automated install as well. However I am a fan of making people perform 
        the commands so that they learn rather then:</p>

    <p> wget bash.sh<br> chmod 755 bash.sh<br> sudo ./bash.sh</p>

    <p> Teaches bad form (and security)!</p>

    <p> Thanks for going through the tutorial. Is there a link to your implementation 
        for others to use?</p></li>
  </ul></li>

  <li>Steven Pemberton

    <p>I've done something similar with a 6+1 node Pi cluster running Ceph. I'm 
       currently using 24x 8GB USB sticks as storage.</p></li>


  <li>Niels 

    <p>Hi Bryan,</p>

  <p> I'm getting stuck at the apt-get install ceph-deploy with the following error:<br> 
Reading state information... Done<br> E: Unable to locate package ceph-deploy</p>

    <p> Any thoughts on why this may be? Using Wheezy, also tried Jessie same result.</p>

    <p> ceph and ceph-common have been installed.</p>

    <p> Thanks,<br> Niels</p></li>


  <li>Dave Graham

    <p> just a note: doesn't work for Debian Jessie.  I either have to backport to 
        Wheezy (not optimal) or go through a ton of various hacking and such without 
        using ceph-deploy.</p>

    <p> just a heads up. <img src="https://s.w.org/images/core/emoji/2/svg/1f642.svg" 
        width="20"></p></li>


  <li>Niels Sommer

    <p>Hi Bryan,</p>

    <p> Great article on Ceph installation. I have one problem, at the step of 
        installing ceph-deploy. It is not found in the package, I have tried 
        different revisions of ceph and the package is just not found. The ceph 
        and ceph-common packages are installed fine. What might I be doing 
        wrong?</p>

    <p> Thanks for this intro to a cost effective ceph cluster <img width="20" 
        src="https://s.w.org/images/core/emoji/2/svg/1f642.svg"></p>

    <p>Cheers,</p>

    <p>Niels</p>

  <ul>
    <li>Bryan Apperson

    <p>It may not be in the repository any longer. Have you looked in the ceph repos? 
       You may be able to pull it down individually.</p></li>
  </ul></li>

  <li>Mayur 

    <p>Hi Bryan,<br>
       Very usefull article Thanks for posting,  I wanted to implement a storage server 
       within area, where client side is windows OS, so is it possible to implement 
       this project.</p>

  <ul>
    <li>Bryan Apperson

      <p>Yes, you'll probably want to use CIFS or NFS to export an RBD image.</p></li>
  </ul></li>

  <li>Marian

    <p> Thanks a lot for this nice tutorial. Quick question: my deployment fails when 
        I do:<br> ceph-deploy mon create-initial</p>

    <p>It connects to the remote host, runs a bunch of stuff, then comes up with this 
        error:<br> Failed to execute command: sudo systemctl enable ceph.target</p>

    <p>I'm stuck; don't know what to do next. If I run that command manually, I get 
       the same message.<br> Failed to execute operation: No such file or directory</p>
  </li>


  <li>Marian 

    <p>HI Bryan,</p>

    <p> Do you have any experience with the Ubuntu Mate on arm processor? I have the new 
        Odroid which is much better (hardware-wise) than Rpi and I have trouble getting 
        stuff to work. It installs CEPH just fine from repositories, but then.. I'm 
        stuck <img src="https://s.w.org/images/core/emoji/2/svg/1f642.svg" 
        width="20"><br> Any advice?</p>

</ol>


<a name="FlashingLinuxImageToXu4"></a>
<h4>Table of Contents</h4>

<ul style="" class="toc">
  <li><a href="#flashing_s_w_release">Flashing S/W Release</a></div>
  <ul class="toc">
    <li><a href="#downloading">Downloading</a></div></li>
    <li><a href="#checking_the_file_integrity">Checking the file 
        integrity</a></div></li>
    <li><a href="#extracting">Extracting</a></div></li>
    <li><a href="#flashing">Flashing</a></div>
    <ul class="toc">
      <li><a href="#using_win32diskimager">Using Win32DiskImager</a></div></li>
      <li><a href="#trouble_shooting">Trouble shooting</a></div></li>
      <li><a href="#using_dd_disk_destroyer">Using dd (Disk Destroyer)</a></div></li>
      <li><a href="#notes">Notes</a></div></li>
    </ul></li>
    <li><a href="#booting">Booting</a></div></li>
  </ul></li>
  <li><a href="#usb_card_reader_compatibility">USB Card Reader Compatibility issue on 
      the eMMC connection to your PC</a></div></li>
  <li><a href="#tips">Tips</a></div>
  <ul class="toc">
    <li><a href="#verifying_the_burned_image_with_linux">Verifying the burned image 
        with Linux</a></div></li>
  </ul></li>
</ul>
<!-- TOC END -->

<h3 class="sectionedit1" id="flashing_s_w_release">Flashing S/W Release 
<a href="http://odroid.com/dokuwiki/doku.php?id=en:odroid_flashing_tools" 
target="_b">(Source Origin)</a></h3>

<p> This page introduce you how you can flash S/W release onto your eMMC or MicroSD and 
can boot without a failure or panic. 
<img src="http://odroid.com/dokuwiki/lib/images/smileys/icon_cool.gif"> So here is the 
generic process for ODROID boards to install new S/W release.
</p>

</div>

<h4 class="sectionedit2" id="downloading">Downloading</h4>

<p> Download the S/W release image from our server. The server is shared whenever a new 
release is announced. Basically Hardkernel release two different O/S images officially, 
Android and Linux (Ubuntu), see <a href="http://dn.odroid.com/5422/ODROID-XU3/" 
target="_b">5422/ODROID-XU3</a>.  I think the next image will be better: 
<a href="https://image.armbian.com/" target="_b">armbian image</a>, choose 
Armbian_5.20_Odroidxu4_Debian_jessie_3.10.103_desktop.7z.  A file with the 7Z file 
extension is a 7-Zip Compressed file.  Probably, the utilities provided by package 
<b>p7zip-full</b> can be used to unpack 7z archives.
</p>

<h4 class="sectionedit4" id="extracting">Extracting</h4>

<p> The Linux S/W release from Hardkernel are basically compressed as .xz format and 
Android images are compressed with ZIP. Therefore you must extract the file with 
particular uncompressing tools to obtain the binary to flash.  Based on the feedback 
from many ODROID users, we strongly recommend <strong>7-zip</strong> for Windows or 
Linux user, and <strong>unxz</strong> for Linux users.  Since other extract tools 
could make dirty chunk while extracting. Click 
<a href="http://www.7-zip.org/download.html">this</a> to download <strong>7-zip</strong> 
and select a proper version for your desktop.
</p>

<h4 class="sectionedit5" id="flashing">Flashing</h4>

<p> You also need a special tool to flash the image into your eMMC or MicroSD, since 
the S/W release binary is not a regular file as is in the storage. Linux users can 
use <strong>dd</strong> which is mostly included in many Linux distributions. 
</p>


<h4>Using dd (Disk Destroyer)</h4>

<p> <strong>dd</strong> is included in most Linux distribution by default and very 
useful tool to maintain your storages. In contrast, it is very harmful as its name if 
you use wrong command. For example, if can destroy your desktop with your ODROID image 
which is to be installed into your MicroSD.
</p>

<p>
Here is the example to use the command.
</p>

<pre class="code">
$ sudo dd if=&lt;my/odroid/image.img&gt; of=&lt;/dev/path/of/card&gt; bs=1M conv=fsync
$ sync
</pre>

<p> In order to flush all cached data into MicroSD, you <strong>MUST</strong> do the 
command <strong>sync</strong> multiple times. Otherwise data would be corrupted.
</p>

<p> The image file would be compressed to decrease the file size, and its file name must 
be terminated with <strong>.xz</strong>, in that case you must extract the file to 
obtain correct image which terminate with <strong>.img</strong>.</p>

<pre class="code">
$ unxz my-odroid-image.img.xz
</pre>

<p> We are strongly recommend to check the exact path of your USB card reader to flash 
before starting <strong>dd</strong> command. Or you can easily find the card reader 
from <code>/dev/disk/by-id/</code> instead of <code>/dev/sdb</code> or 
<code>/dev/sdc</code>. For example, if you use the Transcend USB card reader and it would show as <code>/dev/disk/by-id/usb-TS-RDF5_SD_Transcend_000000000039-0</code>. This use long path but no harmful to destroy my desktop at all by mistake.
</p>

<h4 class="sectionedit13" id="notes">Notes</h4>

<p> Many SD-to-MicroSD adapter don't work correctly.  You need to connect the flash 
storage to your USB reader directly.<br>

<a href="http://odroid.com/dokuwiki/lib/exe/detail.php?id=en%3Aodroid_flashing_tools&amp;media=en:s_oem-micro-sd-card-micro-001-.jpg""><img src="http://odroid.com/dokuwiki/lib/exe/fetch.php?w=200&tok=0cd523&media=en:s_oem-micro-sd-card-micro-001-.jpg"></a>
</p>

<h4 class="sectionedit14" id="booting">Booting</h4>

<p> If you are done properly up to <strong>Flashing</strong>, you can boot your ODROID 
board. Just insert your card into a slot of the board, and connect DC adaptor to a 
DC-jack. And watch the LED status on the board.  Basically all ODROID board have one 
or two LEDs and one of them is dedicated to monitor the board status, LED color is 
different depend on the board model. When power is on and boot loader is running, a 
LED keeps to turn on. And When the board loads Linux kernel image and kernel image is 
started properly, the LED will start blinking. Which means the board is booted properly.
</p>

<p> It would be also nice if you can check the console log thru serial connection or a 
display out like LCD/DVI/HDMI.
</p>


<h4 id="usb_card_reader_compatibility">USB Card Reader Compatibility issue on the eMMC 
connection to your PC</h4>

<p> You may have a doubt if your USB card reader works to flash the O/S images. In 
general, most of USB card reader works well in the market.  But if you like to check or 
listen other users regarding the USB card reader, please visit 
<a href="http://forum.odroid.com/viewtopic.php?f=53&amp;t=2725">here</a>.
</p>

</div>

<h4 class="sectionedit16" id="tips">Tips: Verifying the burned image with Linux</h4>
<div>
<pre class="code">$ sudo dd if=&lt;/dev/path/of/card&gt; bs=512 count=$((`stat -c%s &lt;my/odroid/image.img&gt;`/512)) | md5sum
167742+0 records in
167742+0 records out
85883904 bytes (86 MB, 82 MiB) copied, 0.153662 s, 559 MB/s
9b085251a00ad7ae16fe42fbfb25c042  -
$ dd if=&lt;my/odroid/image.img&gt; bs=512 count=$((`stat -c%s &lt;my/odroid/image.img&gt;`/512)) | md5sum
167742+0 records in
167742+0 records out
85883904 bytes (86 MB, 82 MiB) copied, 0.140843 s, 610 MB/s
9b085251a00ad7ae16fe42fbfb25c042  - </pre>

<p>
Compare above two MD5 values. They must be identical.
</p>

</body></html>
